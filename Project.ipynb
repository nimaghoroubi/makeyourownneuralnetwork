{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nimaghoroubi/makeyourownneuralnetwork/blob/master/Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4P3kDqcF3l5c",
        "colab_type": "code",
        "outputId": "86bc42ea-24c4-4024-bd3d-2d6523e6789b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Flatten\n",
        "from sklearn.metrics import mean_absolute_error \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import arange\n",
        "import warnings \n",
        "warnings.filterwarnings('ignore')\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y1Rjn5uLGO-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -q \"training-data-5400.zip\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-UeiB_7yVcA",
        "colab_type": "code",
        "outputId": "6a910bda-d7fa-436f-81d7-c767f6336011",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "#For all new data-set dated 12-Dec\n",
        "import glob\n",
        "\n",
        "path = r\"training-data/training-data-1000\"\n",
        "all_files = glob.glob(path + \"/*.csv\")\n",
        "\n",
        "li = []\n",
        "new_table_list = []\n",
        "counter = 0\n",
        "df2 = pd.DataFrame()\n",
        "\n",
        "for filename in all_files:\n",
        "    df = pd.read_csv(filename, header=None)\n",
        "    new_table_list.append(filename.split(\"-\")[4])\n",
        "\n",
        "    string = str(new_table_list[counter])\n",
        "    r_val = string[4:]\n",
        "    r_val = float(r_val)\n",
        "\n",
        "    df.columns = ['A', 'B', 'C', r_val]\n",
        "\n",
        "    df1 = pd.DataFrame(df[r_val])\n",
        "    df2 = df2.append(df1.T)\n",
        "\n",
        "    counter = counter + 1\n",
        "df2 = df2.reset_index()\n",
        "df2 = df2.rename(columns={'index':'Rval'})\n",
        "df2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rval</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>8060</th>\n",
              "      <th>8061</th>\n",
              "      <th>8062</th>\n",
              "      <th>8063</th>\n",
              "      <th>8064</th>\n",
              "      <th>8065</th>\n",
              "      <th>8066</th>\n",
              "      <th>8067</th>\n",
              "      <th>8068</th>\n",
              "      <th>8069</th>\n",
              "      <th>8070</th>\n",
              "      <th>8071</th>\n",
              "      <th>8072</th>\n",
              "      <th>8073</th>\n",
              "      <th>8074</th>\n",
              "      <th>8075</th>\n",
              "      <th>8076</th>\n",
              "      <th>8077</th>\n",
              "      <th>8078</th>\n",
              "      <th>8079</th>\n",
              "      <th>8080</th>\n",
              "      <th>8081</th>\n",
              "      <th>8082</th>\n",
              "      <th>8083</th>\n",
              "      <th>8084</th>\n",
              "      <th>8085</th>\n",
              "      <th>8086</th>\n",
              "      <th>8087</th>\n",
              "      <th>8088</th>\n",
              "      <th>8089</th>\n",
              "      <th>8090</th>\n",
              "      <th>8091</th>\n",
              "      <th>8092</th>\n",
              "      <th>8093</th>\n",
              "      <th>8094</th>\n",
              "      <th>8095</th>\n",
              "      <th>8096</th>\n",
              "      <th>8097</th>\n",
              "      <th>8098</th>\n",
              "      <th>8099</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.259</td>\n",
              "      <td>0.000716</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.001109</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001121</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000946</td>\n",
              "      <td>0.001939</td>\n",
              "      <td>0.013783</td>\n",
              "      <td>0.014734</td>\n",
              "      <td>0.017301</td>\n",
              "      <td>0.023676</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.003589</td>\n",
              "      <td>0.000923</td>\n",
              "      <td>0.003472</td>\n",
              "      <td>0.004351</td>\n",
              "      <td>0.000716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.286</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.001623</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.001134</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000957</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.000957</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000949</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000957</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.018172</td>\n",
              "      <td>0.019134</td>\n",
              "      <td>0.021732</td>\n",
              "      <td>0.022686</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.005490</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>0.000725</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2.625</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.001289</td>\n",
              "      <td>0.001288</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001303</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.001005</td>\n",
              "      <td>0.001080</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001089</td>\n",
              "      <td>0.001020</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001290</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.001099</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.012107</td>\n",
              "      <td>0.013212</td>\n",
              "      <td>0.016195</td>\n",
              "      <td>0.019424</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.004171</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.004034</td>\n",
              "      <td>0.005057</td>\n",
              "      <td>0.000832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.027</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000057</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.087</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000028</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>2.166</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001075</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.001064</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000952</td>\n",
              "      <td>0.011608</td>\n",
              "      <td>0.012519</td>\n",
              "      <td>0.166958</td>\n",
              "      <td>0.169622</td>\n",
              "      <td>0.170550</td>\n",
              "      <td>0.171431</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.004962</td>\n",
              "      <td>0.005805</td>\n",
              "      <td>0.000687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>0.444</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000175</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000170</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000172</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000568</td>\n",
              "      <td>0.003071</td>\n",
              "      <td>0.004324</td>\n",
              "      <td>0.004514</td>\n",
              "      <td>0.005029</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000141</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>0.531</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000215</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>0.003894</td>\n",
              "      <td>0.004122</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>2.013</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.000988</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000999</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.000828</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.014488</td>\n",
              "      <td>0.015335</td>\n",
              "      <td>0.016182</td>\n",
              "      <td>0.017022</td>\n",
              "      <td>0.000863</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.010892</td>\n",
              "      <td>0.011676</td>\n",
              "      <td>0.000638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>1.200</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000852</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000589</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>0.009190</td>\n",
              "      <td>0.010667</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.001907</td>\n",
              "      <td>0.000491</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>0.000467</td>\n",
              "      <td>0.000380</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 8101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Rval         0         1  ...      8097      8098      8099\n",
              "0    2.259  0.000716  0.000888  ...  0.003472  0.004351  0.000716\n",
              "1    2.286  0.000725  0.001623  ...  0.000990  0.001880  0.000725\n",
              "2    2.625  0.000832  0.001032  ...  0.004034  0.005057  0.000832\n",
              "3    0.027  0.000009  0.000019  ...  0.000030  0.000041  0.000009\n",
              "4    0.087  0.000028  0.000034  ...  0.000038  0.000034  0.000028\n",
              "..     ...       ...       ...  ...       ...       ...       ...\n",
              "995  2.166  0.000687  0.001538  ...  0.004962  0.005805  0.000687\n",
              "996  0.444  0.000141  0.000175  ...  0.000501  0.000173  0.000141\n",
              "997  0.531  0.000168  0.000209  ...  0.000230  0.000207  0.000168\n",
              "998  2.013  0.000638  0.001429  ...  0.010892  0.011676  0.000638\n",
              "999  1.200  0.000380  0.000852  ...  0.001354  0.000467  0.000380\n",
              "\n",
              "[1000 rows x 8101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6SYCCpyKrSF",
        "colab_type": "code",
        "outputId": "2e47f2a1-4bc4-4bba-b0e6-7fb24bdbb7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "fig, axs =plt.subplots(1,1)\n",
        "clust_data = np.random.random((10,3))\n",
        "collabel=(\"Rval\", \"Node 1\", \"Node 2\")\n",
        "the_table = axs.table(cellText=clust_data,colLabels=collabel,loc='center')\n",
        "\n",
        "axs.plot()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEATBRfh/e2ynCG8de50AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhniJThP3prS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "main = pd.DataFrame(np.zeros((df2.shape[0],df2.shape[1])))\n",
        "main[0] = df2['Rval']\n",
        "target = df2\n",
        "train = main"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-UQcP5AyKFp",
        "colab_type": "code",
        "outputId": "b3d0c41d-d3ad-40a9-bfff-c56cb6f033c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(train.shape[1])\n",
        "print(target.shape[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8101\n",
            "8101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HCKXazTH5tdA",
        "colab_type": "code",
        "outputId": "8c3a4f19-4423-49fc-8180-b39b178a3521",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "NN_model = Sequential()\n",
        "\n",
        "# The Input Layer :\n",
        "NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "# The Hidden Layers :\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "# The Output Layer :\n",
        "NN_model.add(Dense(train.shape[1], kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "# Compile the network :\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error','accuracy'])\n",
        "#NN_model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4409: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsaEhD2QBPyU",
        "colab_type": "code",
        "outputId": "b4f3d1a7-e921-46b5-e61c-6a54251077ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Start timing')\n",
        "start = time.time()\n",
        "history = NN_model.fit(train, target, epochs=500, batch_size=30, validation_split = 0.3)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start timing\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 700 samples, validate on 300 samples\n",
            "Epoch 1/500\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "700/700 [==============================] - 10s 14ms/step - loss: 0.0213 - mean_absolute_error: 0.0213 - acc: 0.0000e+00 - val_loss: 0.0202 - val_mean_absolute_error: 0.0202 - val_acc: 0.0000e+00\n",
            "Epoch 2/500\n",
            "700/700 [==============================] - 0s 428us/step - loss: 0.0188 - mean_absolute_error: 0.0188 - acc: 0.0000e+00 - val_loss: 0.0189 - val_mean_absolute_error: 0.0189 - val_acc: 0.0000e+00\n",
            "Epoch 3/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0178 - mean_absolute_error: 0.0178 - acc: 0.0000e+00 - val_loss: 0.0181 - val_mean_absolute_error: 0.0181 - val_acc: 0.0000e+00\n",
            "Epoch 4/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0171 - mean_absolute_error: 0.0171 - acc: 0.0000e+00 - val_loss: 0.0173 - val_mean_absolute_error: 0.0173 - val_acc: 0.0000e+00\n",
            "Epoch 5/500\n",
            "700/700 [==============================] - 0s 422us/step - loss: 0.0164 - mean_absolute_error: 0.0164 - acc: 0.0000e+00 - val_loss: 0.0167 - val_mean_absolute_error: 0.0167 - val_acc: 0.0000e+00\n",
            "Epoch 6/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0158 - mean_absolute_error: 0.0158 - acc: 0.0000e+00 - val_loss: 0.0160 - val_mean_absolute_error: 0.0160 - val_acc: 0.0000e+00\n",
            "Epoch 7/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0153 - mean_absolute_error: 0.0153 - acc: 0.0000e+00 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - val_acc: 0.0000e+00\n",
            "Epoch 8/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0146 - mean_absolute_error: 0.0146 - acc: 0.0000e+00 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - val_acc: 0.0000e+00\n",
            "Epoch 9/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0139 - mean_absolute_error: 0.0139 - acc: 0.0000e+00 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 0.0000e+00\n",
            "Epoch 10/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0132 - mean_absolute_error: 0.0132 - acc: 0.0000e+00 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 0.0000e+00\n",
            "Epoch 11/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0117 - mean_absolute_error: 0.0117 - acc: 0.0014 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - val_acc: 0.0733\n",
            "Epoch 12/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0073 - mean_absolute_error: 0.0073 - acc: 0.2629 - val_loss: 0.0060 - val_mean_absolute_error: 0.0060 - val_acc: 0.2500\n",
            "Epoch 13/500\n",
            "700/700 [==============================] - 0s 417us/step - loss: 0.0058 - mean_absolute_error: 0.0058 - acc: 0.2586 - val_loss: 0.0053 - val_mean_absolute_error: 0.0053 - val_acc: 0.2333\n",
            "Epoch 14/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0050 - mean_absolute_error: 0.0050 - acc: 0.2629 - val_loss: 0.0049 - val_mean_absolute_error: 0.0049 - val_acc: 0.2333\n",
            "Epoch 15/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0048 - mean_absolute_error: 0.0048 - acc: 0.2743 - val_loss: 0.0047 - val_mean_absolute_error: 0.0047 - val_acc: 0.2767\n",
            "Epoch 16/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - acc: 0.4586 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046 - val_acc: 0.8533\n",
            "Epoch 17/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - acc: 0.9871 - val_loss: 0.0050 - val_mean_absolute_error: 0.0050 - val_acc: 1.0000\n",
            "Epoch 18/500\n",
            "700/700 [==============================] - 0s 419us/step - loss: 0.0046 - mean_absolute_error: 0.0046 - acc: 1.0000 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045 - val_acc: 1.0000\n",
            "Epoch 19/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0044 - mean_absolute_error: 0.0044 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 20/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 21/500\n",
            "700/700 [==============================] - 0s 416us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 22/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0045 - mean_absolute_error: 0.0045 - acc: 1.0000 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045 - val_acc: 1.0000\n",
            "Epoch 23/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 24/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 25/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 26/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 27/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0043 - mean_absolute_error: 0.0043 - acc: 1.0000 - val_loss: 0.0045 - val_mean_absolute_error: 0.0045 - val_acc: 1.0000\n",
            "Epoch 28/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0042 - mean_absolute_error: 0.0042 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 29/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 30/500\n",
            "700/700 [==============================] - 0s 422us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 31/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 32/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 33/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 34/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 35/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 36/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 37/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 38/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 39/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 40/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 41/500\n",
            "700/700 [==============================] - 0s 423us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 42/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 43/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 44/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 45/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 46/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 47/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 48/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 49/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 50/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 51/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 52/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 53/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 54/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 55/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 56/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 57/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 58/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 59/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 60/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 61/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 1.0000\n",
            "Epoch 62/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 1.0000\n",
            "Epoch 63/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 64/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0044 - val_mean_absolute_error: 0.0044 - val_acc: 1.0000\n",
            "Epoch 65/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 66/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 67/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 68/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 1.0000 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 1.0000\n",
            "Epoch 69/500\n",
            "700/700 [==============================] - 0s 429us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9771 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 70/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 71/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 72/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 73/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 74/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 75/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 76/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 77/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 78/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 79/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0046 - val_mean_absolute_error: 0.0046 - val_acc: 0.9600\n",
            "Epoch 80/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 81/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 82/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 83/500\n",
            "700/700 [==============================] - 0s 421us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 84/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 85/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 86/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 87/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 88/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 89/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 90/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 91/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 92/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 93/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 94/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 95/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9733\n",
            "Epoch 96/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 97/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 98/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 99/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 100/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 101/500\n",
            "700/700 [==============================] - 0s 426us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 102/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 103/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 104/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 105/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 106/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 107/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 0.9600\n",
            "Epoch 108/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 109/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9700\n",
            "Epoch 110/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9700\n",
            "Epoch 111/500\n",
            "700/700 [==============================] - 0s 388us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 112/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 113/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 114/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 115/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 116/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 117/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 118/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 119/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 120/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 121/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 122/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 123/500\n",
            "700/700 [==============================] - 0s 423us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 124/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 125/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 126/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 127/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 128/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 129/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 130/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 131/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 132/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 133/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 134/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 135/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 136/500\n",
            "700/700 [==============================] - 0s 417us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 137/500\n",
            "700/700 [==============================] - 0s 420us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 138/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 139/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 140/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 141/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 142/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 143/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9700\n",
            "Epoch 144/500\n",
            "700/700 [==============================] - 0s 420us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9700\n",
            "Epoch 145/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 146/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 147/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 148/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 149/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 150/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 151/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 152/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 153/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 154/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 155/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 156/500\n",
            "700/700 [==============================] - 0s 419us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 157/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 158/500\n",
            "700/700 [==============================] - 0s 420us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 159/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 160/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 161/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 162/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 163/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 164/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9733\n",
            "Epoch 165/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 166/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 167/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 168/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 169/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 170/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 171/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 172/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 173/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 174/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 175/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 176/500\n",
            "700/700 [==============================] - 0s 421us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 177/500\n",
            "700/700 [==============================] - 0s 425us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 178/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 179/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 180/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 181/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 182/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 183/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 184/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 185/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 186/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0041 - mean_absolute_error: 0.0041 - acc: 0.9700 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 187/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 188/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 189/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 190/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 191/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 192/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 193/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 194/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 195/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 196/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 197/500\n",
            "700/700 [==============================] - 0s 381us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 198/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 199/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 200/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 201/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 202/500\n",
            "700/700 [==============================] - 0s 426us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 203/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 204/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 205/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 206/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 207/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 208/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 209/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 210/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 211/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 212/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 213/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 214/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 215/500\n",
            "700/700 [==============================] - 0s 386us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 216/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 217/500\n",
            "700/700 [==============================] - 0s 416us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 218/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 219/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 220/500\n",
            "700/700 [==============================] - 0s 421us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 221/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 222/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 223/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 224/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 225/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 226/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 227/500\n",
            "700/700 [==============================] - 0s 417us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 228/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 229/500\n",
            "700/700 [==============================] - 0s 388us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 230/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 231/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 232/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 233/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 234/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 235/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 236/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 237/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 238/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0043 - val_mean_absolute_error: 0.0043 - val_acc: 0.9600\n",
            "Epoch 239/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0040 - mean_absolute_error: 0.0040 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 240/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 241/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 242/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 243/500\n",
            "700/700 [==============================] - 0s 416us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 244/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 245/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 246/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 247/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 248/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 249/500\n",
            "700/700 [==============================] - 0s 431us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 250/500\n",
            "700/700 [==============================] - 0s 380us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 251/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 252/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 253/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9700\n",
            "Epoch 254/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 255/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 256/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 257/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 258/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 259/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 260/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 261/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 262/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9700\n",
            "Epoch 263/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9733\n",
            "Epoch 264/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 265/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 266/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 267/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 268/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 269/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 270/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 271/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 272/500\n",
            "700/700 [==============================] - 0s 385us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 273/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 274/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 275/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 276/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 277/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 278/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 279/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 280/500\n",
            "700/700 [==============================] - 0s 428us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 281/500\n",
            "700/700 [==============================] - 0s 433us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 282/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 283/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 284/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 285/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 286/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 287/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 288/500\n",
            "700/700 [==============================] - 0s 416us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 289/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 290/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 291/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 292/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 293/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 294/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 295/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 296/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 297/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 298/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 299/500\n",
            "700/700 [==============================] - 0s 419us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 300/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 301/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 302/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 303/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 304/500\n",
            "700/700 [==============================] - 0s 386us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 305/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 306/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 307/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 308/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 309/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 310/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 311/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 312/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 313/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 314/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 315/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 316/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 317/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 318/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 319/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9743 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 320/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9700\n",
            "Epoch 321/500\n",
            "700/700 [==============================] - 0s 429us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 322/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 323/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 324/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 325/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 326/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 327/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 328/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 329/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 330/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 331/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 332/500\n",
            "700/700 [==============================] - 0s 420us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 333/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 334/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 335/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 336/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 337/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 338/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 339/500\n",
            "700/700 [==============================] - 0s 413us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 340/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 341/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 342/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 343/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 344/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 345/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 346/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 347/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 348/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 349/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 350/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9671 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 351/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 352/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 353/500\n",
            "700/700 [==============================] - 0s 407us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 354/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 355/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 356/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 357/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 358/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 359/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 360/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 361/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 362/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 363/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 364/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 365/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 366/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 367/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 368/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 369/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 370/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 371/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 372/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 373/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 374/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 375/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 376/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 377/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 378/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 379/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 380/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 381/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 382/500\n",
            "700/700 [==============================] - 0s 425us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 383/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 384/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 385/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 386/500\n",
            "700/700 [==============================] - 0s 432us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 387/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 388/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 389/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 390/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 391/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 392/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9600\n",
            "Epoch 393/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 394/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 395/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 396/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9700\n",
            "Epoch 397/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 398/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 399/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 400/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 401/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 402/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 403/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 404/500\n",
            "700/700 [==============================] - 0s 411us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9743 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 405/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 406/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 407/500\n",
            "700/700 [==============================] - 0s 428us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 408/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 409/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 410/500\n",
            "700/700 [==============================] - 0s 386us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 411/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 412/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 413/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 414/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 415/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 416/500\n",
            "700/700 [==============================] - 0s 378us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 417/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 418/500\n",
            "700/700 [==============================] - 0s 436us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 419/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 420/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 421/500\n",
            "700/700 [==============================] - 0s 385us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 422/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 423/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 424/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 425/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 426/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 427/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 428/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 429/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 430/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 431/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 432/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 433/500\n",
            "700/700 [==============================] - 0s 412us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 434/500\n",
            "700/700 [==============================] - 0s 418us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 435/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 436/500\n",
            "700/700 [==============================] - 0s 414us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 437/500\n",
            "700/700 [==============================] - 0s 401us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 438/500\n",
            "700/700 [==============================] - 0s 410us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 439/500\n",
            "700/700 [==============================] - 0s 395us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 440/500\n",
            "700/700 [==============================] - 0s 406us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 441/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 442/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9671 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 443/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 444/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 445/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9633\n",
            "Epoch 446/500\n",
            "700/700 [==============================] - 0s 382us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 447/500\n",
            "700/700 [==============================] - 0s 421us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 448/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 449/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 450/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9743 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9667\n",
            "Epoch 451/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0042 - val_mean_absolute_error: 0.0042 - val_acc: 0.9633\n",
            "Epoch 452/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 453/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 454/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 455/500\n",
            "700/700 [==============================] - 0s 427us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 456/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 457/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 458/500\n",
            "700/700 [==============================] - 0s 391us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 459/500\n",
            "700/700 [==============================] - 0s 385us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 460/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 461/500\n",
            "700/700 [==============================] - 0s 409us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 462/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 463/500\n",
            "700/700 [==============================] - 0s 386us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 464/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 465/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 466/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 467/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 468/500\n",
            "700/700 [==============================] - 0s 400us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 469/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 470/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 471/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 472/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 473/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 474/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9700 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 475/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 476/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9600\n",
            "Epoch 477/500\n",
            "700/700 [==============================] - 0s 382us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9686 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9733\n",
            "Epoch 478/500\n",
            "700/700 [==============================] - 0s 387us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 479/500\n",
            "700/700 [==============================] - 0s 405us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 480/500\n",
            "700/700 [==============================] - 0s 396us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 481/500\n",
            "700/700 [==============================] - 0s 390us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 482/500\n",
            "700/700 [==============================] - 0s 402us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 483/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 484/500\n",
            "700/700 [==============================] - 0s 397us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 485/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 486/500\n",
            "700/700 [==============================] - 0s 408us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 487/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9700 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 488/500\n",
            "700/700 [==============================] - 0s 388us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9633\n",
            "Epoch 489/500\n",
            "700/700 [==============================] - 0s 384us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 490/500\n",
            "700/700 [==============================] - 0s 404us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9600\n",
            "Epoch 491/500\n",
            "700/700 [==============================] - 0s 392us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 492/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "Epoch 493/500\n",
            "700/700 [==============================] - 0s 403us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 494/500\n",
            "700/700 [==============================] - 0s 399us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 495/500\n",
            "700/700 [==============================] - 0s 415us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 496/500\n",
            "700/700 [==============================] - 0s 389us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9729 - val_loss: 0.0041 - val_mean_absolute_error: 0.0041 - val_acc: 0.9667\n",
            "Epoch 497/500\n",
            "700/700 [==============================] - 0s 393us/step - loss: 0.0039 - mean_absolute_error: 0.0039 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 498/500\n",
            "700/700 [==============================] - 0s 398us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 499/500\n",
            "700/700 [==============================] - 0s 386us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9714 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9667\n",
            "Epoch 500/500\n",
            "700/700 [==============================] - 0s 394us/step - loss: 0.0038 - mean_absolute_error: 0.0038 - acc: 0.9729 - val_loss: 0.0040 - val_mean_absolute_error: 0.0040 - val_acc: 0.9633\n",
            "151.52123522758484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xK9P6HQb6AF",
        "colab_type": "code",
        "outputId": "bea67155-624a-4824-870b-a19866caf1e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['mean_absolute_error'])\n",
        "plt.plot(history.history['val_mean_absolute_error'])\n",
        "plt.title('Mean Absolute Error')\n",
        "plt.ylabel('MAE')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.savefig('MAE.png',format='png',dpi=1200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZUAAAEWCAYAAACufwpNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deZxU1Znw8d9TW+87zdpAo6BsImC7\nxZUQjZpENKJCzESNkYwT3ywT3xlMJsaY+I6+k4nGxHGiUaO+iehoTDBiXDHRJCJgEFlEWmjWht73\npbqqnvePe7sp2upuaKq6aOr5fj716XvPPffcc4qinjrn3EVUFWOMMSYePMmugDHGmGOHBRVjjDFx\nY0HFGGNM3FhQMcYYEzcWVIwxxsSNBRVjjDFxY0HFmAQRERWRyXEu8w0R+Uo8yzQmniyomGFBRCpE\nJCgiI3ql/9398i5NUr0miUhERB5IxvH7c6QByN2/Q0Raol7Px7OO5thjQcUMJ9uBxd0rInISkJm8\n6gDwJaAeuFpE0pJcl0S4WVWzo16fi5VJRHyHktafw81vjk4WVMxw8gTOl3i3a4HHozOISJqI/FhE\ndorIfhH5bxHJcLcViMgfRKRaROrd5ZKofd8QkR+KyF9EpFlEXu7dM+p1LHHr829AFxDrC/cSEdkm\nIjUi8h8i4nH3nSwifxKRRnfbU1HlfkJEVrvbVovIJ/o4/u0i8v+i1kvdXptPRO4EzgF+7vYwfu7m\nmSoir4hInYhsEZGr+mpff0TkfBHZLSL/KiL7gEdjpbl5bxSRcveYy0VkbFQ5KiJfE5GtwNbB1MUc\nXSyomOHkbSBXRKaJiBdYBPy/XnnuAk4AZgOTgXHAbe42D84X3URgAtAO/LzX/l8ArgdGAgHgln7q\nczZQAiwDnsYJcr1dDpQBc4EFwJfd9B8CLwMFbhk/AxCRQuAF4D6gCPgJ8IKIFPVTj49R1e8Cb3Kg\np3GziGQBrwC/cdu3CPgvEZl+OGVHGQ0U4ryfS2KlicgngX8HrgLGADtw3q9olwGnA4OthzmKWFAx\nw013b+UCYDOwp3uD23NYAnxLVetUtRn4Pzhfnqhqrao+q6pt7rY7gfN6lf+oqn6oqu04gWJ2P3W5\nFnhRVetxvqgvEpGRvfLc7dZlJ3AvB4bvunC+eMeqaoeqvuWmfwbYqqpPqGpIVZ8EPiB2L+hwfRao\nUNVH3bL/DjwLXNnPPveJSEPU64dR2yLA91W1032/YqVdAzyiqu+qaidwK3Bmrzmwf3ffo3bMsGdB\nxQw3T+D0Jq6j19AXUIwzx7K2+0sQ+KObjohkisgvRGSHiDQBfwby3V5Pt31Ry21AdqxKuENqVwK/\nBlDVvwE73bpF2xW1vAPoHvr5F0CAd0Rko4h092DGuvnotd+4WPU4TBOB06ODBM6X/uh+9vm6quZH\nvb4Xta1aVTt65e+ddlB7VLUFqOXg9kS/R2aYs6BihhVV3YEzYX8J8Ntem2twhrRmRH0J5qlqd2D4\nNnAicLqq5gLnuukyiKpcDuTiDB/tc+cQxvHxIbDxUcsTgL1uO/ap6o2qOhb4qlvOZHf7xF5lTCCq\nRxallYNPVOgdHHrfgnwX8KdeQSJbVW/qt6V9i3WL895pB7XHHYIr4uD22K3SjyEWVMxwdAPwSVVt\njU5U1QjwEHBP9zCUiIwTkU+7WXJwgk6DO3fx/SOow7XAI8BJOENks4GzgJPds9K6/W/3BIHxwDeA\np9x6XRl1kkA9zhdrBFgBnCAiX3An3K/GmWv4Q4w6rAPOFZEJIpKHM7QUbT9wXNT6H9yy/0FE/O7r\nVBGZNuh3YWBPAteLyGz37Lj/A6xS1YoEHtMkkQUVM+yo6kequqaPzf8KlANvu0Ncr+L0TsCZ08jA\n6dG8jTM0dthEZBwwH7jX7XF0v9a6ZUb3Vn4PrMUJAC8AD7vppwKrRKQFWA58Q1W3qWotztzHt3GG\nif4F+Kyq1sR4H17BCVLr3WP0Djw/BRa6Z7rd584jXYgzx7QXZ6jvbqC/U6G7zx7rfq09pDfpQB1f\nBb6HM3dTCRzvHt8co8Qe0mWMMSZerKdijDEmbiyoGGOMiRsLKsYYY+LGgooxxpi4SekbuI0YMUJL\nS0uTXQ1jjBlW1q5dW6OqxbG2pXRQKS0tZc2avs5MNcYYE4uI9L7rQw8b/jLGGBM3FlSMMcbEjQUV\nY4wxcZPScyqxdHV1sXv3bjo6et981QxWeno6JSUl+P3+ZFfFGJNgFlR62b17Nzk5OZSWluI8nsMc\nCVWltraW3bt3M2nSpGRXxxiTYDb81UtHRwdFRUUWUOJERCgqKrKenzEpwoJKDBZQ4sveT2NShwWV\nQWjtDLGvsYOI3eHZGGMOYkFlENqCIaqaO0hETKmtrWX27NnMnj2b0aNHM27cuJ71YDB4SGVcf/31\nbNmyJf6VM8aYAdhE/SCI+/RZ51k08R3aKSoqYt26dQDcfvvtZGdnc8sttxyUR1VRVTye2L8JHn30\n0bjWyRhjDpX1VAahe4pgKAe/ysvLmT59Otdccw0zZsygsrKSJUuWUFZWxowZM7jjjjt68p599tms\nW7eOUChEfn4+S5cu5eSTT+bMM8+kqqpqCGttjEk11lPpxw+e38imvU0fSw9FInR2RcgM+DjcOejp\nY3P5/udmDKo+H3zwAY8//jhlZWUA3HXXXRQWFhIKhZg3bx4LFy5k+vTpB+3T2NjIeeedx1133cU/\n//M/88gjj7B06dJBHd8YYwZiPZVB6Y4kQztRf/zxx/cEFIAnn3ySuXPnMnfuXDZv3symTZs+tk9G\nRgYXX3wxAKeccgoVFRVDVV1jTAqynko/+upRNLQF2VnXxgmjckj3e4esPllZWT3LW7du5ac//Snv\nvPMO+fn5fPGLX4x5LUggEOhZ9nq9hEKhIamrMSY1WU9lEJLTTzlYU1MTOTk55ObmUllZyUsvvZTE\n2hhjjMN6KoPQfTGfJvE6lblz5zJ9+nSmTp3KxIkTOeuss5JWF2OM6SbJ/GJMtrKyMu39kK7Nmzcz\nbdq0fvdr7uhie00rxxdnk5VmcflQHMr7aowZHkRkraqWxdpmw1+DcDQMfxljzNHIgsogHA3DX8YY\nczSyoHIELKQYY8zBEhpUROQiEdkiIuUi8rEr7kQkTUSecrevEpFSN/0CEVkrIu+7fz8Ztc8pbnq5\niNwnbrdBRApF5BUR2er+LUhUu7zhdoql0XoqxhjTS8KCioh4gfuBi4HpwGIRmd4r2w1AvapOBu4B\n7nbTa4DPqepJwLXAE1H7PADcCExxXxe56UuB11R1CvCau54Q3q5WxkgdRCKJOoQxxgxLieypnAaU\nq+o2VQ0Cy4AFvfIsAB5zl58B5ouIqOrfVXWvm74RyHB7NWOAXFV9W51uwuPAZTHKeiwqPf7EfdvU\ngooxxkRLZFAZB+yKWt/tpsXMo6ohoBEo6pXnCuBdVe108+/uo8xRqlrpLu8DRsWqlIgsEZE1IrKm\nurr68FrUzZO4oDJv3ryPXch47733ctNNN/W5T3Z2NgB79+5l4cKFMfOcf/759D59urd7772Xtra2\nnvVLLrmEhoaGQ626McYc3RP1IjIDZ0jsq4ezn9uLiTnhoaoPqmqZqpYVFxcPsl6JCyqLFy9m2bJl\nB6UtW7aMxYsXD7jv2LFjeeaZZwZ97N5BZcWKFeTn5w+6PGNM6klkUNkDjI9aL3HTYuYRER+QB9S6\n6yXAc8CXVPWjqPwlfZS53x0ew/2buHu8i3u/Lw3HveiFCxfywgsv9DyQq6Kigr179zJnzhzmz5/P\n3LlzOemkk/j973//sX0rKiqYOXMmAO3t7SxatIhp06Zx+eWX097e3pPvpptu6rll/ve//30A7rvv\nPvbu3cu8efOYN28eAKWlpdTU1ADwk5/8hJkzZzJz5kzuvffenuNNmzaNG2+8kRkzZnDhhRcedBxj\nTOpJ5OXgq4EpIjIJ54t/EfCFXnmW40zE/w1YCLyuqioi+cALwFJV/Ut3ZlWtFJEmETkDWAV8CfhZ\nr7Lucv9+/Fv3cL24FPa9/7FkbyQEoXayPOng8x9emaNPgovv6nNzYWEhp512Gi+++CILFixg2bJl\nXHXVVWRkZPDcc8+Rm5tLTU0NZ5xxBpdeemmfz39/4IEHyMzMZPPmzaxfv565c+f2bLvzzjspLCwk\nHA4zf/581q9fz9e//nV+8pOfsHLlSkaMGHFQWWvXruXRRx9l1apVqCqnn3465513HgUFBWzdupUn\nn3yShx56iKuuuopnn32WL37xi4f3nhhjjhkJ66m4cyQ3Ay8Bm4GnVXWjiNwhIpe62R4GikSkHPhn\nDpyxdTMwGbhNRNa5r5Hutn8CfgmUAx8BL7rpdwEXiMhW4FPuemJIYq+pjx4C6x76UlW+853vMGvW\nLD71qU+xZ88e9u/f32cZf/7zn3u+3GfNmsWsWbN6tj399NPMnTuXOXPmsHHjxpi3zI/21ltvcfnl\nl5OVlUV2djaf//znefPNNwGYNGkSs2fPBuzW+saYBN9QUlVXACt6pd0WtdwBXBljvx8BP+qjzDXA\nzBjptcD8I6zywfroUWhXB1K9mda0seQVxTwf4IgsWLCAb33rW7z77ru0tbVxyimn8Ktf/Yrq6mrW\nrl2L3++ntLQ05q3uB7J9+3Z+/OMfs3r1agoKCrjuuusGVU63tLS0nmWv12vDX8akuKN6ov5o1T1R\nLyTmlOLs7GzmzZvHl7/85Z4J+sbGRkaOHInf72flypXs2LGj3zLOPfdcfvOb3wCwYcMG1q9fDzi3\nzM/KyiIvL4/9+/fz4osv9uyTk5NDc3Pzx8o655xz+N3vfkdbWxutra0899xznHPOOfFqrjHmGGK3\n2B0ESeApxd0WL17M5Zdf3jMMds011/C5z32Ok046ibKyMqZOndrv/jfddBPXX38906ZNY9q0aZxy\nyikAnHzyycyZM4epU6cyfvz4g26Zv2TJEi666CLGjh3LypUre9Lnzp3Lddddx2mnnQbAV77yFebM\nmWNDXcaYj7Fb3w/i1veoQuU6mv0jyCke339eA9it7405ltit7+NNhAhiV9QbY0wvFlQGKYIkbE7F\nGGOGKwsqMRzKkKDiQaynckhSeYjVmFRjQaWX9PR0amtrB/wijCAWVA6BqlJbW0t6enqyq2KMGQJ2\n9lcvJSUl7N69m4FuNtnVUAUi+OtDQ1Sz4Ss9PZ2SkpKBMxpjhj0LKr34/X4mTZo0YL7Vd95MsTRS\n+p3+7/xrjDGpxIa/BqnTl0tG+OMXChpjTCqzoDJIQX8umRELKsYYE82CyiCFArlkaZs9UtgYY6JY\nUBmkcFoeHhQ6G5NdFWOMOWpYUBkkTctz/rbb43aNMaabBZVBkowCAIIt9UmuiTHGHD0sqAySJ8sJ\nKu1NtUmuiTHGHD0sqAySP9MJKp0tdUmuiTHGHD0sqAySP7sQgC4LKsYY08OCyiCl5Tg9lVCrzakY\nY0y3hAYVEblIRLaISLmILI2xPU1EnnK3rxKRUje9SERWikiLiPw8Kn+OiKyLetWIyL3ututEpDpq\n21cS2bbc3HxC6iHUZkHFGGO6JezeXyLiBe4HLgB2A6tFZLmqborKdgNQr6qTRWQRcDdwNdABfA+Y\n6b4AUNVmYHbUMdYCv40q7ylVvTlBTTpIQVYajWQRsaBijDE9EtlTOQ0oV9VtqhoElgELeuVZADzm\nLj8DzBcRUdVWVX0LJ7jEJCInACOBN+Nf9YHlZ/pp1CzosOtUjDGmWyKDyjhgV9T6bjctZh5VDQGN\nQNEhlr8Ip2cS/eCTK0RkvYg8IyIJfXh8ms9Li2Tj7WxK5GGMMWZYGc4T9YuAJ6PWnwdKVXUW8AoH\nekAHEZElIrJGRNYM9MyUgbR7c/AFLagYY0y3RAaVPUB0b6HETYuZR0R8QB4w4NWEInIy4FPVtd1p\nqlqrqp3u6i+BU2Ltq6oPqmqZqpYVFxcfalti6vTlkBayoGKMMd0SGVRWA1NEZJKIBHB6Fst75VkO\nXOsuLwRe10N7oPliDu6lICJjolYvBTYPqtaHoTOQT1bYbihpjDHdEnb2l6qGRORm4CXACzyiqhtF\n5A5gjaouBx4GnhCRcqAOJ/AAICIVQC4QEJHLgAujzhy7Crik1yG/LiKXAiG3rOsS1bZuXelF5LS0\nQLgLvP5EH84YY456CX2csKquAFb0SrstarkDuLKPfUv7Kfe4GGm3ArcOtq6DEckY4Sy01ULO6KE8\ntDHGHJWG80R90kmWE1S6mvYnuSbGGHN0sKByBHy5IwForduX5JoYY8zRwYLKEfDnjgKgvcF6KsYY\nAxZUjkhmvhNUgo0WVIwxBiyoHJGcgiK61EuouSrZVTHGmKOCBZUjUJCVTj05aJs9/dEYY8CCyhEp\nzApQq7l422qSXRVjjDkqWFA5Aul+L/WSh7/DeirGGAMWVI5Yqy+ftKA9UtgYY8CCyhHrCBSSHbJn\nqhhjDFhQOWKh9CIytA26+nyemDHGpAwLKkdIM91nitlkvTHGWFA5UpLt3Kol0nxkD/wyxphjgQWV\nIxRw7//VUm/3/zLGGAsqRyjdvVVLqwUVY4yxoHKksgud56h0NlhQMcYYCypHqLCgiE7102VzKsYY\nY0HlSBXlpFNLDrRaUDHGGAsqRyg/w0+d5uKxm0oaY4wFlSPl8Qjt3hy8waZkV8UYY5IuoUFFRC4S\nkS0iUi4iS2NsTxORp9ztq0Sk1E0vEpGVItIiIj/vtc8bbpnr3NfI/soaCh2+HPxdFlSMMSZhQUVE\nvMD9wMXAdGCxiEzvle0GoF5VJwP3AHe76R3A94Bb+ij+GlWd7b66n5DVV1kJF/bnkh5uGarDGWPM\nUSuRPZXTgHJV3aaqQWAZsKBXngXAY+7yM8B8ERFVbVXVt3CCy6GKWdbgq3/oIum5ZEYsqBhjTCKD\nyjhgV9T6bjctZh5VDQGNQNEhlP2oO/T1vajAcUhlicgSEVkjImuqq+NzxpakF5BBJ4SCcSnPGGOG\nq+E4UX+Nqp4EnOO+/uFwdlbVB1W1TFXLiouL41Ihb1Y+AG3NdgaYMSa1JTKo7AHGR62XuGkx84iI\nD8gD+v1mVtU97t9m4Dc4w2yDKiteAlkFADTW2Z2KjTGpLZFBZTUwRUQmiUgAWAQs75VnOXCtu7wQ\neF1Vta8CRcQnIiPcZT/wWWDDYMqKp4zcQgCa6u0CSGNMavMlqmBVDYnIzcBLgBd4RFU3isgdwBpV\nXQ48DDwhIuVAHU7gAUBEKoBcICAilwEXAjuAl9yA4gVeBR5yd+mzrETLynOmblqb7LHCxpjUlrCg\nAqCqK4AVvdJui1ruAK7sY9/SPoo9pY/8fZaVaDn5IwBot6BijElxw3Gi/qiTV+BM+He1WlAxxqQ2\nCypxkJ7jzKmEWuuTXBNjjEkuCyrx4EsniA/taEx2TYwxJqksqMSDCK2eHDwWVIwxKc6CSpx0erPx\n2k0ljTEpzoJKnAT9uaR1NSe7GsYYk1QWVOIkFHBuKjlE11saY8xRyYJKnEQCueTQSktnKNlVMcaY\npLGgEi8ZeeRJKw1tXcmuiTHGJI0FlTjxpueRTQeN7RZUjDGpy4JKnPgzc0mTLhqbW5NdFWOMSRoL\nKnESyMoDoKXZrqo3xqQuCypxku4GlbbmhiTXxBhjkqffoCIiuf1smxD/6gxfGTnO0x87W+2qemNM\n6hqop/JG94KIvNZr2+/iXpthzJ/hxN9gmwUVY0zqGiioSNRyYT/bTJoTVKTTrqo3xqSugYKK9rEc\naz21BbIB8HS1JLkixhiTPAM9+XGkiPwzTq+kexl3vTihNRtu0nIA8ATtlGJjTOoaKKg8BOTEWAb4\nZUJqNFy5QcVrN5U0xqSwfoOKqv6gr20icupAhYvIRcBPAS/wS1W9q9f2NOBxnOfO1wJXq2qFiBQB\nzwCnAr9S1Zvd/JnA/wDHA2HgeVVd6m67DvgPYI9b/M9VdegCnzv85QtZT8UYk7oO6zoVEZkuIj8U\nkXLggQHyeoH7gYuB6cBiEZneK9sNQL2qTgbuAe520zuA7wG3xCj6x6o6FZgDnCUiF0dte0pVZ7uv\noe1JeTwECeANdwzpYY0x5mgy0PAXIlIKLHZfXcBEoExVKwbY9TSgXFW3ueUsAxYAm6LyLABud5ef\nAX4uIqKqrcBbIjI5ukBVbQNWustBEXkXKBmoDUMlIl40bPf+MsakroEufvwb8AJO8LlCVU8Bmg8h\noACMA3ZFre9202LmUdUQ0AgUHUrFRSQf+BwQff3MFSKyXkSeEZHxfey3RETWiMia6urqQznUIYuI\nF42E41qmMcYMJwMNf+3HmZwfxYGzvZJ+KrGI+IAngfu6e0LA80Cpqs4CXgEei7Wvqj6oqmWqWlZc\nHN8T2MLig7A9T8UYk7r6DSqqehlwErAWuF1EtgMFInLaIZS9B4juLZRwYBL9Y3ncQJGHM2E/kAeB\nrap6b1Rda1W10139Jc7k/5BS8aIRCyrGmNQ14ES9qjaq6qOqeiFwBnAbcI+I7Bpg19XAFBGZJCIB\nYBGwvFee5cC17vJC4HUd4Hm8IvIjnODzzV7pY6JWLwU2D1C/uFOPD7GgYoxJYQNO1EdT1f3Az4Cf\nicjEAfKGRORm4CWcU4ofUdWNInIHsEZVlwMPA0+4Z5PV4QQeAESkAsgFAiJyGXAh0AR8F/gAeFdE\n4MCpw18XkUuBkFvWdYfTtnhQ8YIFFWNMCus3qIhI755Fb5f2t1FVVwAreqXdFrXcAVzZx76lfVWr\nj/y3Arf2V5+E8/jwEqYrHMHvtacKGGNSz0A9lTNxzs56EliF3USyXypevIRp7wpbUDHGpKSBgspo\n4AKca1S+gHN68ZOqujHRFRuWvH58ROjoCpOb7k92bYwxZsgNdPZXWFX/qKrX4kzSlwNvuHMlpjd3\n+KsjGEl2TYwxJikO5Yr6NOAzOL2VUuA+4LnEVmuY8njxESEUsaBijElNA03UPw7MxJls/4GqbhiS\nWg1TKj58BAlHkn59qDHGJMVAPZUvAq3AN3BO2e1OF0BVtc9n2KckjxefRAj3f6mNMcYcswa69b2d\nwnQY1OPHS9h6KsaYlGVBI47U48NHBJtSMcakKgsq8eRxrlOx4S9jTKqyoBJPbk/Fhr+MManKgko8\niQ8fIQsqxpiUZUElnjw+vNZTMcakMAsq8eT14SNMxOZUjDEpyoJKPHl8eMV6KsaY1GVBJZ48Tk/F\nzv4yxqQqCypxJG5QiVhPxRiToiyoxJMbVEIWVIwxKcqCSjx5vXiJWE/FGJOyLKjEkXj9NqdijElp\nCQ0qInKRiGwRkXIRWRpje5qIPOVuXyUipW56kYisFJEWEfl5r31OEZH33X3uE/fWySJSKCKviMhW\n929BItsWi7gP6bKzv4wxqSphQUVEvMD9wMXAdGCxiEzvle0GoF5VJwP3AHe76R3A94BbYhT9AHAj\nMMV9XeSmLwVeU9UpwGvu+pASr3tDSeupGGNSVCJ7KqcB5aq6TVWDwDJgQa88C4DH3OVngPkiIqra\nqqpv4QSXHiIyBshV1bdVVYHHgctilPVYVPqQEY8fjyjhcHioD22MMUeFRAaVccCuqPXdblrMPKoa\nAhqBogHK3N1HmaNUtdJd3geMilWAiCwRkTUisqa6uvpQ2nHovM7jaTTUFd9yjTFmmDgmJ+rdXkzM\nMShVfVBVy1S1rLi4OK7H9XicoBIJh+JarjHGDBeJDCp7gPFR6yVuWsw8IuID8oDaAcos6aPM/e7w\nWPcwWdWgaz5YPrenErGgYoxJTYkMKquBKSIySUQCwCJgea88y4Fr3eWFwOtuLyMmd3irSUTOcM/6\n+hLw+xhlXRuVPmQ8Xr+zYD0VY0yK6vcZ9UdCVUMicjPwEuAFHlHVjSJyB7BGVZcDDwNPiEg5UIcT\neAAQkQogFwiIyGXAhaq6Cfgn4FdABvCi+wK4C3haRG4AdgBXJaptfbHhL2NMqktYUAFQ1RXAil5p\nt0UtdwBX9rFvaR/pa4CZMdJrgflHUN0jJu5EPRGbqDfGpKZjcqI+WbqDilpPxRiToiyoxJF4A4BN\n1BtjUpcFlTjydA9/WU/FGJOiLKjEUffZX2pzKsaYFGVBJY6sp2KMSXUWVOLJPaUYtaBijElNFlTi\nyWNnfxljUpsFlXjquU7FgooxJjVZUImn7uGviN363hiTmiyoxJPHeirGmNRmQSWePHabFmNMarOg\nEk8er/PXhr+MMSnKgko8eew6FWNMarOgEk8e54p6setUjDEpyoJKPLk9FbGJemNMirKgEk/dcypq\ncyrGmNRkQSWeenoqdvaXMSY1WVCJp+6gYj0VY0yKsqAST+6t7+3sL2NMqkpoUBGRi0Rki4iUi8jS\nGNvTROQpd/sqESmN2narm75FRD7tpp0oIuuiXk0i8k132+0isidq2yWJbFtM7pyK9VSMManKl6iC\nRcQL3A9cAOwGVovIclXdFJXtBqBeVSeLyCLgbuBqEZkOLAJmAGOBV0XkBFXdAsyOKn8P8FxUefeo\n6o8T1aYB9Qx/WU/FGJOaEtlTOQ0oV9VtqhoElgELeuVZADzmLj8DzBcRcdOXqWqnqm4Hyt3yos0H\nPlLVHQlrweGyORVjTIpLZFAZB+yKWt/tpsXMo6ohoBEoOsR9FwFP9kq7WUTWi8gjIlIQq1IiskRE\n1ojImurq6sNpz8DcoOKx61SMMSlqWE7Ui0gAuBT4n6jkB4DjcYbHKoH/jLWvqj6oqmWqWlZcXBzf\nivWcUmw9FWNMakpkUNkDjI9aL3HTYuYRER+QB9Qewr4XA++q6v7uBFXdr6phVY0AD/Hx4bLEEyGM\nx+ZUjDEpK5FBZTUwRUQmuT2LRcDyXnmWA9e6ywuB11VV3fRF7tlhk4ApwDtR+y2m19CXiIyJWr0c\n2BC3lhyGCF57nLAxJmUl7OwvVQ2JyM3AS4AXeERVN4rIHcAaVV0OPAw8ISLlQB1O4MHN9zSwCQgB\nX1N1Zr9FJAvnjLKv9jrk/xWR2YACFTG2D4mweFGbUzHGpKiEBRUAVV0BrOiVdlvUcgdwZR/73gnc\nGSO9FWcyv3f6PxxpfeNBxQdhu02LMSY1DcuJ+qNZRLyoTdQbY1KUBZU4U/Hare+NMSnLgkqcRTw+\nsLO/jDEpyoJKnKn48GiYUO6cIYUAABgeSURBVDiS7KoYY8yQs6ASbx4vXsJ0hCyoGGNSjwWVOFOP\nDx9hOrpsst4Yk3osqMSbx4ePCO1BCyrGmNRjQSXOxOPDS5jOkAUVY0zqsaASbz3DXzanYoxJPRZU\n4s3rw0uEdptTMcakIAsqcebx2kS9MSZ1WVCJN38mWdJhw1/GmJRkQSXONKOQAppt+MsYk5IsqMSZ\nJ6uIfGmhrdNu1WKMST0JvfV9KvLnFJMm7TS3tiW7KsYYM+SspxJngRznUS+dzbVJrokxxgw9Cypx\nJplOUAm11iS5JsYYM/QsqMRbZiEA2mo9FWNM6rGgEm8ZTlCR9vokV8QYY4ZeQoOKiFwkIltEpFxE\nlsbYniYiT7nbV4lIadS2W930LSLy6aj0ChF5X0TWiciaqPRCEXlFRLa6fwsS2bY+ZY8CIK2jOimH\nN8aYZEpYUBERL3A/cDEwHVgsItN7ZbsBqFfVycA9wN3uvtOBRcAM4CLgv9zyus1T1dmqWhaVthR4\nTVWnAK+560Mvq5gu8ZPXWZmUwxtjTDIlsqdyGlCuqttUNQgsAxb0yrMAeMxdfgaYLyLipi9T1U5V\n3Q6Uu+X1J7qsx4DL4tCGw+fx0BgYTWFoP2z7E2z+Q1KqYYwxyZDIoDIO2BW1vttNi5lHVUNAI1A0\nwL4KvCwia0VkSVSeUara3T3YB4yKRyMGoyNzLKMi1fD4pfDUNcmqhjHGDLnhePHj2aq6R0RGAq+I\nyAeq+ufoDKqqIqKxdnYD0RKACRMmJKSCmSMnUVL3QULKNsaYo1kieyp7gPFR6yVuWsw8IuID8oDa\n/vZV1e6/VcBzHBgW2y8iY9yyxgBVsSqlqg+qapmqlhUXFw+6cf3JL51NsTQeSAi2JuQ4xhhztElk\nUFkNTBGRSSISwJl4X94rz3LgWnd5IfC6qqqbvsg9O2wSMAV4R0SyRCQHQESygAuBDTHKuhb4fYLa\nNSDPtM8enNC8LzkVMcaYIZaw4S9VDYnIzcBLgBd4RFU3isgdwBpVXQ48DDwhIuVAHU7gwc33NLAJ\nCAFfU9WwiIwCnnPm8vEBv1HVP7qHvAt4WkRuAHYAVyWqbQPKH0+XpOHXTgA+/GgrJxQdn7TqfEyo\nE8QL3uE4+mmMOZqJ0zFITWVlZbpmzZqBMw7Ce68/zcl/vhGA3x1/BzPDm0mr3cT4b/8ZqjZD3XaY\neklCjj2g2/Ng8qfgi88m5/jGmGFNRNb2uqSjh/1UTZBZ865kz8xPkvvAyZy3/R4KIs4V9pEH5+HZ\n+66T6bv74I274MyvQfbI+Bw4EgbxgNObi7HdfXhY+avxOZ4xxkSx27QkiIgwbuQInp7yY4LhA73B\nnoAC8PgC+Mu98OrtR3awnaugq9153VHolNmXNrvRpTEmcSyoJNhZn/wM53Xewz8Fv84b4ZN50xPV\nY9y1yvnbXAmV653gEApCRxNs+C08/SXobDmQv6MR3nkIasoh7D4EbPdaeORCuHM0/PpKJ+2Nu/uu\nUFPUCXhHMvT5wrfhz/9xaHkP5zjBVlj1CydAJlJbHdRtS+wx+vOLc2Hlv/efJxI50LM0x56ORqg6\n9i49sOGvBJs6Opc/3vJpXnh/Fvuzb+S+18qpa6hnXdoS0sQNDB+97rxi2fk2nHGTc0+x578J4U7I\nGev8PW8pbPrdgbwVbzp/xQN73nW+NE9a6KS11kB6HjRF3T6mo9EJaI9eDFc9AVnFgELxVNj8PNRu\nhVO/4uwXrW4brP6ls5yWB74AzFrkrPvTD+RThY2/hZe+C59/CCad0/+bFWyF3y6BD/4AO/4KY06G\ns74BHm//+4W7IBICjw+CLZAxwG3ffnM1fOie3/FvVeBLO7Dt/Wfg70/A534Kr9wGZV+G484/eH9V\n54y+3DFQ/aHzNy3HCfgjpjj563eAPyP2sGZLFVS+57zm3dp3PX+9ECJdcO3zsberQtUm59+rr/do\nw7PQUg1n/GPfx+lL9Yfwzi8gPR9a9sOlP3PSu4dWwyHn85M/vu8yDlV7vfPvn1dyePup9j3UGyvv\n81+HaZfClAti59nzrvN+BjIHLq+9AfZvgNKzB87bVuf8f9u83PkBufBhePnf4N3H4ZIfw2k3Hlob\nuoW7oGYrjOp956vks4n6BE3U96WjK8xfP6ph+bs72PP+m5yWVkGZbuSMibmkVzrDWM2aTqbfgxRO\nwlu14cDOGQVEcsfj2b/+4EILSqG+IvYBJ5zpBIsPX4IRJ0DVRlDn16+OnoXsW//xfbxpTtDqNvMK\n8GU4X5IagfJXoGFn7OONnOF8MRRNhurNBwfLSec5gSJ7JATbnIAz8wrwBmDHX2Dryx8vL3s0jDsF\nQh0wbq5zkkPTHucYky+AirectkVCTjBp2u3cKfr8W50vKgCvH/LGO1/8VZvgtR8cKN+XAZ/5T2db\ncyW8+C8HH9/jh0/8L5j4Cec/cXouVH8Af/3Zwe2a+yV49gZn/VO3w5v3OHW68IfO+7/7HfhoJZz9\nLaetax5x8s79EuSOg/zuC3EF0rKdY7z+Iyep8HgYOwdOvBj8mVDzITTtdd7f7X+G0bOc/Sed63zZ\ntNfBuifhhE/D2kedMj7/S6c+jbudf9tQBzTvd46/9+8QDsLY2c77FHZ7y7+7CRp2HGinPwuKT4Sy\n6533/0//ATv/CmPnOl/EeSXOZxEgZzRkFjn/1jUfwu7VBz5b21bCxLOg9BxY9d+w6x1orXYC42f+\nE/a9D2sfg5Ovdv7ti6eCLx06m2Hn32D90zDvO85ZjC9/1/lCzh7d87lm3Fx4+7/hg+edwFd2PUye\n7wTz577q5DnnFueHy6U/c35UhDqcHwK/vgJGnAifvccJ6BkF7ueq0vk3GzPL+ff2+uGpf4CaLU55\np/8jTLnQ+fHnS4fOJieQ5JU4//YvfNt5/7vrWDDJ+RyHg85nsOzLcPw85z3ILYGOBudHR8mpzmez\n+8dJ8z7nB+KKW5yANuVCmH+bU3aoE0ZOd35o7t8EuWNh1tXOe9vZ5PxAbdzlfD73b4KCiU6eQehv\not6CyhAHlW6RiLLkibXUtnayubKJjq4IaT5BQ0GC+MlO86EaZl7WDmaNL6Szdgc7s+fy6keN/LTw\nOYpGlbClsp7mwpk83TSLypoaRoarWDqnixOr/sjoXD/hmm14/elIpIugPwdv4y68nfWExcf6cClz\nPOUAaOFxfOSZxPjgR+wfcQbjpAbPiRdT7ymkYNMTyM6/QejAcNTezKlUnfRVTqhaQaDsS/jCHfDb\nqF9avnTnP2m3s75JZNPziD8NqfnQ+Q/QB80dR23pZymqfw9pq4XmSkJZo5GG7Xg17GQqPN75ggg2\nQ3o+XePK0MoNBNr2OV+Kjbv6LB9As4r5dv0VXOhdy6fTNiCh2ENt+ul/R1b998FfrNFGzXS+MMPB\nj2/z+Jxg2TUEj5Xu70fFkSo9B60tR5r7uUHqyOnOj4xgS995+iJe5ws/Pd/pafd3nMMq1+MEre1/\nik95CdJVPAN/9cbD3k89fmTSuU6w7mwa3MEH00NyWVDpQzKDSrTNlU28tnk/DW1d/OWjWkZkBwiG\nIqzf3Uh7l/NFWpyThs8jTB2dw8otB99W/6Rxeby/p/GgtGljcvlwfzPTx+QyZ0I+T76zk1A4zBVz\nxvG79yoJReCMwjamTzmOLVVt/GX7gQ/mzHG5AGzY08SMsbmcP6WQnHQ/Wd4Qb2zcyWs7wj15x+Sl\n87mTx1KgjfxsVT3hYAc3fGI8Ez215HjaeV9OZFd9OyveryTd7+XKqWlMylHyPa0EC08ks6uOQKQD\ncsfhlwivVAT5zaqdnHtCMQtnFRFReHZ9DWu37uZU/3ZGTTuTtMw8tuyp5tKJIf5Sm8WLm+vxEWLO\nqAC14QzyOvey5NQCPKNm0NrcSK63g5JAK4GuJkJpBfzwbyHe3Ol82Qfo4qLRzXxregvt3mwa/KP4\n5p+Umek1rG0rZvqoTL5xagYZbXsJiR9vsImgpLFs31h++14VxxVl8o+TG5heGGFjvY+G5iamTjmB\nd+qy2L6vhstHVZEuIbJ9EeqKT6V431sECsZy11/beLcqTCvpLJiaxaWjG/BkFlLRpEzM6mRKaSkb\nGvw89PLfuWZykNziEvK0iQJfJ95xswnWVPBRZBQvbwsS8Pup2LOP8Wkt5BSNIa+lgvHFeXQ2VxMq\nPJE126r5wpQuRo8/ntV1WVTV1nLqhDwa9myhck8Fs087j462ZiZ0VZBGkE5Jp92TRWvYy/LacTyy\nah/njerghvNOICN/JN6aD8n3dqIeL/+5IYvtdZ3Mn1rM06+9zShPAzecMZZphZDjhwKa6FQf7b48\ndvomctP/bKGNdH59kZc16zcwdvIsflc1it317WR7gtx64n4mTppCTVuYsHgJNVYysmEd/owcvBm5\ntGaWUOMbTc6eNwnmjOePjRP5ZFYF4bR8gg2VFBaPoqT9A/blz+WnG9PxNO3lhiktFGV4kMadrOma\nREb9h5wwdQZ15DOydQt56V6aQz5CKtz/bjtn5tZxxsQcfIEAfg3R1dVJc5eHzsmfIVT5Pr5gI5ld\ndbSQxdfWH8eM3Da+cbJS5A8RCbbilzC+jjpq/GPIkBD+zBz+a207v94C3zlxD3mlcxlFDb6df+GL\nm8/gmtJmrjlvFkWhfTS0h8iXFtraOygPnEikupxpxQG0qRKv10MkayR/3NLEg1vSOH7aXL5wgjKr\n+U2yC0YSAQIN2ynPLqNu70fkFBQzMribEblZEMhk895GNu6o5JyRHQSmnE/axFPJLBgzqO8sCyp9\nOFqCSn/agiHWVNQzqySP/MwAAM+s3c0722v5/NwSKhvb+eyssVQ1d7KjppUddW2s29nA29trEaC2\nNUg4opxcko/XI7xVXsO4/AyuLCvh+ff2UtXcSW66n+OKs3h/TyMNbV1kBrxkBnxcctJo3t1Zz6a9\nTUSiPibjCzP42vmTaQ2G+fXbO9hV30ZXWMlO8zEyJ41tNQduS+P3Cjnpfk6fVIgIrHh/4LsLFGYF\naOkMEQwdmKT+/NxxrPygivq2roPyegQ+O2sslY3t7G1wekehSIT9TZ3054cLZvDaB1W8saX/5974\nPEIocuj/R7weIRxRPAJj8jLY0xC7F+TzCGdPGUFWmo+/fVRLXWuM3s4hyAx4aQuGyUnz0dwZGlSd\n++P1CKdPKmR7TSuVjR0D7zAIuek+Zo7LY3d9Ozvr4tezy/B78XuFpo6+e8bHmgy/t+eHaDe/15lz\n6oo6C1UE7rzsJL5w+uDuf2hBpQ/DIagcqe5/X/cuBHR0hfF7PXg9fU9uqmpPfoBwROkKR6hu7iQ7\nzUdWmo+A7+ATB/c3dZAR8JId8LGzro2i7ADNHSFG5qTh8x7I29EVxucRWoNhWjtDhCNKOKK0dIZo\n6QxRnJPG8cXZNLQFqWsNIiIUZgXIy/DTFY6gCu3BMGl+Dzvr2shJ9zEmL+OgunSFI5RXtdDRFSYn\n3UcwpOxrau85CW1MXgbTx+aiqjS1h+gIhVm3qwEB0v1ezjiuiLrWIHkZfpo6uthZ14ZHwCOCRwSv\nRxiXn0F7V5hwRGkLhtnX1IEAZaUFbK5soqQgk5E5aWyubMbrEXbVtTEiJ422YIjmjhCzx+czKtc5\nqSEUjrCrvp22YIgJhZmsrqijtiVIZsDHp6aPpLyqhYDXQ3NniNqWIPVtQQozA4zOS2fyyGwa2rrI\nzfDx9rZa8jMDTBmZzft7GunsitDU0cW8qSN5ZeN+RGDG2DyKsgO8t6uBkoJMfF5h6/4W8jP9fFTd\ngs8jZAR8ZPi9ZAa8zBibS35mgNbOEH/9qBafx2n/3oZ2MtN8FGYGyAh4CUecHxXTxuTw6uYqIqo0\ntnVR3dJJVsBLVpqPouwAuel+0v1edta19fSm50zIZ2ROOi2dIZav20soEmFUbjqqitfjoTMUpi0Y\npqMrTCisTCjMxO/z0NYZoig7jermTgqzAmSn+dhR18q26laKsgNcMH0UAa+HN7fWEAxF8HqEMXnp\nFGWnsbqijtG56exv6qC+rYui7ACNbV2ceXwRqrB5XxOdoQidXWEiquRnBlBVCjIDBHwe2oNhguEI\n504pprqlk3e219EZiiBAMByhKxQh3e/F5xWCoQgTCjOZN3Ukr27ez9j8DLZVt9LU3sXlc8axeV8T\nFTVtVDV3kO533suCzACdoTDVzZ0UZAbITvfR3NFFS0eIORML8Hs8TB+by47aVjZVNlHfGiQcgcZ2\n57Mwd0IB+5o6qGzooDPkBJk0n5fPzBrN02t24/MIi06dwISiQzghIQYLKn1IhaBijDHx1l9QsetU\njDHGxI0FFWOMMXFjQcUYY0zcWFAxxhgTNxZUjDHGxI0FFWOMMXFjQcUYY0zcWFAxxhgTNyl98aOI\nVOM8z34wRgCp9sQra3NqsDanhiNp80RVLY61IaWDypEQkTV9XVF6rLI2pwZrc2pIVJtt+MsYY0zc\nWFAxxhgTNxZUBu/BZFcgCazNqcHanBoS0mabUzHGGBM31lMxxhgTNxZUjDHGxI0FlUEQkYtEZIuI\nlIvI0mTXJ15E5BERqRKRDVFphSLyiohsdf8WuOkiIve578F6EZmbvJoPnoiMF5GVIrJJRDaKyDfc\n9GO23SKSLiLviMh7bpt/4KZPEpFVbtueEpGAm57mrpe720uTWf/BEhGviPxdRP7grh/T7QUQkQoR\neV9E1onIGjctoZ9tCyqHSUS8wP3AxcB0YLGITE9ureLmV8BFvdKWAq+p6hTgNXcdnPZPcV9LgAeG\nqI7xFgK+rarTgTOAr7n/nsdyuzuBT6rqycBs4CIROQO4G7hHVScD9cANbv4bgHo3/R4333D0DWBz\n1Pqx3t5u81R1dtQ1KYn9bKuqvQ7jBZwJvBS1fitwa7LrFcf2lQIbota3AGPc5THAFnf5F8DiWPmG\n8wv4PXBBqrQbyATeBU7Hubra56b3fM6Bl4Az3WWfm0+SXffDbGeJ+wX6SeAPgBzL7Y1qdwUwolda\nQj/b1lM5fOOAXVHru920Y9UoVa10l/cBo9zlY+59cIc55gCrOMbb7Q4FrQOqgFeAj4AGVQ25WaLb\n1dNmd3sjUDS0NT5i9wL/AkTc9SKO7fZ2U+BlEVkrIkvctIR+tn2DralJPaqqInJMnoMuItnAs8A3\nVbVJRHq2HYvtVtUwMFtE8oHngKlJrlLCiMhngSpVXSsi5ye7PkPsbFXdIyIjgVdE5IPojYn4bFtP\n5fDtAcZHrZe4aceq/SIyBsD9W+WmHzPvg4j4cQLKr1X1t27yMd9uAFVtAFbiDP/ki0j3D83odvW0\n2d2eB9QOcVWPxFnApSJSASzDGQL7Kcdue3uo6h73bxXOj4fTSPBn24LK4VsNTHHPHAkAi4DlSa5T\nIi0HrnWXr8WZc+hO/5J7xsgZQGNUl3rYEKdL8jCwWVV/ErXpmG23iBS7PRREJANnDmkzTnBZ6Gbr\n3ebu92Ih8Lq6g+7DgareqqolqlqK8//1dVW9hmO0vd1EJEtEcrqXgQuBDST6s53siaTh+AIuAT7E\nGYf+brLrE8d2PQlUAl0446k34IwlvwZsBV4FCt28gnMW3EfA+0BZsus/yDafjTPuvB5Y574uOZbb\nDcwC/u62eQNwm5t+HPAOUA78D5Dmpqe76+Xu9uOS3YYjaPv5wB9Sob1u+95zXxu7v6sS/dm227QY\nY4yJGxv+MsYYEzcWVIwxxsSNBRVjjDFxY0HFGGNM3FhQMcYYEzcWVIxJIBEJu3eI7X7F7a7WIlIq\nUXeUNuZoYLdpMSax2lV1drIrYcxQsZ6KMUngPufi/7rPunhHRCa76aUi8rr7PIvXRGSCmz5KRJ5z\nn4Hynoh8wi3KKyIPuc9Fedm9Qt6YpLGgYkxiZfQa/ro6alujqp4E/BznLroAPwMeU9VZwK+B+9z0\n+4A/qfMMlLk4V0iD8+yL+1V1BtAAXJHg9hjTL7ui3pgEEpEWVc2OkV6B86Csbe4NLfepapGI1OA8\nw6LLTa9U1REiUg2UqGpnVBmlwCvqPGwJEflXwK+qP0p8y4yJzXoqxiSP9rF8ODqjlsPYPKlJMgsq\nxiTP1VF//+Yu/xXnTroA1wBvusuvATdBzwO28oaqksYcDvtVY0xiZbhPWOz2R1XtPq24QETW4/Q2\nFrtp/wt4VET+N1ANXO+mfwN4UERuwOmR3IRzR2ljjio2p2JMErhzKmWqWpPsuhgTTzb8ZYwxJm6s\np2KMMSZurKdijDEmbiyoGGOMiRsLKsYYY+LGgooxxpi4saBijDEmbv4/zk4YO7eCIn0AAAAASUVO\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCR75o1XdAJ4",
        "colab_type": "code",
        "outputId": "30eff06c-cada-4cf6-f307-4d8c4b032ce4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "plt.plot(history.history['acc'])\n",
        "plt.plot(history.history['val_acc'])\n",
        "plt.title('Accuracy against Validation')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.savefig('Acc.png',format='png',dpi=1200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZ9FJIhSt3Tt",
        "colab_type": "code",
        "outputId": "63e37c0e-8252-4b3f-c868-dd296c3a9f84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        }
      },
      "source": [
        "z = np.arange(-2, 2, .1)\n",
        "zero = np.zeros(len(z))\n",
        "y = np.max([zero, z], axis=0)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "ax.plot(z, y)\n",
        "ax.set_ylim([-2.0, 2.0])\n",
        "ax.set_xlim([-2.0, 2.0])\n",
        "ax.grid(True)\n",
        "ax.set_xlabel('z')\n",
        "ax.set_title('Rectified linear unit')\n",
        "plt.savefig('ReLu.png',format='png',dpi=1200)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO3deXxU9b3/8deHXQg7Yd+FsAlVQXC7\nFVyRquBW0VZFpWhbbuu9bQXrWvW6tbXVq9biVm2tSgEREWWRxH0DCwk7EZB9lyXsST6/P+Zwf2ma\nkJA5M3NC3s/HYx45Z853zvczJzDvOds35u6IiIhUS3UBIiISDQoEEREBFAgiIhJQIIiICKBAEBGR\ngAJBREQABYJUYmaWZ2adg+njzOwtM9tpZv8wsx+Y2YwKrneEmX1UyrKOZuZmViOYf8fMrq/4u4iG\nY+V9SHxqpLoAObaY2SqgBVAA5AHvAqPdPS/O9WYBf3P35w4/5+5pRZpcEfTb1N3zg+deiafP8nD3\nCxPdRzIUfR9mNgIY6e5npq4iSQXtIUgiXBx8WJ8InATcnoQ+OwDLioTBMevw3olI2BQIkjDuvhGY\nTiwYADCz2mb2OzNbbWabzOwZMzuuyPKhZjbPzHaZ2ddmNtjM/gf4D+DJ4DDRk0FbN7MuZvYb4G7g\nqmD5TcUP+5hZdzObaWbbzWypmX2/yLKmZjYl6PML4PjyvkczyzKzkcH0CDP7KHh/35rZSjMr+s27\noZk9b2YbzGydmT1gZtWDZceb2Wwz22ZmW83sFTNrVOS1q8xsjJllA3uKh0LxQ1kVqC3LzEaaWQ/g\nGeC0YFvuKO+2kMpPgSAJY2ZtgQuB3CJPPwxkEAuJLkAbYh/mmFl/4GXgV0Aj4LvAKne/A/iQ2KGn\nNHcfXbQfd78HeBB4PVj+fLE66gEzgb8DzYHhwNNm1jNo8hSwH2gF3Bg8KmoAsBRoBjwKPG9mFiz7\nC5AfvO+TgPOBkYfLBB4CWgM9gHbAvcXWfTXwPaBRBfeEjlQbAO6+GLgF+DTYlo3+fTVyrFIgSCJM\nNrPdwBpgM3APQPDhMwr4L3ff7u67iX2QDw9edxPwgrvPdPdCd1/n7ktCqOciYsHyorvnu/s/gYnA\nlcE39MuBu919j7svAF6Ko69v3P1Zdy8I1tMKaGFmLYAhwK1BP5uBPxC8d3fPDd73AXffAjwGnFVs\n3U+4+xp33xdmbRVclxyDdCxSEmGYu88ys7OIfStvBuwA0oG6wNwiX0wNqB5MtwOmJaCeDsCAYoc/\nagB/DWqqQSy8Dvsmjr42Hp5w973B+0wDmgA1gQ1F3nu1w/0GgfE4sUNj9YNl3xZb9xriU1ptIoAC\nQRLI3d83s78AvwOGAVuBfUAvd19XwkvWUPrx+3iG5V0DvO/u5xVfEOwh5BMLo8N7I+3j6OtINRwA\nmpVyuOdBYu+xt7tvN7NhwJPF2hxpG+wJftYFdgXTLStYq4ZArqJ0yEgS7Y/AeWb2HXcvBJ4F/mBm\nzQHMrI2ZXRC0fR64wczOMbNqwbLuwbJNQOcK1jAVyDCza82sZvA4xcx6BIdPJgH3mlnd4LxC6Nfj\nu/sGYAbwezNrELy/44O9KIjtFeQBO82sDbHzKEez/i3AOuCHZlbdzG7kKE6OF7MJaGtmtSr4eqmk\nFAiSUMEH1csEJ46BMcROMn9mZruAWUC3oO0XwA3Ejq3vBN4ndrgHYodTrgiukHniKGvYTewE7nBg\nPbFDJ48AtYMmo4kdOtlI7MTvi0f7PsvpOqAWsIjY4aAJxI7jA/wGOJnY+36bWEgdrR8RC5JtQC/g\nkwrWORtYCGw0s60VXIdUQqY/kCMiIqA9BBERCcQdCGbWzswyzWyRmS00s5+X0MbM7AkzyzWzbDM7\nOd5+RUQkXGFcZZQP/MLdvzKz+sQuKZzp7ouKtLkQ6Bo8BgB/Cn6KiEhExL2H4O4b3P2rYHo3sJjY\n3adFDQVe9pjPgEZm1goREYmMUO9DMLOOxG7J/7zYojb86001a4PnNpSwjlHE7malTp06fdu3T8Ql\n4eEpLCykWrXon4pRneFSneFSneFZtmzZVndPr9CL3T2UB7HL9uYCl5WwbCpwZpH594B+Za0zIyPD\noy4zMzPVJZSL6gyX6gyX6gwPMMcr+DkeStSZWU1iY8O84u4lXT+9jtidoIe1DZ4TEZGICOMqIyN2\nh+lid3+slGZTgOuCq41OBXZ67M5NERGJiDDOIZwBXAvkmNm84LlfE4wH4+7PEBuwbAixO1T3Ersb\nVUREjkJhoZN3MJ8GdWomZP1xB4K7f0RsxMojtXHgp/H2JSJSVeVuzmPsxGzS6tTgxRGnUOxPWYQi\n2qfLRUSquEMFhTyVmcuQxz8kd0seF/dpnbC+NPy1iEhELVi3k9smZLNowy6+17sV917Si/T6tct+\nYQUpEEREImb/oQIef2854z5YQZN6tXjmh30ZfEJF/7xF+SkQREQi5MtV2xkzIZsVW/fw/X5tuWNI\nTxrWTcxJ5OIUCCIiEZB3IJ9H313Cy59+Q9vGx/G3mwZwZtdmSa1BgSAikmKZSzdzx6QcNuzazw1n\ndORXF3Sjbq3kfzwrEEREUuTbPQe5f+oiJv1zHV2apzHhltPp26FxyupRIIiIJJm7My1nI/dMWcCO\nvYf42dld+OnZXahdo3pK61IgiIgk0eZd+7lz8gJmLNpE7zYNefnGAfRs3SDVZQEKBBGRpHB3/jFn\nLfe/vYiD+YXcfmF3bjqzEzWqR+f+YAWCiEiCrdm+l9sn5fBR7lb6d2rCw5f1pnN6WqrL+jcKBBGR\nBCkodF76ZBW/nb6U6tWM+4edwA/6t6datfDHIQqDAkFEJAGWb9rNmInZfLV6BwO7pfPgpb1p3ei4\nVJd1RAoEEZEQHSoo5Jmsr/nf2bnUq12dP151IkNPbJ2Q0UnDpkAQEQlJztqd/GrCfJZs3M1FfWKD\n0TVLS9xgdGFTIIiIxGn/oQL+MGsZz36wgmZptRl3bV/O75X4wejCpkAQEYnD5yu2MXZSDiu37uGq\nfu349fd60PC45AxGFzYFgohIBezef4hH3l3C3z5bTfsmdXll5ADO6JLcwejCFkogmNkLwEXAZnc/\noYTlA4E3gZXBU5Pc/b4w+hYRSbbMJZv59Rs5bNq1n5FnduK/z89IyWB0YQvrHfwFeBJ4+QhtPnT3\ni0LqT0Qk6XYfdG597Z9Mnreers3TePrHp3NS+9QNRhe2UALB3T8ws45hrEtEJGrcnbeyN3DHh3vZ\nX7iPn5/TlZ8MOj7lg9GFLZn7OKeZ2XxgPfBLd1+YxL5FRCpk487YYHSzFm+iU8Nq/OmGM+jeMhqD\n0YXN3D2cFcX2EKaWcg6hAVDo7nlmNgR43N27lrKeUcAogPT09L7jx48Ppb5EycvLIy0temOSFKc6\nw6U6wxXFOt2d99fm8/rSgxQUwqVda3FGswM0qB+tOosbNGjQXHfvV6EXu3soD6AjsKCcbVcBzcpq\nl5GR4VGXmZmZ6hLKRXWGS3WGK2p1rtqa58P//Kl3GDPVr/rzJ75yS567R6/OkgBzvIKf40k5ZGRm\nLYFN7u5m1h+oBmxLRt8iIuVVUOi8+PFKfjdjKTWrVeOhy3oz/JR2lWLYiTCEddnpq8BAoJmZrQXu\nAWoCuPszwBXAj80sH9gHDA+STEQkEpZu3M1tE7OZv2YH5/ZozgPDetOyYZ1Ul5VUYV1ldHUZy58k\ndlmqiEikHMwv5OmsXJ7KzKV+nZo8cfVJXNynVZXZKyiq8t9JISJSQfPW7GDMhGyWbtrNJd9pzT0X\n96RpJRqMLmwKBBGpcvYdLOD3M5bywscraV6/Ds9f349zerRIdVkpp0AQkSrlk6+3MnZiDqu37+Wa\nAe0Ze2F3GtSpnIPRhU2BICJVwq79h3ho2mJe/WINHZvW5dUfncppxzdNdVmRokAQkWPezEWbuHNy\nDlt2H+Dm73bm1nMzOK7WsTXsRBgUCCJyzNqad4B7pyxkavYGuresz7PX9aNP20apLiuyFAgicsxx\nd96ct57fvLWQvAP5/Pd5Gdxy1vHUqlEt1aVFmgJBRI4p63fs487JC5i9ZDMntmvEo1f0IaNF/VSX\nVSkoEETkmFBY6Lz65WoemraEgkLnrot6MuL0jlSvVvVuMKsoBYKIVHort+5h7MRsPl+5nTO6NOWh\nS/vQvmndVJdV6SgQRKTSyi8o5IWPV/L7GcuoVaMaj17ehyv7ta2Sw06EQYEgIpXS4g27GDMxm+y1\nOzmvZwseGHYCLRpUrcHowqZAEJFK5UB+AU/NzuXprK9pVLcmT11zMkN6t9ReQQgUCCJSaXy1+lvG\nTMhm+eY8LjupDXdd1JPG9WqluqxjhgJBRCJv78F8fjd9GS9+spJWDerw4g2nMKhb81SXdcxRIIhI\npH2cu5Wxk7JZs30f157agdsGd6O+BqNLCAWCiETSzn2HePDtxbw+Zw2dmtXj9VGnMqCzBqNLJAWC\niETO9IUbuWvyArbtOcgtZx3Pred2pU5NDUaXaAoEEYmMLbtjg9G9nbOBHq0a8Pz1p9C7bcNUl1Vl\nhBIIZvYCcBGw2d1PKGG5AY8DQ4C9wAh3/yqMvkWk8nN3Jn21lvumLmLvgQJ+dUE3Rn23MzWrazC6\nZAprD+EvwJPAy6UsvxDoGjwGAH8KfopIFbduxz4em3uAnK3z6duhMY9c3ocuzdNSXVaVFEoguPsH\nZtbxCE2GAi+7uwOfmVkjM2vl7hvC6F9EKp/CQueVz7/h4XeWkF9QwL0X9+S60zpSTYPRpYzFPqND\nWFEsEKaWcshoKvCwu38UzL8HjHH3OSW0HQWMAkhPT+87fvz4UOpLlLy8PNLSov9tRnWGS3XGZ0Ne\nIS8uPMCybwvp1bQa3+9UQIdm0auzuKhuz6IGDRo01937VeS1kTup7O7jgHEA3bp184EDB6a2oDJk\nZWUR9RpBdYZNdVZMfkEh4z5cwR8/W06dGtX47RUncEXftrz//vuRqrM0UdueYUtWIKwD2hWZbxs8\nJyJVxML1OxkzMZsF63YxuFdL7hvWi+b1NRhdlCQrEKYAo83sNWInk3fq/IFI1bD/UAH/O3s5z7y/\ngsZ1a/GnH5zMhb1bpbosKUFYl52+CgwEmpnZWuAeoCaAuz8DTCN2yWkusctObwijXxGJtrnfbOe2\nCdl8vWUPl5/clrsu6kGjuhqMLqrCusro6jKWO/DTMPoSkejbcyCf305fykufrqJ1w+N46cb+nJWR\nnuqypAyRO6ksIpXbB8u2cPukHNbv3Md1p3bgV4O7k1ZbHzWVgX5LIhKKnXsPcf/bi5gwdy2d0+sx\n/ubTOKVjk1SXJUdBgSAicXt3wQbuenMh2/cc5KeDjuc/z9ZgdJWRAkFEKmzz7v3c8+ZC3lmwkV6t\nG/CXG06hV2sNRldZKRBE5Ki5OxO/Wsf9Uxex71ABtw3uxo/+Q4PRVXYKBBE5Kmu27+XXb+Tw4fKt\n9OvQmIc1GN0xQ4EgIuVSWOi8/OkqHp2+FAPuG9qLHw7ooMHojiEKBBEpU+7m3YyZmMPcb77lrIx0\n/ufSE2jbuG6qy5KQKRBEpFSHCgoZ98EKHp+1nLq1q/PY97/DpSe1IfY3r+RYo0AQkRItWLeT2yZk\ns2jDLr7XpxX3XtyL9Pq1U12WJJACQUT+xf5DBfxx1nKe/XAFTerV4s/X9uWCXi1TXZYkgQJBRP7P\nFyu3M3ZiNiu27uGqfu349ZAeNKxbM9VlSZIoEESEvAP5PPLOEv762Te0bXwcf7tpAGd2bZbqsiTJ\nFAgiVVzm0s3cMSmHDbv2c+MZnfjlBRnUraWPhqpIv3WRKurbPQe5f+oiJv1zHV2apzHhltPp26Fx\nqsuSFFIgiFQx7s60nI3cM2UBO/Ye4mdnd+GnZ3ehdg0NRlfVKRBEqpBNu/Zz1+QFzFi0id5tGvLX\nmwbQo1WDVJclEaFAEKkC3J3xc9bwwNuLOZhfyO0XduemMztRQ4PRSRGh/Gsws8FmttTMcs1sbAnL\nR5jZFjObFzxGhtGviJRt9ba9/PD5zxkzMYcerRrw7q3f5eazjlcYyL+Jew/BzKoDTwHnAWuBL81s\nirsvKtb0dXcfHW9/IlI+BYXO9FWHmPzeB1SvZjww7ASu6d9eg9FJqcI4ZNQfyHX3FQBm9howFCge\nCCKSJMs37ea2idn8c/VBzu7enAeGnUDrRseluiyJOHP3+FZgdgUw2N1HBvPXAgOK7g2Y2QjgIWAL\nsAz4L3dfU8r6RgGjANLT0/uOHz8+rvoSLS8vj7S06I8FrzrDFdU68wudt1cc4q2vD1GnBlzeyRnY\nqV7kB6OL6vYsrjLUOWjQoLnu3q9CL3b3uB7AFcBzReavBZ4s1qYpUDuYvhmYXZ51Z2RkeNRlZmam\nuoRyUZ3himKd81Z/6xf84X3vMGaqj/77V7519/5I1lkS1RkeYI5X8PM8jENG64B2RebbBs8VDZ1t\nRWafAx4NoV8RAfYdLOCPs5bx7IcrSK9fm2ev68d5PVukuiyphMIIhC+BrmbWiVgQDAeuKdrAzFq5\n+4Zg9hJgcQj9ilR5n63YxtiJ2azatper+7fj9iE9aFBHg9FJxcQdCO6eb2ajgelAdeAFd19oZvcR\n23WZAvzMzC4B8oHtwIh4+xWpynbvP8TD7yzhlc9X075JXf4+cgCnd9FgdBKfUG5Mc/dpwLRiz91d\nZPp24PYw+hKp6mYv2cQdbyxg0679jDyzE784vxvH1dKwExI/3aksUkls33OQ+95ayOR568lokcbT\nPzidk9prMDoJjwJBJOLcnbeyN3DvlIXs3n+IW8/tyk8GdqFWDd1pLOFSIIhE2Mad+7lz8gJmLd7E\nd9o14tHL+9CtZf1UlyXHKAWCSAS5O699uYYH317MocJC7vxeD244oxPVNeyEJJACQSRivtm2h7ET\nc/h0xTZO69yUhy/vTYem9VJdllQBCgSRiCgodF78eCW/m7GUmtWq8eClvbm6f7vIDzshxw4FgkgE\nLN0YG4xu/podnNujOQ8M603LhnVSXZZUMQoEkRQ6mF/IU5m5PJ2VS/06NXni6pO4uE8r7RVISigQ\nRFJk3pod3DZhPss25THsxNbcfXEvmtSrleqypApTIIgk2d6D+Tw2YxkvfLySFg3q8MKIfpzdXYPR\nSeopEESS6JPcrYydlMPq7Xu5ZkB7br+wO/U1GJ1EhAJBJAl27jvEQ9MW89qXa+jYtC6vjTqVUzs3\nTXVZIv9CgSCSYDMXbeLOyTls2X2Am7/bmVvPzdBgdBJJCgSRBNmWd4B731rEW/PX071lfZ69rh99\n2jZKdVkipVIgiITM3Zkyfz33TlnIngMF/OK8DG4+63gNRieRp0AQCdH6Hfu4c/ICZi/ZzEntY4PR\ndW2hweikclAgiISgsND5+xerefidJRQUOndd1JMRp3fUYHRSqSgQROK0cusexk7M5vOV2zmjS1Me\nurQP7ZvWTXVZIkctlEAws8HA48T+pvJz7v5wseW1gZeBvsA24Cp3XxVG3yKpkl9QyPMfreSxmcuo\nVaMaj17ehyv7tdWwE1JpxR0IZlYdeAo4D1gLfGlmU9x9UZFmNwHfunsXMxsOPAJcFW/fIqmyelcB\nlz79CTnrdnJezxY8MOwEWjTQYHRSuYWxh9AfyHX3FQBm9howFCgaCEOBe4PpCcCTZmbu7kda8ea9\nzo//NjeEEhNny5b9vL422jWC6gzToYJCMpfsp3G9Qp665mSG9G6pvQI5JoQRCG2ANUXm1wIDSmvj\n7vlmthNoCmwtvjIzGwWMAqjTohPZqzaFUGLiFBYWsmFPtGsE1Rm2/s2dH/SqQb3tS3n//aWpLqdU\neXl5ZGVlpbqMMqnOaIjcSWV3HweMA+jWrZt/fOeQFFd0ZFlZWQwcODDVZZRJdYZLdYZLdUZDGHfK\nrAPaFZlvGzxXYhszqwE0JHZyWUREIiKMQPgS6GpmncysFjAcmFKszRTg+mD6CmB2WecPREQkueI+\nZBScExgNTCd22ekL7r7QzO4D5rj7FOB54K9mlgtsJxYaIiISIaGcQ3D3acC0Ys/dXWR6P3BlGH2J\niEhiaLQtEREBFAgiIhJQIIiICKBAEBGRgAJBREQABYKIiAQUCCIiAigQREQkoEAQERFAgSAiIgEF\ngoiIAAoEEREJKBBERARQIIiISECBICIigAJBREQCCgQREQEUCCIiEogrEMysiZnNNLPlwc/GpbQr\nMLN5wWNKPH2KiEhixLuHMBZ4z927Au8F8yXZ5+4nBo9L4uxTREQSIN5AGAq8FEy/BAyLc30iIpIi\n5u4Vf7HZDndvFEwb8O3h+WLt8oF5QD7wsLtPPsI6RwGjANLT0/uOHz++wvUlQ15eHmlpaakuo0yq\nM1yqM1yqMzyDBg2a6+79KvRidz/iA5gFLCjhMRTYUaztt6Wso03wszOwCji+rH7dnYyMDI+6zMzM\nVJdQLqozXKozXKozPMAcL8fna0mPGuUIjHNLW2Zmm8yslbtvMLNWwOZS1rEu+LnCzLKAk4Cvy5FX\nIiKSJPGeQ5gCXB9MXw+8WbyBmTU2s9rBdDPgDGBRnP2KiEjI4g2Eh4HzzGw5cG4wj5n1M7PngjY9\ngDlmNh/IJHYOQYEgIhIxZR4yOhJ33wacU8Lzc4CRwfQnQO94+hERkcTTncoiIgIoEEREJKBAEBER\nQIEgIiIBBYKIiAAKBBERCSgQREQEUCCIiEhAgSAiIoACQUREAgoEEREBFAgiIhJQIIiICKBAEBGR\ngAJBREQABYKIiAQUCCIiAigQREQkEFcgmNmVZrbQzArNrN8R2g02s6VmlmtmY+PpU0REEiPePYQF\nwGXAB6U1MLPqwFPAhUBP4Goz6xlnvyIiErIa8bzY3RcDmNmRmvUHct19RdD2NWAosCievkVEJFzm\n7vGvxCwL+KW7zylh2RXAYHcfGcxfCwxw99GlrGsUMAogPT297/jx4+OuL5Hy8vJIS0tLdRllUp3h\nUp3hUp3hGTRo0Fx3L/UQ/pGUuYdgZrOAliUsusPd36xIp0fi7uOAcQDdunXzgQMHht1FqLKysoh6\njaA6w6Y6w6U6o6HMQHD3c+PsYx3Qrsh82+A5ERGJkGRcdvol0NXMOplZLWA4MCUJ/YqIyFGI97LT\nS81sLXAa8LaZTQ+eb21m0wDcPR8YDUwHFgPj3X1hfGWLiEjY4r3K6A3gjRKeXw8MKTI/DZgWT18i\nIpJYulNZREQABYKIiAQUCCIiAigQREQkoEAQERFAgSAiIgEFgoiIAAoEEREJKBBERARQIIiISECB\nICIigAJBREQCCgQREQEUCCIiElAgiIgIoEAQEZGAAkFERAAFgoiIBOL9m8pXmtlCMys0s35HaLfK\nzHLMbJ6ZzYmnTxERSYy4/qYysAC4DPhzOdoOcvetcfYnIiIJElcguPtiADMLpxoREUmZZJ1DcGCG\nmc01s1FJ6lNERI6CufuRG5jNAlqWsOgOd38zaJMF/NLdSzw/YGZt3H2dmTUHZgL/6e4flNJ2FDAK\nID09ve/48ePL+15SIi8vj7S0tFSXUSbVGS7VGS7VGZ5BgwbNdfdSz+kekbvH/QCygH7lbHsvsfAo\ns21GRoZHXWZmZqpLKBfVGS7VGS7VGR5gjlfwszzhh4zMrJ6Z1T88DZxP7GS0iIhESLyXnV5qZmuB\n04C3zWx68HxrM5sWNGsBfGRm84EvgLfd/d14+hURkfDFe5XRG8AbJTy/HhgSTK8AvhNPPyIikni6\nU1lERAAFgoiIBBQIIiICKBBERCSgQBAREUCBICIiAQWCiIgACgQREQkoEEREBFAgiIhIQIEgIiKA\nAkFERAIKBBERARQIIiISUCCIiAigQBARkYACQUREAAWCiIgEFAgiIgLEGQhm9lszW2Jm2Wb2hpk1\nKqXdYDNbama5ZjY2nj5FRCQx4t1DmAmc4O59gGXA7cUbmFl14CngQqAncLWZ9YyzXxERCVlcgeDu\nM9w9P5j9DGhbQrP+QK67r3D3g8BrwNB4+hURkfDVCHFdNwKvl/B8G2BNkfm1wIDSVmJmo4BRwewB\nM1sQWoWJ0QzYmuoiykF1hkt1hkt1hqdbRV9YZiCY2SygZQmL7nD3N4M2dwD5wCsVLeQwdx8HjAvW\nO8fd+8W7zkSqDDWC6gyb6gyX6gyPmc2p6GvLDAR3P7eMzkcAFwHnuLuX0GQd0K7IfNvgORERiZB4\nrzIaDNwGXOLue0tp9iXQ1cw6mVktYDgwJZ5+RUQkfPFeZfQkUB+YaWbzzOwZADNrbWbTAIKTzqOB\n6cBiYLy7Lyzn+sfFWV8yVIYaQXWGTXWGS3WGp8I1WslHeUREpKrRncoiIgIoEEREJBCpQKgMQ2GY\n2ZVmttDMCs2s1MvPzGyVmeUE51YqfBlYRR1FnSkdVsTMmpjZTDNbHvxsXEq7gmBbzjOzpF2UUNb2\nMbPaZvZ6sPxzM+uYrNqK1VFWnSPMbEuRbTgyBTW+YGabS7u3yGKeCN5DtpmdnOwagzrKqnOgme0s\nsi3vTkGN7cws08wWBf/Pf15Cm6Pfnu4emQdwPlAjmH4EeKSENtWBr4HOQC1gPtAziTX2IHbjRxbQ\n7wjtVgHNUrgty6wz1dsyqOFRYGwwPbak33mwLC8F27DM7QP8BHgmmB4OvB7ROkcATya7tmI1fBc4\nGVhQyvIhwDuAAacCn0e0zoHA1BRvy1bAycF0fWJDBxX/nR/19ozUHoJXgqEw3H2xuy9NVn8VVc46\nozCsyFDgpWD6JWBYkvs/kvJsn6L1TwDOMTNLYo0Qjd9jmdz9A2D7EZoMBV72mM+ARmbWKjnV/X/l\nqDPl3H2Du38VTO8mdgVnm2LNjnp7RioQirmRWLoVV9JQGMU3RBQ4MMPM5gbDcURRFLZlC3ffEExv\nBFqU0q6Omc0xs8/MLFmhUZ7t839tgi8zO4GmSamuhBoCpf0eLw8OHUwws3YlLE+1KPx7LK/TzGy+\nmb1jZr1SWUhwmPIk4PNii456e4Y5llG5JHsojIooT43lcKa7rzOz5sTu01gSfPMITUh1JtyR6iw6\n4+5uZqVdB90h2J6dgdlmluPuX4dd6zHsLeBVdz9gZjcT26s5O8U1VVZfEfv3mGdmQ4DJQNdUFGJm\nacBE4FZ33xXv+pIeCF4Jhrsy27gAAAJlSURBVMIoq8ZyrmNd8HOzmb1BbLc+1EAIoc6kDCtypDrN\nbJOZtXL3DcHu7OZS1nF4e64wsyxi34gSHQjl2T6H26w1sxpAQ2Bbgusqrsw63b1oTc8RO3cTNZVi\nmJuiH7zuPs3MnjazZu6e1EHvzKwmsTB4xd0nldDkqLdnpA4Z2TEyFIaZ1TOz+oeniZ0sj+KorVHY\nllOA64Pp64F/27Mxs8ZmVjuYbgacASxKQm3l2T5F678CmF3KF5lEKrPOYseOLyF2zDlqpgDXBVfH\nnArsLHI4MTLMrOXh80Rm1p/Y52hSvwQE/T8PLHb3x0ppdvTbM5Vnyks4c55L7JjXvOBx+OqN1sC0\nYmfPlxH7hnhHkmu8lNixuAPAJmB68RqJXe0xP3gsTHaN5a0z1dsy6L8p8B6wHJgFNAme7wc8F0yf\nDuQE2zMHuCmJ9f3b9gHuI/alBaAO8I/g3+4XQOdkb8Ny1vlQ8G9xPpAJdE9Bja8CG4BDwb/Nm4Bb\ngFuC5Ubsj2l9HfyeS72KL8V1ji6yLT8DTk9BjWcSO0+ZXeTzcki821NDV4iICBCxQ0YiIpI6CgQR\nEQEUCCIiElAgiIgIoEAQEZGAAkFERAAFgoiIBBQIIkfBzG4pMg7+SjPLTHVNImHRjWkiFRCMIzMb\neNTd30p1PSJh0B6CSMU8TmzcIoWBHDOSPtqpSGUXjMjbgdiYNiLHDB0yEjkKZtaX2N8S+A93/zbV\n9YiESYeMRI7OaKAJkBmcWH4u1QWJhEV7CCIiAmgPQUREAgoEEREBFAgiIhJQIIiICKBAEBGRgAJB\nREQABYKIiAT+HyfyKNa1Xr+qAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTs1Joe630rB",
        "colab_type": "code",
        "outputId": "bc414add-a625-4f43-90c7-1126a0a6980c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "Rval = np.arange(0.00,10.00,0.01)\n",
        "Rval = Rval[0:df2.shape[0]]\n",
        "test = pd.DataFrame(np.zeros((df2.shape[0],df2.shape[1])))\n",
        "test[0] = Rval.T\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>8061</th>\n",
              "      <th>8062</th>\n",
              "      <th>8063</th>\n",
              "      <th>8064</th>\n",
              "      <th>8065</th>\n",
              "      <th>8066</th>\n",
              "      <th>8067</th>\n",
              "      <th>8068</th>\n",
              "      <th>8069</th>\n",
              "      <th>8070</th>\n",
              "      <th>8071</th>\n",
              "      <th>8072</th>\n",
              "      <th>8073</th>\n",
              "      <th>8074</th>\n",
              "      <th>8075</th>\n",
              "      <th>8076</th>\n",
              "      <th>8077</th>\n",
              "      <th>8078</th>\n",
              "      <th>8079</th>\n",
              "      <th>8080</th>\n",
              "      <th>8081</th>\n",
              "      <th>8082</th>\n",
              "      <th>8083</th>\n",
              "      <th>8084</th>\n",
              "      <th>8085</th>\n",
              "      <th>8086</th>\n",
              "      <th>8087</th>\n",
              "      <th>8088</th>\n",
              "      <th>8089</th>\n",
              "      <th>8090</th>\n",
              "      <th>8091</th>\n",
              "      <th>8092</th>\n",
              "      <th>8093</th>\n",
              "      <th>8094</th>\n",
              "      <th>8095</th>\n",
              "      <th>8096</th>\n",
              "      <th>8097</th>\n",
              "      <th>8098</th>\n",
              "      <th>8099</th>\n",
              "      <th>8100</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>9.95</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>9.96</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>9.97</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>9.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>9.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows Ã— 8101 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0     1     2     3     4     5     ...  8095  8096  8097  8098  8099  8100\n",
              "0    0.00   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "1    0.01   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "2    0.02   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "3    0.03   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "4    0.04   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "..    ...   ...   ...   ...   ...   ...  ...   ...   ...   ...   ...   ...   ...\n",
              "995  9.95   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "996  9.96   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "997  9.97   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "998  9.98   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "999  9.99   0.0   0.0   0.0   0.0   0.0  ...   0.0   0.0   0.0   0.0   0.0   0.0\n",
              "\n",
              "[1000 rows x 8101 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YDwRMEw8A0J",
        "colab_type": "code",
        "outputId": "5fff802e-a20e-42d2-cd73-dd6b5d1da1e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print('Start timing for predictions')\n",
        "start = time.time()\n",
        "predictions = NN_model.predict(test)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start timing for predictions\n",
            "0.09470534324645996\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wVI6s589S65",
        "colab_type": "code",
        "outputId": "523602f2-1078-420b-fa8e-494370a7619a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 609
        }
      },
      "source": [
        "pred = pd.DataFrame(predictions)\n",
        "pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1311</th>\n",
              "      <th>1312</th>\n",
              "      <th>1313</th>\n",
              "      <th>1314</th>\n",
              "      <th>1315</th>\n",
              "      <th>1316</th>\n",
              "      <th>1317</th>\n",
              "      <th>1318</th>\n",
              "      <th>1319</th>\n",
              "      <th>1320</th>\n",
              "      <th>1321</th>\n",
              "      <th>1322</th>\n",
              "      <th>1323</th>\n",
              "      <th>1324</th>\n",
              "      <th>1325</th>\n",
              "      <th>1326</th>\n",
              "      <th>1327</th>\n",
              "      <th>1328</th>\n",
              "      <th>1329</th>\n",
              "      <th>1330</th>\n",
              "      <th>1331</th>\n",
              "      <th>1332</th>\n",
              "      <th>1333</th>\n",
              "      <th>1334</th>\n",
              "      <th>1335</th>\n",
              "      <th>1336</th>\n",
              "      <th>1337</th>\n",
              "      <th>1338</th>\n",
              "      <th>1339</th>\n",
              "      <th>1340</th>\n",
              "      <th>1341</th>\n",
              "      <th>1342</th>\n",
              "      <th>1343</th>\n",
              "      <th>1344</th>\n",
              "      <th>1345</th>\n",
              "      <th>1346</th>\n",
              "      <th>1347</th>\n",
              "      <th>1348</th>\n",
              "      <th>1349</th>\n",
              "      <th>1350</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.060594</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-8.158386e-07</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000245</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>-0.000218</td>\n",
              "      <td>-0.000122</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>-0.000260</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000680</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>-0.000302</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.008133</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.000700</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>-0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.060594</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-8.158386e-07</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000245</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>-0.000218</td>\n",
              "      <td>-0.000122</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>-0.000260</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000680</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>-0.000302</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.008133</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.000700</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>-0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.060594</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-8.158386e-07</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000245</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>-0.000218</td>\n",
              "      <td>-0.000122</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>-0.000260</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000680</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>-0.000302</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.008133</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.000700</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>-0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.060594</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>-8.158386e-07</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>-0.000153</td>\n",
              "      <td>-0.000126</td>\n",
              "      <td>-0.000131</td>\n",
              "      <td>-0.000212</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>-0.000245</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>-0.000208</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>-0.000218</td>\n",
              "      <td>-0.000122</td>\n",
              "      <td>-0.000168</td>\n",
              "      <td>-0.000125</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>-0.000061</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>-0.000260</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>-0.000024</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000192</td>\n",
              "      <td>-0.000146</td>\n",
              "      <td>-0.000077</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>-0.000137</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>-0.000084</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000680</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000329</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>-0.000302</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>-0.000039</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000837</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.020795</td>\n",
              "      <td>0.008133</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000268</td>\n",
              "      <td>-0.000700</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>-0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.062861</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>-9.793555e-06</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>-0.000151</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>-0.000120</td>\n",
              "      <td>-0.000237</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>-0.000205</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>-0.000230</td>\n",
              "      <td>-0.000116</td>\n",
              "      <td>-0.000187</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>-0.000008</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>-0.000257</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>0.000238</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>-0.000127</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.000166</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>-0.000130</td>\n",
              "      <td>-0.000196</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>-0.000179</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>-0.000068</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>-0.000214</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>-0.000033</td>\n",
              "      <td>-0.000670</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000032</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000353</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>-0.000300</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>-0.000053</td>\n",
              "      <td>-0.000028</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000846</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.019904</td>\n",
              "      <td>0.007686</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>-0.000282</td>\n",
              "      <td>-0.000702</td>\n",
              "      <td>0.000295</td>\n",
              "      <td>-0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>245</th>\n",
              "      <td>2.428112</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>1.151401e-03</td>\n",
              "      <td>0.001014</td>\n",
              "      <td>0.001398</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.001422</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000814</td>\n",
              "      <td>0.001400</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000770</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000895</td>\n",
              "      <td>0.001367</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.001533</td>\n",
              "      <td>0.000772</td>\n",
              "      <td>0.001790</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.000471</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.001518</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.001202</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>-0.000382</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.000628</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>0.001777</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>0.001369</td>\n",
              "      <td>0.001305</td>\n",
              "      <td>0.000870</td>\n",
              "      <td>0.001613</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>0.002089</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.002557</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.001417</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001461</td>\n",
              "      <td>0.000577</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.001311</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.002024</td>\n",
              "      <td>0.002545</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.002337</td>\n",
              "      <td>0.478077</td>\n",
              "      <td>0.146795</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.000016</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>246</th>\n",
              "      <td>2.438079</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>1.156006e-03</td>\n",
              "      <td>0.001018</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.001421</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000898</td>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.001247</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.001538</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.000962</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.001002</td>\n",
              "      <td>0.000475</td>\n",
              "      <td>0.001917</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.001525</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>-0.000384</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.001377</td>\n",
              "      <td>0.001310</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.001525</td>\n",
              "      <td>0.001024</td>\n",
              "      <td>0.002098</td>\n",
              "      <td>0.001314</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.002568</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.001068</td>\n",
              "      <td>0.001077</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.002117</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.001466</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.000605</td>\n",
              "      <td>0.000908</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.002034</td>\n",
              "      <td>0.002558</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.002344</td>\n",
              "      <td>0.480887</td>\n",
              "      <td>0.147932</td>\n",
              "      <td>0.001330</td>\n",
              "      <td>0.000635</td>\n",
              "      <td>0.001311</td>\n",
              "      <td>0.001331</td>\n",
              "      <td>0.000013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>247</th>\n",
              "      <td>2.448046</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>1.160610e-03</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.001430</td>\n",
              "      <td>0.000334</td>\n",
              "      <td>0.000819</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.001382</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001101</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.001543</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.001807</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.001004</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.001532</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.000525</td>\n",
              "      <td>-0.000386</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001519</td>\n",
              "      <td>0.001788</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>0.000582</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.001627</td>\n",
              "      <td>0.001530</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.002106</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.002579</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.001072</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.001721</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.002126</td>\n",
              "      <td>0.001177</td>\n",
              "      <td>0.001471</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.000913</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.001329</td>\n",
              "      <td>0.002043</td>\n",
              "      <td>0.002571</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002351</td>\n",
              "      <td>0.483697</td>\n",
              "      <td>0.149070</td>\n",
              "      <td>0.001334</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.000010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>248</th>\n",
              "      <td>2.458015</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>1.165064e-03</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>0.001419</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.001434</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000788</td>\n",
              "      <td>0.000538</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.001390</td>\n",
              "      <td>0.001259</td>\n",
              "      <td>0.001105</td>\n",
              "      <td>0.000784</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.001548</td>\n",
              "      <td>0.000777</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000947</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>0.001006</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.001932</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.001233</td>\n",
              "      <td>0.001216</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>-0.000387</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.001341</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001527</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.000585</td>\n",
              "      <td>0.000781</td>\n",
              "      <td>0.001391</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>0.001035</td>\n",
              "      <td>0.002115</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.002590</td>\n",
              "      <td>0.001375</td>\n",
              "      <td>0.001077</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.001729</td>\n",
              "      <td>0.001438</td>\n",
              "      <td>0.002136</td>\n",
              "      <td>0.001181</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>0.001324</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>0.002052</td>\n",
              "      <td>0.002584</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.002358</td>\n",
              "      <td>0.486507</td>\n",
              "      <td>0.150208</td>\n",
              "      <td>0.001338</td>\n",
              "      <td>0.000637</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.001349</td>\n",
              "      <td>0.000007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>249</th>\n",
              "      <td>2.467988</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>1.169466e-03</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.001426</td>\n",
              "      <td>0.000615</td>\n",
              "      <td>0.001436</td>\n",
              "      <td>0.001437</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000794</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.001110</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.001553</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.001822</td>\n",
              "      <td>0.000969</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.001008</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.001939</td>\n",
              "      <td>0.000431</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001220</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>-0.000389</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.000640</td>\n",
              "      <td>0.001345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001535</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000785</td>\n",
              "      <td>0.001399</td>\n",
              "      <td>0.001325</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.001642</td>\n",
              "      <td>0.001539</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.002124</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.002601</td>\n",
              "      <td>0.001379</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.001444</td>\n",
              "      <td>0.002146</td>\n",
              "      <td>0.001187</td>\n",
              "      <td>0.001482</td>\n",
              "      <td>0.000581</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000922</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>0.001328</td>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.002062</td>\n",
              "      <td>0.002596</td>\n",
              "      <td>0.001074</td>\n",
              "      <td>0.002365</td>\n",
              "      <td>0.489318</td>\n",
              "      <td>0.151346</td>\n",
              "      <td>0.001342</td>\n",
              "      <td>0.000638</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.001359</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>250 rows Ã— 1351 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0         1             2     ...      1348      1349      1350\n",
              "0    0.060594  0.000100 -8.158386e-07  ... -0.000700  0.000307 -0.000014\n",
              "1    0.060594  0.000100 -8.158386e-07  ... -0.000700  0.000307 -0.000014\n",
              "2    0.060594  0.000100 -8.158386e-07  ... -0.000700  0.000307 -0.000014\n",
              "3    0.060594  0.000100 -8.158386e-07  ... -0.000700  0.000307 -0.000014\n",
              "4    0.062861  0.000112 -9.793555e-06  ... -0.000702  0.000295 -0.000018\n",
              "..        ...       ...           ...  ...       ...       ...       ...\n",
              "245  2.428112  0.000638  1.151401e-03  ...  0.001299  0.001322  0.000016\n",
              "246  2.438079  0.000641  1.156006e-03  ...  0.001311  0.001331  0.000013\n",
              "247  2.448046  0.000644  1.160610e-03  ...  0.001324  0.001340  0.000010\n",
              "248  2.458015  0.000647  1.165064e-03  ...  0.001337  0.001349  0.000007\n",
              "249  2.467988  0.000650  1.169466e-03  ...  0.001350  0.001359  0.000005\n",
              "\n",
              "[250 rows x 1351 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWY6T-pOOSz8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = df2[800:1000]\n",
        "predictions = NN_model.predict(test)\n",
        "y_pred = abs(pd.DataFrame(predictions))\n",
        "y_pred.to_csv(r'5400_1000_200.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vfD3d0p3rvKM",
        "colab_type": "code",
        "outputId": "7c04559f-ded5-4894-a9a9-7a2b37ac01df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#Best: -0.004705 using {'batch_size': 60, 'epochs': 100}\n",
        "# -0.003991 using {'batch_size': 150, 'epochs': 100} for 3k data\n",
        "import numpy\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "# Function to create model, required for KerasClassifier\n",
        "def create_model():\n",
        "  NN_model = Sequential()\n",
        "  # The Input Layer :\n",
        "  NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "  # The Hidden Layers :\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "  # The Output Layer :\n",
        "  NN_model.add(Dense(train.shape[1], kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "  # Compile the network :\n",
        "  NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error','accuracy'])\n",
        "  return NN_model\n",
        "# fix random seed for reproducibility\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# create model\n",
        "model = KerasRegressor(build_fn=create_model, verbose=False)\n",
        "# define the grid search parameters\n",
        "batch_size = [10, 20, 40, 60, 80, 100, 150, 200]\n",
        "epochs = [100,200,300, 400, 500, 600, 700]\n",
        "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train, target)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Best: -0.007146 using {'batch_size': 20, 'epochs': 700}\n",
            "-0.008418 (0.003037) with: {'batch_size': 10, 'epochs': 100}\n",
            "-0.007558 (0.002053) with: {'batch_size': 10, 'epochs': 200}\n",
            "-0.009614 (0.004931) with: {'batch_size': 10, 'epochs': 300}\n",
            "-0.008802 (0.003735) with: {'batch_size': 10, 'epochs': 400}\n",
            "-0.008167 (0.002874) with: {'batch_size': 10, 'epochs': 500}\n",
            "-0.008271 (0.003021) with: {'batch_size': 10, 'epochs': 600}\n",
            "-0.008727 (0.002722) with: {'batch_size': 10, 'epochs': 700}\n",
            "-0.008321 (0.003041) with: {'batch_size': 20, 'epochs': 100}\n",
            "-0.008548 (0.003325) with: {'batch_size': 20, 'epochs': 200}\n",
            "-0.011663 (0.007505) with: {'batch_size': 20, 'epochs': 300}\n",
            "-0.015007 (0.012348) with: {'batch_size': 20, 'epochs': 400}\n",
            "-0.007574 (0.001741) with: {'batch_size': 20, 'epochs': 500}\n",
            "-0.011521 (0.007437) with: {'batch_size': 20, 'epochs': 600}\n",
            "-0.007146 (0.001673) with: {'batch_size': 20, 'epochs': 700}\n",
            "-0.008362 (0.003080) with: {'batch_size': 40, 'epochs': 100}\n",
            "-0.008387 (0.003106) with: {'batch_size': 40, 'epochs': 200}\n",
            "-0.008177 (0.002812) with: {'batch_size': 40, 'epochs': 300}\n",
            "-0.008258 (0.002951) with: {'batch_size': 40, 'epochs': 400}\n",
            "-0.008059 (0.002727) with: {'batch_size': 40, 'epochs': 500}\n",
            "-0.008592 (0.003235) with: {'batch_size': 40, 'epochs': 600}\n",
            "-0.018666 (0.017333) with: {'batch_size': 40, 'epochs': 700}\n",
            "-0.008121 (0.003018) with: {'batch_size': 60, 'epochs': 100}\n",
            "-0.008289 (0.003058) with: {'batch_size': 60, 'epochs': 200}\n",
            "-0.008137 (0.002820) with: {'batch_size': 60, 'epochs': 300}\n",
            "-0.008414 (0.003072) with: {'batch_size': 60, 'epochs': 400}\n",
            "-0.008474 (0.003257) with: {'batch_size': 60, 'epochs': 500}\n",
            "-0.016497 (0.014256) with: {'batch_size': 60, 'epochs': 600}\n",
            "-0.019656 (0.018760) with: {'batch_size': 60, 'epochs': 700}\n",
            "-0.008246 (0.003187) with: {'batch_size': 80, 'epochs': 100}\n",
            "-0.008367 (0.003050) with: {'batch_size': 80, 'epochs': 200}\n",
            "-0.008313 (0.003127) with: {'batch_size': 80, 'epochs': 300}\n",
            "-0.008325 (0.003047) with: {'batch_size': 80, 'epochs': 400}\n",
            "-0.008355 (0.003059) with: {'batch_size': 80, 'epochs': 500}\n",
            "-0.011871 (0.007843) with: {'batch_size': 80, 'epochs': 600}\n",
            "-0.021658 (0.021736) with: {'batch_size': 80, 'epochs': 700}\n",
            "-0.008049 (0.003071) with: {'batch_size': 100, 'epochs': 100}\n",
            "-0.008298 (0.003110) with: {'batch_size': 100, 'epochs': 200}\n",
            "-0.008419 (0.003086) with: {'batch_size': 100, 'epochs': 300}\n",
            "-0.008419 (0.003127) with: {'batch_size': 100, 'epochs': 400}\n",
            "-0.008382 (0.003184) with: {'batch_size': 100, 'epochs': 500}\n",
            "-0.008393 (0.003145) with: {'batch_size': 100, 'epochs': 600}\n",
            "-0.012430 (0.008616) with: {'batch_size': 100, 'epochs': 700}\n",
            "-0.007930 (0.003314) with: {'batch_size': 150, 'epochs': 100}\n",
            "-0.008176 (0.003046) with: {'batch_size': 150, 'epochs': 200}\n",
            "-0.008349 (0.003080) with: {'batch_size': 150, 'epochs': 300}\n",
            "-0.008340 (0.002996) with: {'batch_size': 150, 'epochs': 400}\n",
            "-0.008281 (0.003016) with: {'batch_size': 150, 'epochs': 500}\n",
            "-0.008312 (0.003067) with: {'batch_size': 150, 'epochs': 600}\n",
            "-0.008011 (0.002694) with: {'batch_size': 150, 'epochs': 700}\n",
            "-0.007822 (0.003295) with: {'batch_size': 200, 'epochs': 100}\n",
            "-0.007963 (0.003162) with: {'batch_size': 200, 'epochs': 200}\n",
            "-0.008390 (0.003100) with: {'batch_size': 200, 'epochs': 300}\n",
            "-0.008328 (0.003115) with: {'batch_size': 200, 'epochs': 400}\n",
            "-0.008396 (0.003026) with: {'batch_size': 200, 'epochs': 500}\n",
            "-0.008226 (0.002954) with: {'batch_size': 200, 'epochs': 600}\n",
            "-0.008214 (0.002917) with: {'batch_size': 200, 'epochs': 700}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmNtbxOaslV0",
        "colab_type": "code",
        "outputId": "9a0303e2-c63e-440a-9b64-90229118b085",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c9cd1e2aba91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best: %f using %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mean_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'std_test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'grid_result' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWSRwqdw08Qy",
        "colab_type": "code",
        "outputId": "c0808340-8c32-4849-d073-46c3e02d74cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "def create_model(optimizer='adam'):\n",
        "  NN_model = Sequential()\n",
        "  \n",
        "  NN_model.add(Dense(128, kernel_initializer='normal',input_dim = train.shape[1], activation='relu'))\n",
        "\n",
        "  # The Hidden Layers :\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "  NN_model.add(Dense(512, kernel_initializer='normal',activation='relu'))\n",
        "\n",
        "  # The Output Layer :\n",
        "  NN_model.add(Dense(2551, kernel_initializer='normal',activation='linear'))\n",
        "\n",
        "  # Compile the network :\n",
        "  NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error','accuracy'])\n",
        "  return NN_model\n",
        "seed = 7\n",
        "numpy.random.seed(seed)\n",
        "# create model\n",
        "model = KerasRegressor(build_fn=create_model, epochs=100, batch_size=60, verbose=0)\n",
        "# define the grid search parameters\n",
        "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
        "param_grid = dict(optimizer=optimizer)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
        "grid_result = grid.fit(train, target)\n",
        "# summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best: -0.004632 using {'optimizer': 'Nadam'}\n",
            "-0.004800 (0.001654) with: {'optimizer': 'SGD'}\n",
            "-0.004867 (0.001587) with: {'optimizer': 'RMSprop'}\n",
            "-0.004780 (0.001688) with: {'optimizer': 'Adagrad'}\n",
            "-0.004753 (0.001651) with: {'optimizer': 'Adadelta'}\n",
            "-0.004732 (0.001587) with: {'optimizer': 'Adam'}\n",
            "-0.004872 (0.001828) with: {'optimizer': 'Adamax'}\n",
            "-0.004632 (0.001714) with: {'optimizer': 'Nadam'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwKeoDim4CsB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir Checkpoints \n",
        "checkpoint_name = 'Checkpoints/Weights-{epoch:03d}--{val_loss:.5f}.hdf5' \n",
        "checkpoint = ModelCheckpoint(checkpoint_name, monitor='val_loss', verbose = 1, save_best_only = True, mode ='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YmzWZj7x5EO",
        "colab_type": "code",
        "outputId": "d3c97b51-1da1-43aa-bf97-ca297e737146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "NN_model.fit(train, target, epochs=700, batch_size=20, validation_split = 0.3, callbacks=callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 2099 samples, validate on 900 samples\n",
            "Epoch 1/700\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "2099/2099 [==============================] - 6s 3ms/step - loss: 0.0097 - mean_absolute_error: 0.0097 - acc: 0.1091 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.01407, saving model to Checkpoints/Weights-001--0.01407.hdf5\n",
            "Epoch 2/700\n",
            "2099/2099 [==============================] - 1s 371us/step - loss: 0.0035 - mean_absolute_error: 0.0035 - acc: 1.0000 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.01407 to 0.01355, saving model to Checkpoints/Weights-002--0.01355.hdf5\n",
            "Epoch 3/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0032 - mean_absolute_error: 0.0032 - acc: 1.0000 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.01355 to 0.01317, saving model to Checkpoints/Weights-003--0.01317.hdf5\n",
            "Epoch 4/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0031 - mean_absolute_error: 0.0031 - acc: 1.0000 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.01317\n",
            "Epoch 5/700\n",
            "2099/2099 [==============================] - 1s 381us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 1.0000 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.01317 to 0.01302, saving model to Checkpoints/Weights-005--0.01302.hdf5\n",
            "Epoch 6/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 1.0000 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.01302 to 0.01295, saving model to Checkpoints/Weights-006--0.01295.hdf5\n",
            "Epoch 7/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 1.0000 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.01295 to 0.01282, saving model to Checkpoints/Weights-007--0.01282.hdf5\n",
            "Epoch 8/700\n",
            "2099/2099 [==============================] - 1s 374us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 0.9633 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.01282\n",
            "Epoch 9/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 0.9500 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.01282\n",
            "Epoch 10/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 0.9495 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.01282\n",
            "Epoch 11/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9505 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.01282\n",
            "Epoch 12/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9519 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.01282\n",
            "Epoch 13/700\n",
            "2099/2099 [==============================] - 1s 321us/step - loss: 0.0030 - mean_absolute_error: 0.0030 - acc: 0.9495 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.01282 to 0.01276, saving model to Checkpoints/Weights-013--0.01276.hdf5\n",
            "Epoch 14/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9533 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.01276 to 0.01263, saving model to Checkpoints/Weights-014--0.01263.hdf5\n",
            "Epoch 15/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9533 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.01263 to 0.01252, saving model to Checkpoints/Weights-015--0.01252.hdf5\n",
            "Epoch 16/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9533 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.01252\n",
            "Epoch 17/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9557 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.01252 to 0.01248, saving model to Checkpoints/Weights-017--0.01248.hdf5\n",
            "Epoch 18/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9557 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.01248\n",
            "Epoch 19/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9571 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.01248 to 0.01245, saving model to Checkpoints/Weights-019--0.01245.hdf5\n",
            "Epoch 20/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0029 - mean_absolute_error: 0.0029 - acc: 0.9552 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.01245 to 0.01233, saving model to Checkpoints/Weights-020--0.01233.hdf5\n",
            "Epoch 21/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9576 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.01233\n",
            "Epoch 22/700\n",
            "2099/2099 [==============================] - 1s 363us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9566 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.01233\n",
            "Epoch 23/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9581 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.01233\n",
            "Epoch 24/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9566 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.01233\n",
            "Epoch 25/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9590 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.01233 to 0.01232, saving model to Checkpoints/Weights-025--0.01232.hdf5\n",
            "Epoch 26/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9595 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.01232 to 0.01214, saving model to Checkpoints/Weights-026--0.01214.hdf5\n",
            "Epoch 27/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9581 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.01214\n",
            "Epoch 28/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9609 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.01214\n",
            "Epoch 29/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9619 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.01214\n",
            "Epoch 30/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9595 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.01214\n",
            "Epoch 31/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9619 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.01214 to 0.01210, saving model to Checkpoints/Weights-031--0.01210.hdf5\n",
            "Epoch 32/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9643 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.01210\n",
            "Epoch 33/700\n",
            "2099/2099 [==============================] - 1s 375us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9624 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.01210\n",
            "Epoch 34/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9662 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.01210\n",
            "Epoch 35/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9662 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.01210\n",
            "Epoch 36/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9695 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.01210\n",
            "Epoch 37/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9690 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.01210\n",
            "Epoch 38/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9709 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.01210\n",
            "Epoch 39/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9733 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.01210\n",
            "Epoch 40/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9743 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.01210\n",
            "Epoch 41/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9757 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.01210\n",
            "Epoch 42/700\n",
            "2099/2099 [==============================] - 1s 359us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9738 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.01210\n",
            "Epoch 43/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9747 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.01210\n",
            "Epoch 44/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9752 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.01210\n",
            "Epoch 45/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9738 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.01210\n",
            "Epoch 46/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9767 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.01210\n",
            "Epoch 47/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9762 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.01210\n",
            "Epoch 48/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9767 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.01210\n",
            "Epoch 49/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9771 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.01210\n",
            "Epoch 50/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9776 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.01210\n",
            "Epoch 51/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9781 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.01210\n",
            "Epoch 52/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0028 - mean_absolute_error: 0.0028 - acc: 0.9786 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.01210\n",
            "Epoch 53/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9790 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.01210\n",
            "Epoch 54/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9776 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.01210\n",
            "Epoch 55/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9776 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.01210\n",
            "Epoch 56/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9776 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.01210\n",
            "Epoch 57/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9786 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.01210\n",
            "Epoch 58/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9795 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.01210\n",
            "Epoch 59/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9790 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.01210\n",
            "Epoch 60/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9781 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.01210\n",
            "Epoch 61/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9795 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.01210\n",
            "Epoch 62/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9786 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.01210\n",
            "Epoch 63/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9786 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.01210\n",
            "Epoch 64/700\n",
            "2099/2099 [==============================] - 1s 370us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9790 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.01210\n",
            "Epoch 65/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9757 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.01210\n",
            "Epoch 66/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9800 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.01210\n",
            "Epoch 67/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9790 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.01210\n",
            "Epoch 68/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9786 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.01210\n",
            "Epoch 69/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9790 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.01210\n",
            "Epoch 70/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9795 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.01210\n",
            "Epoch 71/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9795 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00071: val_loss improved from 0.01210 to 0.01202, saving model to Checkpoints/Weights-071--0.01202.hdf5\n",
            "Epoch 72/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9800 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.01202\n",
            "Epoch 73/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9805 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00073: val_loss improved from 0.01202 to 0.01193, saving model to Checkpoints/Weights-073--0.01193.hdf5\n",
            "Epoch 74/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9805 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.01193\n",
            "Epoch 75/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9819 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.01193\n",
            "Epoch 76/700\n",
            "2099/2099 [==============================] - 1s 318us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9800 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.01193 to 0.01186, saving model to Checkpoints/Weights-076--0.01186.hdf5\n",
            "Epoch 77/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9824 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.01186 to 0.01177, saving model to Checkpoints/Weights-077--0.01177.hdf5\n",
            "Epoch 78/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9824 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.01177\n",
            "Epoch 79/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9819 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.01177\n",
            "Epoch 80/700\n",
            "2099/2099 [==============================] - 1s 317us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9805 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.01177\n",
            "Epoch 81/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9819 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.01177\n",
            "Epoch 82/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9814 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.01177\n",
            "Epoch 83/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.01177\n",
            "Epoch 84/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0027 - mean_absolute_error: 0.0027 - acc: 0.9814 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.01177\n",
            "Epoch 85/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00085: val_loss improved from 0.01177 to 0.01176, saving model to Checkpoints/Weights-085--0.01176.hdf5\n",
            "Epoch 86/700\n",
            "2099/2099 [==============================] - 1s 372us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.01176 to 0.01166, saving model to Checkpoints/Weights-086--0.01166.hdf5\n",
            "Epoch 87/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.01166\n",
            "Epoch 88/700\n",
            "2099/2099 [==============================] - 1s 323us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9833 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.01166\n",
            "Epoch 89/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.01166\n",
            "Epoch 90/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.01166\n",
            "Epoch 91/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00091: val_loss improved from 0.01166 to 0.01152, saving model to Checkpoints/Weights-091--0.01152.hdf5\n",
            "Epoch 92/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9824 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.01152\n",
            "Epoch 93/700\n",
            "2099/2099 [==============================] - 1s 366us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9838 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.01152\n",
            "Epoch 94/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00094: val_loss improved from 0.01152 to 0.01149, saving model to Checkpoints/Weights-094--0.01149.hdf5\n",
            "Epoch 95/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9838 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00095: val_loss improved from 0.01149 to 0.01146, saving model to Checkpoints/Weights-095--0.01146.hdf5\n",
            "Epoch 96/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9833 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00096: val_loss improved from 0.01146 to 0.01100, saving model to Checkpoints/Weights-096--0.01100.hdf5\n",
            "Epoch 97/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0105 - val_mean_absolute_error: 0.0105 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00097: val_loss improved from 0.01100 to 0.01051, saving model to Checkpoints/Weights-097--0.01051.hdf5\n",
            "Epoch 98/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9848 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00098: val_loss improved from 0.01051 to 0.01017, saving model to Checkpoints/Weights-098--0.01017.hdf5\n",
            "Epoch 99/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0112 - val_mean_absolute_error: 0.0112 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.01017\n",
            "Epoch 100/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9848 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.01017\n",
            "Epoch 101/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.01017\n",
            "Epoch 102/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.01017\n",
            "Epoch 103/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9857 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.01017\n",
            "Epoch 104/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9828 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.01017\n",
            "Epoch 105/700\n",
            "2099/2099 [==============================] - 1s 359us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.01017\n",
            "Epoch 106/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9848 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.01017\n",
            "Epoch 107/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9852 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.01017\n",
            "Epoch 108/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9843 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.01017\n",
            "Epoch 109/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9848 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.01017\n",
            "Epoch 110/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9833 - val_loss: 0.0103 - val_mean_absolute_error: 0.0103 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.01017\n",
            "Epoch 111/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0026 - mean_absolute_error: 0.0026 - acc: 0.9848 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00111: val_loss improved from 0.01017 to 0.01006, saving model to Checkpoints/Weights-111--0.01006.hdf5\n",
            "Epoch 112/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9838 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00112: val_loss improved from 0.01006 to 0.01004, saving model to Checkpoints/Weights-112--0.01004.hdf5\n",
            "Epoch 113/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9852 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.01004\n",
            "Epoch 114/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9828 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.01004\n",
            "Epoch 115/700\n",
            "2099/2099 [==============================] - 1s 377us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9828 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.01004\n",
            "Epoch 116/700\n",
            "2099/2099 [==============================] - 1s 370us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9848 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.01004\n",
            "Epoch 117/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9833 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.01004\n",
            "Epoch 118/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.01004\n",
            "Epoch 119/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9828 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.01004\n",
            "Epoch 120/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0108 - val_mean_absolute_error: 0.0108 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.01004\n",
            "Epoch 121/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9819 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.01004\n",
            "Epoch 122/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9824 - val_loss: 0.0100 - val_mean_absolute_error: 0.0100 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00122: val_loss improved from 0.01004 to 0.00999, saving model to Checkpoints/Weights-122--0.00999.hdf5\n",
            "Epoch 123/700\n",
            "2099/2099 [==============================] - 1s 364us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9824 - val_loss: 0.0106 - val_mean_absolute_error: 0.0106 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.00999\n",
            "Epoch 124/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9800 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.00999\n",
            "Epoch 125/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9814 - val_loss: 0.0098 - val_mean_absolute_error: 0.0098 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.00999 to 0.00976, saving model to Checkpoints/Weights-125--0.00976.hdf5\n",
            "Epoch 126/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9828 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.00976\n",
            "Epoch 127/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.00976\n",
            "Epoch 128/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9814 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.00976\n",
            "Epoch 129/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9828 - val_loss: 0.0104 - val_mean_absolute_error: 0.0104 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.00976\n",
            "Epoch 130/700\n",
            "2099/2099 [==============================] - 1s 363us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9819 - val_loss: 0.0102 - val_mean_absolute_error: 0.0102 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.00976\n",
            "Epoch 131/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00131: val_loss improved from 0.00976 to 0.00951, saving model to Checkpoints/Weights-131--0.00951.hdf5\n",
            "Epoch 132/700\n",
            "2099/2099 [==============================] - 1s 387us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9819 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.00951\n",
            "Epoch 133/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.00951\n",
            "Epoch 134/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9824 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00134: val_loss improved from 0.00951 to 0.00946, saving model to Checkpoints/Weights-134--0.00946.hdf5\n",
            "Epoch 135/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.00946\n",
            "Epoch 136/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00136: val_loss improved from 0.00946 to 0.00930, saving model to Checkpoints/Weights-136--0.00930.hdf5\n",
            "Epoch 137/700\n",
            "2099/2099 [==============================] - 1s 371us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9819 - val_loss: 0.0095 - val_mean_absolute_error: 0.0095 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.00930\n",
            "Epoch 138/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0092 - val_mean_absolute_error: 0.0092 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00138: val_loss improved from 0.00930 to 0.00922, saving model to Checkpoints/Weights-138--0.00922.hdf5\n",
            "Epoch 139/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9805 - val_loss: 0.0091 - val_mean_absolute_error: 0.0091 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00139: val_loss improved from 0.00922 to 0.00912, saving model to Checkpoints/Weights-139--0.00912.hdf5\n",
            "Epoch 140/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.00912\n",
            "Epoch 141/700\n",
            "2099/2099 [==============================] - 1s 322us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.00912\n",
            "Epoch 142/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0094 - val_mean_absolute_error: 0.0094 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.00912\n",
            "Epoch 143/700\n",
            "2099/2099 [==============================] - 1s 361us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0097 - val_mean_absolute_error: 0.0097 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.00912\n",
            "Epoch 144/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9819 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.00912\n",
            "Epoch 145/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0093 - val_mean_absolute_error: 0.0093 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.00912\n",
            "Epoch 146/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9848 - val_loss: 0.0101 - val_mean_absolute_error: 0.0101 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.00912\n",
            "Epoch 147/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9838 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.00912\n",
            "Epoch 148/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9852 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.00912\n",
            "Epoch 149/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9828 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.00912\n",
            "Epoch 150/700\n",
            "2099/2099 [==============================] - 1s 323us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.00912\n",
            "Epoch 151/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9824 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00151: val_loss did not improve from 0.00912\n",
            "Epoch 152/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00152: val_loss did not improve from 0.00912\n",
            "Epoch 153/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0154 - val_mean_absolute_error: 0.0154 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00153: val_loss did not improve from 0.00912\n",
            "Epoch 154/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00154: val_loss did not improve from 0.00912\n",
            "Epoch 155/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00155: val_loss did not improve from 0.00912\n",
            "Epoch 156/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0025 - mean_absolute_error: 0.0025 - acc: 0.9824 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00156: val_loss did not improve from 0.00912\n",
            "Epoch 157/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00157: val_loss did not improve from 0.00912\n",
            "Epoch 158/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00158: val_loss did not improve from 0.00912\n",
            "Epoch 159/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9833 - val_loss: 0.0150 - val_mean_absolute_error: 0.0150 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00159: val_loss did not improve from 0.00912\n",
            "Epoch 160/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00160: val_loss did not improve from 0.00912\n",
            "Epoch 161/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00161: val_loss did not improve from 0.00912\n",
            "Epoch 162/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9862 - val_loss: 0.0162 - val_mean_absolute_error: 0.0162 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00162: val_loss did not improve from 0.00912\n",
            "Epoch 163/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00163: val_loss did not improve from 0.00912\n",
            "Epoch 164/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0157 - val_mean_absolute_error: 0.0157 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00164: val_loss did not improve from 0.00912\n",
            "Epoch 165/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00165: val_loss did not improve from 0.00912\n",
            "Epoch 166/700\n",
            "2099/2099 [==============================] - 1s 359us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00166: val_loss did not improve from 0.00912\n",
            "Epoch 167/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0146 - val_mean_absolute_error: 0.0146 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00167: val_loss did not improve from 0.00912\n",
            "Epoch 168/700\n",
            "2099/2099 [==============================] - 1s 368us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0148 - val_mean_absolute_error: 0.0148 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00168: val_loss did not improve from 0.00912\n",
            "Epoch 169/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0151 - val_mean_absolute_error: 0.0151 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00169: val_loss did not improve from 0.00912\n",
            "Epoch 170/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00170: val_loss did not improve from 0.00912\n",
            "Epoch 171/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9828 - val_loss: 0.0155 - val_mean_absolute_error: 0.0155 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00171: val_loss did not improve from 0.00912\n",
            "Epoch 172/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9838 - val_loss: 0.0153 - val_mean_absolute_error: 0.0153 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00172: val_loss did not improve from 0.00912\n",
            "Epoch 173/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0152 - val_mean_absolute_error: 0.0152 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00173: val_loss did not improve from 0.00912\n",
            "Epoch 174/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9857 - val_loss: 0.0147 - val_mean_absolute_error: 0.0147 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00174: val_loss did not improve from 0.00912\n",
            "Epoch 175/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9862 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00175: val_loss did not improve from 0.00912\n",
            "Epoch 176/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00176: val_loss did not improve from 0.00912\n",
            "Epoch 177/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00177: val_loss did not improve from 0.00912\n",
            "Epoch 178/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9843 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00178: val_loss did not improve from 0.00912\n",
            "Epoch 179/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00179: val_loss did not improve from 0.00912\n",
            "Epoch 180/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9871 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00180: val_loss did not improve from 0.00912\n",
            "Epoch 181/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9862 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00181: val_loss did not improve from 0.00912\n",
            "Epoch 182/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9848 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00182: val_loss did not improve from 0.00912\n",
            "Epoch 183/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9871 - val_loss: 0.0145 - val_mean_absolute_error: 0.0145 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00183: val_loss did not improve from 0.00912\n",
            "Epoch 184/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9871 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00184: val_loss did not improve from 0.00912\n",
            "Epoch 185/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00185: val_loss did not improve from 0.00912\n",
            "Epoch 186/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9852 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00186: val_loss did not improve from 0.00912\n",
            "Epoch 187/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9862 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00187: val_loss did not improve from 0.00912\n",
            "Epoch 188/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00188: val_loss did not improve from 0.00912\n",
            "Epoch 189/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9876 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00189: val_loss did not improve from 0.00912\n",
            "Epoch 190/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9881 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00190: val_loss did not improve from 0.00912\n",
            "Epoch 191/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00191: val_loss did not improve from 0.00912\n",
            "Epoch 192/700\n",
            "2099/2099 [==============================] - 1s 356us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9876 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00192: val_loss did not improve from 0.00912\n",
            "Epoch 193/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00193: val_loss did not improve from 0.00912\n",
            "Epoch 194/700\n",
            "2099/2099 [==============================] - 1s 320us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00194: val_loss did not improve from 0.00912\n",
            "Epoch 195/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9871 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00195: val_loss did not improve from 0.00912\n",
            "Epoch 196/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00196: val_loss did not improve from 0.00912\n",
            "Epoch 197/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9881 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00197: val_loss did not improve from 0.00912\n",
            "Epoch 198/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9886 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00198: val_loss did not improve from 0.00912\n",
            "Epoch 199/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00199: val_loss did not improve from 0.00912\n",
            "Epoch 200/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9876 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00200: val_loss did not improve from 0.00912\n",
            "Epoch 201/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00201: val_loss did not improve from 0.00912\n",
            "Epoch 202/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9867 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00202: val_loss did not improve from 0.00912\n",
            "Epoch 203/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00203: val_loss did not improve from 0.00912\n",
            "Epoch 204/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9909 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00204: val_loss did not improve from 0.00912\n",
            "Epoch 205/700\n",
            "2099/2099 [==============================] - 1s 369us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9881 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00205: val_loss did not improve from 0.00912\n",
            "Epoch 206/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00206: val_loss did not improve from 0.00912\n",
            "Epoch 207/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00207: val_loss did not improve from 0.00912\n",
            "Epoch 208/700\n",
            "2099/2099 [==============================] - 1s 373us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00208: val_loss did not improve from 0.00912\n",
            "Epoch 209/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9900 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00209: val_loss did not improve from 0.00912\n",
            "Epoch 210/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9890 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00210: val_loss did not improve from 0.00912\n",
            "Epoch 211/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00211: val_loss did not improve from 0.00912\n",
            "Epoch 212/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00212: val_loss did not improve from 0.00912\n",
            "Epoch 213/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9900 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00213: val_loss did not improve from 0.00912\n",
            "Epoch 214/700\n",
            "2099/2099 [==============================] - 1s 318us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9881 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00214: val_loss did not improve from 0.00912\n",
            "Epoch 215/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00215: val_loss did not improve from 0.00912\n",
            "Epoch 216/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00216: val_loss did not improve from 0.00912\n",
            "Epoch 217/700\n",
            "2099/2099 [==============================] - 1s 314us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00217: val_loss did not improve from 0.00912\n",
            "Epoch 218/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00218: val_loss did not improve from 0.00912\n",
            "Epoch 219/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9886 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00219: val_loss did not improve from 0.00912\n",
            "Epoch 220/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00220: val_loss did not improve from 0.00912\n",
            "Epoch 221/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9905 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00221: val_loss did not improve from 0.00912\n",
            "Epoch 222/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00222: val_loss did not improve from 0.00912\n",
            "Epoch 223/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00223: val_loss did not improve from 0.00912\n",
            "Epoch 224/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00224: val_loss did not improve from 0.00912\n",
            "Epoch 225/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00225: val_loss did not improve from 0.00912\n",
            "Epoch 226/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00226: val_loss did not improve from 0.00912\n",
            "Epoch 227/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9881 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00227: val_loss did not improve from 0.00912\n",
            "Epoch 228/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00228: val_loss did not improve from 0.00912\n",
            "Epoch 229/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00229: val_loss did not improve from 0.00912\n",
            "Epoch 230/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00230: val_loss did not improve from 0.00912\n",
            "Epoch 231/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00231: val_loss did not improve from 0.00912\n",
            "Epoch 232/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00232: val_loss did not improve from 0.00912\n",
            "Epoch 233/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00233: val_loss did not improve from 0.00912\n",
            "Epoch 234/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00234: val_loss did not improve from 0.00912\n",
            "Epoch 235/700\n",
            "2099/2099 [==============================] - 1s 359us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00235: val_loss did not improve from 0.00912\n",
            "Epoch 236/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00236: val_loss did not improve from 0.00912\n",
            "Epoch 237/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00237: val_loss did not improve from 0.00912\n",
            "Epoch 238/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00238: val_loss did not improve from 0.00912\n",
            "Epoch 239/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0111 - val_mean_absolute_error: 0.0111 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00239: val_loss did not improve from 0.00912\n",
            "Epoch 240/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9867 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00240: val_loss did not improve from 0.00912\n",
            "Epoch 241/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00241: val_loss did not improve from 0.00912\n",
            "Epoch 242/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9871 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00242: val_loss did not improve from 0.00912\n",
            "Epoch 243/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00243: val_loss did not improve from 0.00912\n",
            "Epoch 244/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00244: val_loss did not improve from 0.00912\n",
            "Epoch 245/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00245: val_loss did not improve from 0.00912\n",
            "Epoch 246/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00246: val_loss did not improve from 0.00912\n",
            "Epoch 247/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00247: val_loss did not improve from 0.00912\n",
            "Epoch 248/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00248: val_loss did not improve from 0.00912\n",
            "Epoch 249/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00249: val_loss did not improve from 0.00912\n",
            "Epoch 250/700\n",
            "2099/2099 [==============================] - 1s 366us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0109 - val_mean_absolute_error: 0.0109 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00250: val_loss did not improve from 0.00912\n",
            "Epoch 251/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00251: val_loss did not improve from 0.00912\n",
            "Epoch 252/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9876 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00252: val_loss did not improve from 0.00912\n",
            "Epoch 253/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00253: val_loss did not improve from 0.00912\n",
            "Epoch 254/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00254: val_loss did not improve from 0.00912\n",
            "Epoch 255/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00255: val_loss did not improve from 0.00912\n",
            "Epoch 256/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00256: val_loss did not improve from 0.00912\n",
            "Epoch 257/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00257: val_loss did not improve from 0.00912\n",
            "Epoch 258/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00258: val_loss did not improve from 0.00912\n",
            "Epoch 259/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00259: val_loss did not improve from 0.00912\n",
            "Epoch 260/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00260: val_loss did not improve from 0.00912\n",
            "Epoch 261/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00261: val_loss did not improve from 0.00912\n",
            "Epoch 262/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00262: val_loss did not improve from 0.00912\n",
            "Epoch 263/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00263: val_loss did not improve from 0.00912\n",
            "Epoch 264/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00264: val_loss did not improve from 0.00912\n",
            "Epoch 265/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00265: val_loss did not improve from 0.00912\n",
            "Epoch 266/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00266: val_loss did not improve from 0.00912\n",
            "Epoch 267/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00267: val_loss did not improve from 0.00912\n",
            "Epoch 268/700\n",
            "2099/2099 [==============================] - 1s 321us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00268: val_loss did not improve from 0.00912\n",
            "Epoch 269/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00269: val_loss did not improve from 0.00912\n",
            "Epoch 270/700\n",
            "2099/2099 [==============================] - 1s 322us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00270: val_loss did not improve from 0.00912\n",
            "Epoch 271/700\n",
            "2099/2099 [==============================] - 1s 322us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00271: val_loss did not improve from 0.00912\n",
            "Epoch 272/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00272: val_loss did not improve from 0.00912\n",
            "Epoch 273/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9867 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00273: val_loss did not improve from 0.00912\n",
            "Epoch 274/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0024 - mean_absolute_error: 0.0024 - acc: 0.9895 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00274: val_loss did not improve from 0.00912\n",
            "Epoch 275/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00275: val_loss did not improve from 0.00912\n",
            "Epoch 276/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00276: val_loss did not improve from 0.00912\n",
            "Epoch 277/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00277: val_loss did not improve from 0.00912\n",
            "Epoch 278/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00278: val_loss did not improve from 0.00912\n",
            "Epoch 279/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00279: val_loss did not improve from 0.00912\n",
            "Epoch 280/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00280: val_loss did not improve from 0.00912\n",
            "Epoch 281/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9871 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00281: val_loss did not improve from 0.00912\n",
            "Epoch 282/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00282: val_loss did not improve from 0.00912\n",
            "Epoch 283/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00283: val_loss did not improve from 0.00912\n",
            "Epoch 284/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00284: val_loss did not improve from 0.00912\n",
            "Epoch 285/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00285: val_loss did not improve from 0.00912\n",
            "Epoch 286/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00286: val_loss did not improve from 0.00912\n",
            "Epoch 287/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00287: val_loss did not improve from 0.00912\n",
            "Epoch 288/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00288: val_loss did not improve from 0.00912\n",
            "Epoch 289/700\n",
            "2099/2099 [==============================] - 1s 364us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00289: val_loss did not improve from 0.00912\n",
            "Epoch 290/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00290: val_loss did not improve from 0.00912\n",
            "Epoch 291/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00291: val_loss did not improve from 0.00912\n",
            "Epoch 292/700\n",
            "2099/2099 [==============================] - 1s 363us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00292: val_loss did not improve from 0.00912\n",
            "Epoch 293/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00293: val_loss did not improve from 0.00912\n",
            "Epoch 294/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00294: val_loss did not improve from 0.00912\n",
            "Epoch 295/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00295: val_loss did not improve from 0.00912\n",
            "Epoch 296/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00296: val_loss did not improve from 0.00912\n",
            "Epoch 297/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00297: val_loss did not improve from 0.00912\n",
            "Epoch 298/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00298: val_loss did not improve from 0.00912\n",
            "Epoch 299/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00299: val_loss did not improve from 0.00912\n",
            "Epoch 300/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00300: val_loss did not improve from 0.00912\n",
            "Epoch 301/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00301: val_loss did not improve from 0.00912\n",
            "Epoch 302/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00302: val_loss did not improve from 0.00912\n",
            "Epoch 303/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00303: val_loss did not improve from 0.00912\n",
            "Epoch 304/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00304: val_loss did not improve from 0.00912\n",
            "Epoch 305/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00305: val_loss did not improve from 0.00912\n",
            "Epoch 306/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00306: val_loss did not improve from 0.00912\n",
            "Epoch 307/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00307: val_loss did not improve from 0.00912\n",
            "Epoch 308/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00308: val_loss did not improve from 0.00912\n",
            "Epoch 309/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00309: val_loss did not improve from 0.00912\n",
            "Epoch 310/700\n",
            "2099/2099 [==============================] - 1s 324us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00310: val_loss did not improve from 0.00912\n",
            "Epoch 311/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00311: val_loss did not improve from 0.00912\n",
            "Epoch 312/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00312: val_loss did not improve from 0.00912\n",
            "Epoch 313/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00313: val_loss did not improve from 0.00912\n",
            "Epoch 314/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00314: val_loss did not improve from 0.00912\n",
            "Epoch 315/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00315: val_loss did not improve from 0.00912\n",
            "Epoch 316/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9876 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00316: val_loss did not improve from 0.00912\n",
            "Epoch 317/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00317: val_loss did not improve from 0.00912\n",
            "Epoch 318/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00318: val_loss did not improve from 0.00912\n",
            "Epoch 319/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00319: val_loss did not improve from 0.00912\n",
            "Epoch 320/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00320: val_loss did not improve from 0.00912\n",
            "Epoch 321/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00321: val_loss did not improve from 0.00912\n",
            "Epoch 322/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00322: val_loss did not improve from 0.00912\n",
            "Epoch 323/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00323: val_loss did not improve from 0.00912\n",
            "Epoch 324/700\n",
            "2099/2099 [==============================] - 1s 316us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00324: val_loss did not improve from 0.00912\n",
            "Epoch 325/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9881 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00325: val_loss did not improve from 0.00912\n",
            "Epoch 326/700\n",
            "2099/2099 [==============================] - 1s 356us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00326: val_loss did not improve from 0.00912\n",
            "Epoch 327/700\n",
            "2099/2099 [==============================] - 1s 318us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00327: val_loss did not improve from 0.00912\n",
            "Epoch 328/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9886 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00328: val_loss did not improve from 0.00912\n",
            "Epoch 329/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00329: val_loss did not improve from 0.00912\n",
            "Epoch 330/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00330: val_loss did not improve from 0.00912\n",
            "Epoch 331/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00331: val_loss did not improve from 0.00912\n",
            "Epoch 332/700\n",
            "2099/2099 [==============================] - 1s 361us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00332: val_loss did not improve from 0.00912\n",
            "Epoch 333/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00333: val_loss did not improve from 0.00912\n",
            "Epoch 334/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00334: val_loss did not improve from 0.00912\n",
            "Epoch 335/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00335: val_loss did not improve from 0.00912\n",
            "Epoch 336/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00336: val_loss did not improve from 0.00912\n",
            "Epoch 337/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00337: val_loss did not improve from 0.00912\n",
            "Epoch 338/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00338: val_loss did not improve from 0.00912\n",
            "Epoch 339/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9890 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00339: val_loss did not improve from 0.00912\n",
            "Epoch 340/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00340: val_loss did not improve from 0.00912\n",
            "Epoch 341/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00341: val_loss did not improve from 0.00912\n",
            "Epoch 342/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00342: val_loss did not improve from 0.00912\n",
            "Epoch 343/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00343: val_loss did not improve from 0.00912\n",
            "Epoch 344/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00344: val_loss did not improve from 0.00912\n",
            "Epoch 345/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9895 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00345: val_loss did not improve from 0.00912\n",
            "Epoch 346/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00346: val_loss did not improve from 0.00912\n",
            "Epoch 347/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00347: val_loss did not improve from 0.00912\n",
            "Epoch 348/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00348: val_loss did not improve from 0.00912\n",
            "Epoch 349/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00349: val_loss did not improve from 0.00912\n",
            "Epoch 350/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00350: val_loss did not improve from 0.00912\n",
            "Epoch 351/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00351: val_loss did not improve from 0.00912\n",
            "Epoch 352/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00352: val_loss did not improve from 0.00912\n",
            "Epoch 353/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00353: val_loss did not improve from 0.00912\n",
            "Epoch 354/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00354: val_loss did not improve from 0.00912\n",
            "Epoch 355/700\n",
            "2099/2099 [==============================] - 1s 372us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00355: val_loss did not improve from 0.00912\n",
            "Epoch 356/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00356: val_loss did not improve from 0.00912\n",
            "Epoch 357/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00357: val_loss did not improve from 0.00912\n",
            "Epoch 358/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00358: val_loss did not improve from 0.00912\n",
            "Epoch 359/700\n",
            "2099/2099 [==============================] - 1s 369us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00359: val_loss did not improve from 0.00912\n",
            "Epoch 360/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00360: val_loss did not improve from 0.00912\n",
            "Epoch 361/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00361: val_loss did not improve from 0.00912\n",
            "Epoch 362/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00362: val_loss did not improve from 0.00912\n",
            "Epoch 363/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00363: val_loss did not improve from 0.00912\n",
            "Epoch 364/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00364: val_loss did not improve from 0.00912\n",
            "Epoch 365/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00365: val_loss did not improve from 0.00912\n",
            "Epoch 366/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00366: val_loss did not improve from 0.00912\n",
            "Epoch 367/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00367: val_loss did not improve from 0.00912\n",
            "Epoch 368/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00368: val_loss did not improve from 0.00912\n",
            "Epoch 369/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9900 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00369: val_loss did not improve from 0.00912\n",
            "Epoch 370/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00370: val_loss did not improve from 0.00912\n",
            "Epoch 371/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0110 - val_mean_absolute_error: 0.0110 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00371: val_loss did not improve from 0.00912\n",
            "Epoch 372/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00372: val_loss did not improve from 0.00912\n",
            "Epoch 373/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00373: val_loss did not improve from 0.00912\n",
            "Epoch 374/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00374: val_loss did not improve from 0.00912\n",
            "Epoch 375/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00375: val_loss did not improve from 0.00912\n",
            "Epoch 376/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00376: val_loss did not improve from 0.00912\n",
            "Epoch 377/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00377: val_loss did not improve from 0.00912\n",
            "Epoch 378/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00378: val_loss did not improve from 0.00912\n",
            "Epoch 379/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00379: val_loss did not improve from 0.00912\n",
            "Epoch 380/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0114 - val_mean_absolute_error: 0.0114 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00380: val_loss did not improve from 0.00912\n",
            "Epoch 381/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00381: val_loss did not improve from 0.00912\n",
            "Epoch 382/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00382: val_loss did not improve from 0.00912\n",
            "Epoch 383/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00383: val_loss did not improve from 0.00912\n",
            "Epoch 384/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00384: val_loss did not improve from 0.00912\n",
            "Epoch 385/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00385: val_loss did not improve from 0.00912\n",
            "Epoch 386/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00386: val_loss did not improve from 0.00912\n",
            "Epoch 387/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00387: val_loss did not improve from 0.00912\n",
            "Epoch 388/700\n",
            "2099/2099 [==============================] - 1s 361us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00388: val_loss did not improve from 0.00912\n",
            "Epoch 389/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00389: val_loss did not improve from 0.00912\n",
            "Epoch 390/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9905 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00390: val_loss did not improve from 0.00912\n",
            "Epoch 391/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00391: val_loss did not improve from 0.00912\n",
            "Epoch 392/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00392: val_loss did not improve from 0.00912\n",
            "Epoch 393/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00393: val_loss did not improve from 0.00912\n",
            "Epoch 394/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00394: val_loss did not improve from 0.00912\n",
            "Epoch 395/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00395: val_loss did not improve from 0.00912\n",
            "Epoch 396/700\n",
            "2099/2099 [==============================] - 1s 323us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00396: val_loss did not improve from 0.00912\n",
            "Epoch 397/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00397: val_loss did not improve from 0.00912\n",
            "Epoch 398/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00398: val_loss did not improve from 0.00912\n",
            "Epoch 399/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00399: val_loss did not improve from 0.00912\n",
            "Epoch 400/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00400: val_loss did not improve from 0.00912\n",
            "Epoch 401/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00401: val_loss did not improve from 0.00912\n",
            "Epoch 402/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0113 - val_mean_absolute_error: 0.0113 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00402: val_loss did not improve from 0.00912\n",
            "Epoch 403/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00403: val_loss did not improve from 0.00912\n",
            "Epoch 404/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00404: val_loss did not improve from 0.00912\n",
            "Epoch 405/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00405: val_loss did not improve from 0.00912\n",
            "Epoch 406/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00406: val_loss did not improve from 0.00912\n",
            "Epoch 407/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00407: val_loss did not improve from 0.00912\n",
            "Epoch 408/700\n",
            "2099/2099 [==============================] - 1s 356us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00408: val_loss did not improve from 0.00912\n",
            "Epoch 409/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00409: val_loss did not improve from 0.00912\n",
            "Epoch 410/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00410: val_loss did not improve from 0.00912\n",
            "Epoch 411/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00411: val_loss did not improve from 0.00912\n",
            "Epoch 412/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00412: val_loss did not improve from 0.00912\n",
            "Epoch 413/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00413: val_loss did not improve from 0.00912\n",
            "Epoch 414/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00414: val_loss did not improve from 0.00912\n",
            "Epoch 415/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00415: val_loss did not improve from 0.00912\n",
            "Epoch 416/700\n",
            "2099/2099 [==============================] - 1s 323us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00416: val_loss did not improve from 0.00912\n",
            "Epoch 417/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00417: val_loss did not improve from 0.00912\n",
            "Epoch 418/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00418: val_loss did not improve from 0.00912\n",
            "Epoch 419/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00419: val_loss did not improve from 0.00912\n",
            "Epoch 420/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00420: val_loss did not improve from 0.00912\n",
            "Epoch 421/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00421: val_loss did not improve from 0.00912\n",
            "Epoch 422/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00422: val_loss did not improve from 0.00912\n",
            "Epoch 423/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00423: val_loss did not improve from 0.00912\n",
            "Epoch 424/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00424: val_loss did not improve from 0.00912\n",
            "Epoch 425/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00425: val_loss did not improve from 0.00912\n",
            "Epoch 426/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00426: val_loss did not improve from 0.00912\n",
            "Epoch 427/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00427: val_loss did not improve from 0.00912\n",
            "Epoch 428/700\n",
            "2099/2099 [==============================] - 1s 367us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00428: val_loss did not improve from 0.00912\n",
            "Epoch 429/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00429: val_loss did not improve from 0.00912\n",
            "Epoch 430/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00430: val_loss did not improve from 0.00912\n",
            "Epoch 431/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00431: val_loss did not improve from 0.00912\n",
            "Epoch 432/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00432: val_loss did not improve from 0.00912\n",
            "Epoch 433/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00433: val_loss did not improve from 0.00912\n",
            "Epoch 434/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00434: val_loss did not improve from 0.00912\n",
            "Epoch 435/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00435: val_loss did not improve from 0.00912\n",
            "Epoch 436/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00436: val_loss did not improve from 0.00912\n",
            "Epoch 437/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00437: val_loss did not improve from 0.00912\n",
            "Epoch 438/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00438: val_loss did not improve from 0.00912\n",
            "Epoch 439/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00439: val_loss did not improve from 0.00912\n",
            "Epoch 440/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00440: val_loss did not improve from 0.00912\n",
            "Epoch 441/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00441: val_loss did not improve from 0.00912\n",
            "Epoch 442/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00442: val_loss did not improve from 0.00912\n",
            "Epoch 443/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00443: val_loss did not improve from 0.00912\n",
            "Epoch 444/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00444: val_loss did not improve from 0.00912\n",
            "Epoch 445/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00445: val_loss did not improve from 0.00912\n",
            "Epoch 446/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00446: val_loss did not improve from 0.00912\n",
            "Epoch 447/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00447: val_loss did not improve from 0.00912\n",
            "Epoch 448/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00448: val_loss did not improve from 0.00912\n",
            "Epoch 449/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00449: val_loss did not improve from 0.00912\n",
            "Epoch 450/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00450: val_loss did not improve from 0.00912\n",
            "Epoch 451/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00451: val_loss did not improve from 0.00912\n",
            "Epoch 452/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00452: val_loss did not improve from 0.00912\n",
            "Epoch 453/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00453: val_loss did not improve from 0.00912\n",
            "Epoch 454/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00454: val_loss did not improve from 0.00912\n",
            "Epoch 455/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00455: val_loss did not improve from 0.00912\n",
            "Epoch 456/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00456: val_loss did not improve from 0.00912\n",
            "Epoch 457/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00457: val_loss did not improve from 0.00912\n",
            "Epoch 458/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00458: val_loss did not improve from 0.00912\n",
            "Epoch 459/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00459: val_loss did not improve from 0.00912\n",
            "Epoch 460/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00460: val_loss did not improve from 0.00912\n",
            "Epoch 461/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00461: val_loss did not improve from 0.00912\n",
            "Epoch 462/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00462: val_loss did not improve from 0.00912\n",
            "Epoch 463/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00463: val_loss did not improve from 0.00912\n",
            "Epoch 464/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00464: val_loss did not improve from 0.00912\n",
            "Epoch 465/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00465: val_loss did not improve from 0.00912\n",
            "Epoch 466/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00466: val_loss did not improve from 0.00912\n",
            "Epoch 467/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00467: val_loss did not improve from 0.00912\n",
            "Epoch 468/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00468: val_loss did not improve from 0.00912\n",
            "Epoch 469/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00469: val_loss did not improve from 0.00912\n",
            "Epoch 470/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00470: val_loss did not improve from 0.00912\n",
            "Epoch 471/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00471: val_loss did not improve from 0.00912\n",
            "Epoch 472/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00472: val_loss did not improve from 0.00912\n",
            "Epoch 473/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00473: val_loss did not improve from 0.00912\n",
            "Epoch 474/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00474: val_loss did not improve from 0.00912\n",
            "Epoch 475/700\n",
            "2099/2099 [==============================] - 1s 320us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00475: val_loss did not improve from 0.00912\n",
            "Epoch 476/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00476: val_loss did not improve from 0.00912\n",
            "Epoch 477/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00477: val_loss did not improve from 0.00912\n",
            "Epoch 478/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00478: val_loss did not improve from 0.00912\n",
            "Epoch 479/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00479: val_loss did not improve from 0.00912\n",
            "Epoch 480/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00480: val_loss did not improve from 0.00912\n",
            "Epoch 481/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00481: val_loss did not improve from 0.00912\n",
            "Epoch 482/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00482: val_loss did not improve from 0.00912\n",
            "Epoch 483/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00483: val_loss did not improve from 0.00912\n",
            "Epoch 484/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00484: val_loss did not improve from 0.00912\n",
            "Epoch 485/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00485: val_loss did not improve from 0.00912\n",
            "Epoch 486/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00486: val_loss did not improve from 0.00912\n",
            "Epoch 487/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00487: val_loss did not improve from 0.00912\n",
            "Epoch 488/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00488: val_loss did not improve from 0.00912\n",
            "Epoch 489/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00489: val_loss did not improve from 0.00912\n",
            "Epoch 490/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00490: val_loss did not improve from 0.00912\n",
            "Epoch 491/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00491: val_loss did not improve from 0.00912\n",
            "Epoch 492/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00492: val_loss did not improve from 0.00912\n",
            "Epoch 493/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00493: val_loss did not improve from 0.00912\n",
            "Epoch 494/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00494: val_loss did not improve from 0.00912\n",
            "Epoch 495/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00495: val_loss did not improve from 0.00912\n",
            "Epoch 496/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00496: val_loss did not improve from 0.00912\n",
            "Epoch 497/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00497: val_loss did not improve from 0.00912\n",
            "Epoch 498/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00498: val_loss did not improve from 0.00912\n",
            "Epoch 499/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00499: val_loss did not improve from 0.00912\n",
            "Epoch 500/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00500: val_loss did not improve from 0.00912\n",
            "Epoch 501/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00501: val_loss did not improve from 0.00912\n",
            "Epoch 502/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00502: val_loss did not improve from 0.00912\n",
            "Epoch 503/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00503: val_loss did not improve from 0.00912\n",
            "Epoch 504/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00504: val_loss did not improve from 0.00912\n",
            "Epoch 505/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00505: val_loss did not improve from 0.00912\n",
            "Epoch 506/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00506: val_loss did not improve from 0.00912\n",
            "Epoch 507/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00507: val_loss did not improve from 0.00912\n",
            "Epoch 508/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9924 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00508: val_loss did not improve from 0.00912\n",
            "Epoch 509/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00509: val_loss did not improve from 0.00912\n",
            "Epoch 510/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00510: val_loss did not improve from 0.00912\n",
            "Epoch 511/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00511: val_loss did not improve from 0.00912\n",
            "Epoch 512/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00512: val_loss did not improve from 0.00912\n",
            "Epoch 513/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00513: val_loss did not improve from 0.00912\n",
            "Epoch 514/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00514: val_loss did not improve from 0.00912\n",
            "Epoch 515/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00515: val_loss did not improve from 0.00912\n",
            "Epoch 516/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9957 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00516: val_loss did not improve from 0.00912\n",
            "Epoch 517/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00517: val_loss did not improve from 0.00912\n",
            "Epoch 518/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9976 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00518: val_loss did not improve from 0.00912\n",
            "Epoch 519/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00519: val_loss did not improve from 0.00912\n",
            "Epoch 520/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00520: val_loss did not improve from 0.00912\n",
            "Epoch 521/700\n",
            "2099/2099 [==============================] - 1s 319us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00521: val_loss did not improve from 0.00912\n",
            "Epoch 522/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00522: val_loss did not improve from 0.00912\n",
            "Epoch 523/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9943 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00523: val_loss did not improve from 0.00912\n",
            "Epoch 524/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00524: val_loss did not improve from 0.00912\n",
            "Epoch 525/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00525: val_loss did not improve from 0.00912\n",
            "Epoch 526/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00526: val_loss did not improve from 0.00912\n",
            "Epoch 527/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00527: val_loss did not improve from 0.00912\n",
            "Epoch 528/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00528: val_loss did not improve from 0.00912\n",
            "Epoch 529/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00529: val_loss did not improve from 0.00912\n",
            "Epoch 530/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00530: val_loss did not improve from 0.00912\n",
            "Epoch 531/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00531: val_loss did not improve from 0.00912\n",
            "Epoch 532/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00532: val_loss did not improve from 0.00912\n",
            "Epoch 533/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00533: val_loss did not improve from 0.00912\n",
            "Epoch 534/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00534: val_loss did not improve from 0.00912\n",
            "Epoch 535/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00535: val_loss did not improve from 0.00912\n",
            "Epoch 536/700\n",
            "2099/2099 [==============================] - 1s 317us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00536: val_loss did not improve from 0.00912\n",
            "Epoch 537/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00537: val_loss did not improve from 0.00912\n",
            "Epoch 538/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00538: val_loss did not improve from 0.00912\n",
            "Epoch 539/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00539: val_loss did not improve from 0.00912\n",
            "Epoch 540/700\n",
            "2099/2099 [==============================] - 1s 319us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00540: val_loss did not improve from 0.00912\n",
            "Epoch 541/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00541: val_loss did not improve from 0.00912\n",
            "Epoch 542/700\n",
            "2099/2099 [==============================] - 1s 368us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9909 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00542: val_loss did not improve from 0.00912\n",
            "Epoch 543/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00543: val_loss did not improve from 0.00912\n",
            "Epoch 544/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00544: val_loss did not improve from 0.00912\n",
            "Epoch 545/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9952 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00545: val_loss did not improve from 0.00912\n",
            "Epoch 546/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00546: val_loss did not improve from 0.00912\n",
            "Epoch 547/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00547: val_loss did not improve from 0.00912\n",
            "Epoch 548/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00548: val_loss did not improve from 0.00912\n",
            "Epoch 549/700\n",
            "2099/2099 [==============================] - 1s 358us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00549: val_loss did not improve from 0.00912\n",
            "Epoch 550/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00550: val_loss did not improve from 0.00912\n",
            "Epoch 551/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00551: val_loss did not improve from 0.00912\n",
            "Epoch 552/700\n",
            "2099/2099 [==============================] - 1s 351us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00552: val_loss did not improve from 0.00912\n",
            "Epoch 553/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00553: val_loss did not improve from 0.00912\n",
            "Epoch 554/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00554: val_loss did not improve from 0.00912\n",
            "Epoch 555/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00555: val_loss did not improve from 0.00912\n",
            "Epoch 556/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0116 - val_mean_absolute_error: 0.0116 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00556: val_loss did not improve from 0.00912\n",
            "Epoch 557/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00557: val_loss did not improve from 0.00912\n",
            "Epoch 558/700\n",
            "2099/2099 [==============================] - 1s 313us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00558: val_loss did not improve from 0.00912\n",
            "Epoch 559/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00559: val_loss did not improve from 0.00912\n",
            "Epoch 560/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00560: val_loss did not improve from 0.00912\n",
            "Epoch 561/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00561: val_loss did not improve from 0.00912\n",
            "Epoch 562/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00562: val_loss did not improve from 0.00912\n",
            "Epoch 563/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00563: val_loss did not improve from 0.00912\n",
            "Epoch 564/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00564: val_loss did not improve from 0.00912\n",
            "Epoch 565/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9952 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00565: val_loss did not improve from 0.00912\n",
            "Epoch 566/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00566: val_loss did not improve from 0.00912\n",
            "Epoch 567/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00567: val_loss did not improve from 0.00912\n",
            "Epoch 568/700\n",
            "2099/2099 [==============================] - 1s 320us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00568: val_loss did not improve from 0.00912\n",
            "Epoch 569/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0115 - val_mean_absolute_error: 0.0115 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00569: val_loss did not improve from 0.00912\n",
            "Epoch 570/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00570: val_loss did not improve from 0.00912\n",
            "Epoch 571/700\n",
            "2099/2099 [==============================] - 1s 366us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00571: val_loss did not improve from 0.00912\n",
            "Epoch 572/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00572: val_loss did not improve from 0.00912\n",
            "Epoch 573/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9924 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00573: val_loss did not improve from 0.00912\n",
            "Epoch 574/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00574: val_loss did not improve from 0.00912\n",
            "Epoch 575/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00575: val_loss did not improve from 0.00912\n",
            "Epoch 576/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00576: val_loss did not improve from 0.00912\n",
            "Epoch 577/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00577: val_loss did not improve from 0.00912\n",
            "Epoch 578/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00578: val_loss did not improve from 0.00912\n",
            "Epoch 579/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00579: val_loss did not improve from 0.00912\n",
            "Epoch 580/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00580: val_loss did not improve from 0.00912\n",
            "Epoch 581/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0119 - val_mean_absolute_error: 0.0119 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00581: val_loss did not improve from 0.00912\n",
            "Epoch 582/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00582: val_loss did not improve from 0.00912\n",
            "Epoch 583/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00583: val_loss did not improve from 0.00912\n",
            "Epoch 584/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9943 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00584: val_loss did not improve from 0.00912\n",
            "Epoch 585/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00585: val_loss did not improve from 0.00912\n",
            "Epoch 586/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00586: val_loss did not improve from 0.00912\n",
            "Epoch 587/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00587: val_loss did not improve from 0.00912\n",
            "Epoch 588/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00588: val_loss did not improve from 0.00912\n",
            "Epoch 589/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9943 - val_loss: 0.0117 - val_mean_absolute_error: 0.0117 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00589: val_loss did not improve from 0.00912\n",
            "Epoch 590/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00590: val_loss did not improve from 0.00912\n",
            "Epoch 591/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00591: val_loss did not improve from 0.00912\n",
            "Epoch 592/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00592: val_loss did not improve from 0.00912\n",
            "Epoch 593/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00593: val_loss did not improve from 0.00912\n",
            "Epoch 594/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00594: val_loss did not improve from 0.00912\n",
            "Epoch 595/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00595: val_loss did not improve from 0.00912\n",
            "Epoch 596/700\n",
            "2099/2099 [==============================] - 1s 355us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00596: val_loss did not improve from 0.00912\n",
            "Epoch 597/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00597: val_loss did not improve from 0.00912\n",
            "Epoch 598/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00598: val_loss did not improve from 0.00912\n",
            "Epoch 599/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00599: val_loss did not improve from 0.00912\n",
            "Epoch 600/700\n",
            "2099/2099 [==============================] - 1s 322us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00600: val_loss did not improve from 0.00912\n",
            "Epoch 601/700\n",
            "2099/2099 [==============================] - 1s 318us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00601: val_loss did not improve from 0.00912\n",
            "Epoch 602/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00602: val_loss did not improve from 0.00912\n",
            "Epoch 603/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00603: val_loss did not improve from 0.00912\n",
            "Epoch 604/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00604: val_loss did not improve from 0.00912\n",
            "Epoch 605/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00605: val_loss did not improve from 0.00912\n",
            "Epoch 606/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00606: val_loss did not improve from 0.00912\n",
            "Epoch 607/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00607: val_loss did not improve from 0.00912\n",
            "Epoch 608/700\n",
            "2099/2099 [==============================] - 1s 314us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00608: val_loss did not improve from 0.00912\n",
            "Epoch 609/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0122 - val_mean_absolute_error: 0.0122 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00609: val_loss did not improve from 0.00912\n",
            "Epoch 610/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00610: val_loss did not improve from 0.00912\n",
            "Epoch 611/700\n",
            "2099/2099 [==============================] - 1s 331us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00611: val_loss did not improve from 0.00912\n",
            "Epoch 612/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0118 - val_mean_absolute_error: 0.0118 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00612: val_loss did not improve from 0.00912\n",
            "Epoch 613/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00613: val_loss did not improve from 0.00912\n",
            "Epoch 614/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00614: val_loss did not improve from 0.00912\n",
            "Epoch 615/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00615: val_loss did not improve from 0.00912\n",
            "Epoch 616/700\n",
            "2099/2099 [==============================] - 1s 320us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00616: val_loss did not improve from 0.00912\n",
            "Epoch 617/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00617: val_loss did not improve from 0.00912\n",
            "Epoch 618/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9924 - val_loss: 0.0126 - val_mean_absolute_error: 0.0126 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00618: val_loss did not improve from 0.00912\n",
            "Epoch 619/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00619: val_loss did not improve from 0.00912\n",
            "Epoch 620/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00620: val_loss did not improve from 0.00912\n",
            "Epoch 621/700\n",
            "2099/2099 [==============================] - 1s 318us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0120 - val_mean_absolute_error: 0.0120 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00621: val_loss did not improve from 0.00912\n",
            "Epoch 622/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0121 - val_mean_absolute_error: 0.0121 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00622: val_loss did not improve from 0.00912\n",
            "Epoch 623/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0123 - val_mean_absolute_error: 0.0123 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00623: val_loss did not improve from 0.00912\n",
            "Epoch 624/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00624: val_loss did not improve from 0.00912\n",
            "Epoch 625/700\n",
            "2099/2099 [==============================] - 1s 333us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00625: val_loss did not improve from 0.00912\n",
            "Epoch 626/700\n",
            "2099/2099 [==============================] - 1s 340us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9957 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00626: val_loss did not improve from 0.00912\n",
            "Epoch 627/700\n",
            "2099/2099 [==============================] - 1s 360us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00627: val_loss did not improve from 0.00912\n",
            "Epoch 628/700\n",
            "2099/2099 [==============================] - 1s 321us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00628: val_loss did not improve from 0.00912\n",
            "Epoch 629/700\n",
            "2099/2099 [==============================] - 1s 354us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00629: val_loss did not improve from 0.00912\n",
            "Epoch 630/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00630: val_loss did not improve from 0.00912\n",
            "Epoch 631/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00631: val_loss did not improve from 0.00912\n",
            "Epoch 632/700\n",
            "2099/2099 [==============================] - 1s 362us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00632: val_loss did not improve from 0.00912\n",
            "Epoch 633/700\n",
            "2099/2099 [==============================] - 1s 357us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00633: val_loss did not improve from 0.00912\n",
            "Epoch 634/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00634: val_loss did not improve from 0.00912\n",
            "Epoch 635/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0129 - val_mean_absolute_error: 0.0129 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00635: val_loss did not improve from 0.00912\n",
            "Epoch 636/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00636: val_loss did not improve from 0.00912\n",
            "Epoch 637/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00637: val_loss did not improve from 0.00912\n",
            "Epoch 638/700\n",
            "2099/2099 [==============================] - 1s 347us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00638: val_loss did not improve from 0.00912\n",
            "Epoch 639/700\n",
            "2099/2099 [==============================] - 1s 350us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9957 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00639: val_loss did not improve from 0.00912\n",
            "Epoch 640/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0124 - val_mean_absolute_error: 0.0124 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00640: val_loss did not improve from 0.00912\n",
            "Epoch 641/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00641: val_loss did not improve from 0.00912\n",
            "Epoch 642/700\n",
            "2099/2099 [==============================] - 1s 348us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00642: val_loss did not improve from 0.00912\n",
            "Epoch 643/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0125 - val_mean_absolute_error: 0.0125 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00643: val_loss did not improve from 0.00912\n",
            "Epoch 644/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00644: val_loss did not improve from 0.00912\n",
            "Epoch 645/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00645: val_loss did not improve from 0.00912\n",
            "Epoch 646/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9919 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00646: val_loss did not improve from 0.00912\n",
            "Epoch 647/700\n",
            "2099/2099 [==============================] - 1s 330us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00647: val_loss did not improve from 0.00912\n",
            "Epoch 648/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00648: val_loss did not improve from 0.00912\n",
            "Epoch 649/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0139 - val_mean_absolute_error: 0.0139 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00649: val_loss did not improve from 0.00912\n",
            "Epoch 650/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9924 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00650: val_loss did not improve from 0.00912\n",
            "Epoch 651/700\n",
            "2099/2099 [==============================] - 1s 323us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9919 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00651: val_loss did not improve from 0.00912\n",
            "Epoch 652/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00652: val_loss did not improve from 0.00912\n",
            "Epoch 653/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00653: val_loss did not improve from 0.00912\n",
            "Epoch 654/700\n",
            "2099/2099 [==============================] - 1s 313us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00654: val_loss did not improve from 0.00912\n",
            "Epoch 655/700\n",
            "2099/2099 [==============================] - 1s 319us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00655: val_loss did not improve from 0.00912\n",
            "Epoch 656/700\n",
            "2099/2099 [==============================] - 1s 324us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00656: val_loss did not improve from 0.00912\n",
            "Epoch 657/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00657: val_loss did not improve from 0.00912\n",
            "Epoch 658/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0136 - val_mean_absolute_error: 0.0136 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00658: val_loss did not improve from 0.00912\n",
            "Epoch 659/700\n",
            "2099/2099 [==============================] - 1s 334us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9919 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00659: val_loss did not improve from 0.00912\n",
            "Epoch 660/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00660: val_loss did not improve from 0.00912\n",
            "Epoch 661/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00661: val_loss did not improve from 0.00912\n",
            "Epoch 662/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0131 - val_mean_absolute_error: 0.0131 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00662: val_loss did not improve from 0.00912\n",
            "Epoch 663/700\n",
            "2099/2099 [==============================] - 1s 345us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9914 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00663: val_loss did not improve from 0.00912\n",
            "Epoch 664/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00664: val_loss did not improve from 0.00912\n",
            "Epoch 665/700\n",
            "2099/2099 [==============================] - 1s 336us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0144 - val_mean_absolute_error: 0.0144 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00665: val_loss did not improve from 0.00912\n",
            "Epoch 666/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00666: val_loss did not improve from 0.00912\n",
            "Epoch 667/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00667: val_loss did not improve from 0.00912\n",
            "Epoch 668/700\n",
            "2099/2099 [==============================] - 1s 346us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00668: val_loss did not improve from 0.00912\n",
            "Epoch 669/700\n",
            "2099/2099 [==============================] - 1s 359us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00669: val_loss did not improve from 0.00912\n",
            "Epoch 670/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9933 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00670: val_loss did not improve from 0.00912\n",
            "Epoch 671/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00671: val_loss did not improve from 0.00912\n",
            "Epoch 672/700\n",
            "2099/2099 [==============================] - 1s 328us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00672: val_loss did not improve from 0.00912\n",
            "Epoch 673/700\n",
            "2099/2099 [==============================] - 1s 327us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00673: val_loss did not improve from 0.00912\n",
            "Epoch 674/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00674: val_loss did not improve from 0.00912\n",
            "Epoch 675/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0127 - val_mean_absolute_error: 0.0127 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00675: val_loss did not improve from 0.00912\n",
            "Epoch 676/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9919 - val_loss: 0.0128 - val_mean_absolute_error: 0.0128 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00676: val_loss did not improve from 0.00912\n",
            "Epoch 677/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00677: val_loss did not improve from 0.00912\n",
            "Epoch 678/700\n",
            "2099/2099 [==============================] - 1s 349us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00678: val_loss did not improve from 0.00912\n",
            "Epoch 679/700\n",
            "2099/2099 [==============================] - 1s 344us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9924 - val_loss: 0.0132 - val_mean_absolute_error: 0.0132 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00679: val_loss did not improve from 0.00912\n",
            "Epoch 680/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00680: val_loss did not improve from 0.00912\n",
            "Epoch 681/700\n",
            "2099/2099 [==============================] - 1s 353us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00681: val_loss did not improve from 0.00912\n",
            "Epoch 682/700\n",
            "2099/2099 [==============================] - 1s 335us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00682: val_loss did not improve from 0.00912\n",
            "Epoch 683/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9919 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00683: val_loss did not improve from 0.00912\n",
            "Epoch 684/700\n",
            "2099/2099 [==============================] - 1s 326us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0135 - val_mean_absolute_error: 0.0135 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00684: val_loss did not improve from 0.00912\n",
            "Epoch 685/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9909 - val_loss: 0.0133 - val_mean_absolute_error: 0.0133 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00685: val_loss did not improve from 0.00912\n",
            "Epoch 686/700\n",
            "2099/2099 [==============================] - 1s 337us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00686: val_loss did not improve from 0.00912\n",
            "Epoch 687/700\n",
            "2099/2099 [==============================] - 1s 342us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9943 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00687: val_loss did not improve from 0.00912\n",
            "Epoch 688/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0138 - val_mean_absolute_error: 0.0138 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00688: val_loss did not improve from 0.00912\n",
            "Epoch 689/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00689: val_loss did not improve from 0.00912\n",
            "Epoch 690/700\n",
            "2099/2099 [==============================] - 1s 341us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9929 - val_loss: 0.0143 - val_mean_absolute_error: 0.0143 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00690: val_loss did not improve from 0.00912\n",
            "Epoch 691/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0130 - val_mean_absolute_error: 0.0130 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00691: val_loss did not improve from 0.00912\n",
            "Epoch 692/700\n",
            "2099/2099 [==============================] - 1s 352us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9933 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00692: val_loss did not improve from 0.00912\n",
            "Epoch 693/700\n",
            "2099/2099 [==============================] - 1s 325us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9938 - val_loss: 0.0137 - val_mean_absolute_error: 0.0137 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00693: val_loss did not improve from 0.00912\n",
            "Epoch 694/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9948 - val_loss: 0.0134 - val_mean_absolute_error: 0.0134 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00694: val_loss did not improve from 0.00912\n",
            "Epoch 695/700\n",
            "2099/2099 [==============================] - 1s 343us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00695: val_loss did not improve from 0.00912\n",
            "Epoch 696/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9948 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00696: val_loss did not improve from 0.00912\n",
            "Epoch 697/700\n",
            "2099/2099 [==============================] - 1s 329us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9929 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00697: val_loss did not improve from 0.00912\n",
            "Epoch 698/700\n",
            "2099/2099 [==============================] - 1s 339us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9924 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00698: val_loss did not improve from 0.00912\n",
            "Epoch 699/700\n",
            "2099/2099 [==============================] - 1s 338us/step - loss: 0.0023 - mean_absolute_error: 0.0023 - acc: 0.9952 - val_loss: 0.0140 - val_mean_absolute_error: 0.0140 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00699: val_loss did not improve from 0.00912\n",
            "Epoch 700/700\n",
            "2099/2099 [==============================] - 1s 332us/step - loss: 0.0022 - mean_absolute_error: 0.0022 - acc: 0.9938 - val_loss: 0.0141 - val_mean_absolute_error: 0.0141 - val_acc: 1.0000\n",
            "\n",
            "Epoch 00700: val_loss did not improve from 0.00912\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0ec3d83160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQAuXm-pBNQq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnIyW1Ch1fNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load wights file of the best model :\n",
        "wights_file = 'Checkpoints/Weights-187--0.00486.hdf5' # choose the best checkpoint \n",
        "NN_model.load_weights(wights_file) # load it\n",
        "NN_model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['mean_absolute_error'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhYtZ3jY1uV5",
        "colab_type": "code",
        "outputId": "c136ef5f-0b5e-4d47-9b29-e873f6b2dfc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "test = df2[200:250]\n",
        "predictions = NN_model.predict(test)\n",
        "y_pred = abs(pd.DataFrame(predictions))\n",
        "y_pred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>1311</th>\n",
              "      <th>1312</th>\n",
              "      <th>1313</th>\n",
              "      <th>1314</th>\n",
              "      <th>1315</th>\n",
              "      <th>1316</th>\n",
              "      <th>1317</th>\n",
              "      <th>1318</th>\n",
              "      <th>1319</th>\n",
              "      <th>1320</th>\n",
              "      <th>1321</th>\n",
              "      <th>1322</th>\n",
              "      <th>1323</th>\n",
              "      <th>1324</th>\n",
              "      <th>1325</th>\n",
              "      <th>1326</th>\n",
              "      <th>1327</th>\n",
              "      <th>1328</th>\n",
              "      <th>1329</th>\n",
              "      <th>1330</th>\n",
              "      <th>1331</th>\n",
              "      <th>1332</th>\n",
              "      <th>1333</th>\n",
              "      <th>1334</th>\n",
              "      <th>1335</th>\n",
              "      <th>1336</th>\n",
              "      <th>1337</th>\n",
              "      <th>1338</th>\n",
              "      <th>1339</th>\n",
              "      <th>1340</th>\n",
              "      <th>1341</th>\n",
              "      <th>1342</th>\n",
              "      <th>1343</th>\n",
              "      <th>1344</th>\n",
              "      <th>1345</th>\n",
              "      <th>1346</th>\n",
              "      <th>1347</th>\n",
              "      <th>1348</th>\n",
              "      <th>1349</th>\n",
              "      <th>1350</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.136901</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000323</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.000502</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000206</td>\n",
              "      <td>0.000912</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>1.692679e-07</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.000409</td>\n",
              "      <td>0.000291</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000320</td>\n",
              "      <td>0.000646</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.000616</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>4.287576e-07</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>0.000235</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.000118</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000189</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000557</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.032095</td>\n",
              "      <td>0.013388</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000139</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.000018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.161448</td>\n",
              "      <td>0.007069</td>\n",
              "      <td>0.002558</td>\n",
              "      <td>0.006324</td>\n",
              "      <td>0.001617</td>\n",
              "      <td>0.004314</td>\n",
              "      <td>0.004521</td>\n",
              "      <td>0.005379</td>\n",
              "      <td>0.011237</td>\n",
              "      <td>0.019477</td>\n",
              "      <td>0.013909</td>\n",
              "      <td>0.002257</td>\n",
              "      <td>0.020698</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.002802</td>\n",
              "      <td>0.010736</td>\n",
              "      <td>8.162922e-03</td>\n",
              "      <td>0.019589</td>\n",
              "      <td>0.004492</td>\n",
              "      <td>0.006971</td>\n",
              "      <td>0.012153</td>\n",
              "      <td>0.006043</td>\n",
              "      <td>0.015023</td>\n",
              "      <td>0.011978</td>\n",
              "      <td>0.008130</td>\n",
              "      <td>0.011019</td>\n",
              "      <td>0.009769</td>\n",
              "      <td>0.017688</td>\n",
              "      <td>0.003487</td>\n",
              "      <td>0.024998</td>\n",
              "      <td>0.005458</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.011760</td>\n",
              "      <td>0.002283</td>\n",
              "      <td>0.025820</td>\n",
              "      <td>0.006615</td>\n",
              "      <td>0.012621</td>\n",
              "      <td>0.014617</td>\n",
              "      <td>0.022466</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010148</td>\n",
              "      <td>0.006249</td>\n",
              "      <td>0.026467</td>\n",
              "      <td>5.255426e-03</td>\n",
              "      <td>0.009161</td>\n",
              "      <td>0.001575</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.004432</td>\n",
              "      <td>0.004723</td>\n",
              "      <td>0.004741</td>\n",
              "      <td>0.023998</td>\n",
              "      <td>0.001814</td>\n",
              "      <td>0.005183</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.009584</td>\n",
              "      <td>0.014632</td>\n",
              "      <td>0.008198</td>\n",
              "      <td>0.003120</td>\n",
              "      <td>0.019332</td>\n",
              "      <td>0.031759</td>\n",
              "      <td>0.001645</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.021829</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>0.011359</td>\n",
              "      <td>0.021788</td>\n",
              "      <td>0.025485</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.002868</td>\n",
              "      <td>0.013773</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.004933</td>\n",
              "      <td>0.012505</td>\n",
              "      <td>1.033887</td>\n",
              "      <td>0.912151</td>\n",
              "      <td>0.020132</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.008188</td>\n",
              "      <td>0.006677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.344760</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.000669</td>\n",
              "      <td>0.000817</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.000932</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>6.412284e-05</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000532</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000052</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>2.508333e-04</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000675</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000927</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000073</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000342</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000275</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.000660</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000272</td>\n",
              "      <td>0.000397</td>\n",
              "      <td>0.022151</td>\n",
              "      <td>0.010022</td>\n",
              "      <td>0.000499</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000485</td>\n",
              "      <td>0.000411</td>\n",
              "      <td>0.000109</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.533932</td>\n",
              "      <td>0.004435</td>\n",
              "      <td>0.002577</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.002565</td>\n",
              "      <td>0.003160</td>\n",
              "      <td>0.001131</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.002221</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.004392</td>\n",
              "      <td>0.000490</td>\n",
              "      <td>0.003529</td>\n",
              "      <td>0.002211</td>\n",
              "      <td>0.001552</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>7.342606e-03</td>\n",
              "      <td>0.002067</td>\n",
              "      <td>0.002348</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.001761</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.006669</td>\n",
              "      <td>0.000657</td>\n",
              "      <td>0.001598</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.004181</td>\n",
              "      <td>0.005214</td>\n",
              "      <td>0.004197</td>\n",
              "      <td>0.002490</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001490</td>\n",
              "      <td>0.002612</td>\n",
              "      <td>0.001503</td>\n",
              "      <td>0.004329</td>\n",
              "      <td>0.002936</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000763</td>\n",
              "      <td>0.005172</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002180</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.006675</td>\n",
              "      <td>2.062787e-03</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.002646</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.001698</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.005323</td>\n",
              "      <td>0.000840</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001990</td>\n",
              "      <td>0.002564</td>\n",
              "      <td>0.003082</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>0.000556</td>\n",
              "      <td>0.004847</td>\n",
              "      <td>0.007088</td>\n",
              "      <td>0.001297</td>\n",
              "      <td>0.003189</td>\n",
              "      <td>0.001851</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.002062</td>\n",
              "      <td>0.005611</td>\n",
              "      <td>0.003364</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>0.002005</td>\n",
              "      <td>0.003177</td>\n",
              "      <td>0.003458</td>\n",
              "      <td>0.004998</td>\n",
              "      <td>0.186874</td>\n",
              "      <td>0.148312</td>\n",
              "      <td>0.002937</td>\n",
              "      <td>0.003188</td>\n",
              "      <td>0.001985</td>\n",
              "      <td>0.001812</td>\n",
              "      <td>0.001010</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.842596</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000544</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000111</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.001462</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>1.987524e-03</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.002087</td>\n",
              "      <td>0.000609</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.000082</td>\n",
              "      <td>0.001210</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.001263</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.001767</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.001394</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>8.713651e-04</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.000340</td>\n",
              "      <td>0.000274</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000816</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.000607</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000971</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.001522</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.002303</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>0.002921</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000658</td>\n",
              "      <td>0.001477</td>\n",
              "      <td>0.000338</td>\n",
              "      <td>0.000579</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.740930</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.001739</td>\n",
              "      <td>0.000820</td>\n",
              "      <td>0.000989</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000862</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.000212</td>\n",
              "      <td>3.225983e-04</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.001644</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.001066</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000815</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>8.696722e-04</td>\n",
              "      <td>0.001754</td>\n",
              "      <td>0.000535</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000446</td>\n",
              "      <td>0.000489</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000787</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000928</td>\n",
              "      <td>0.001078</td>\n",
              "      <td>0.001271</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.001015</td>\n",
              "      <td>0.000805</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000613</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000789</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000839</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.002602</td>\n",
              "      <td>0.003677</td>\n",
              "      <td>0.000734</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000795</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.000549</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1.810221</td>\n",
              "      <td>0.003367</td>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.005259</td>\n",
              "      <td>0.003473</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.001738</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.006659</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.002953</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.001566</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>1.062997e-02</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>0.002465</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.003771</td>\n",
              "      <td>0.000551</td>\n",
              "      <td>0.002851</td>\n",
              "      <td>0.002153</td>\n",
              "      <td>0.004493</td>\n",
              "      <td>0.001479</td>\n",
              "      <td>0.001188</td>\n",
              "      <td>0.003553</td>\n",
              "      <td>0.004028</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.001713</td>\n",
              "      <td>0.000860</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>0.004334</td>\n",
              "      <td>0.001899</td>\n",
              "      <td>0.007705</td>\n",
              "      <td>0.001385</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.001847</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.006804</td>\n",
              "      <td>2.047826e-04</td>\n",
              "      <td>0.005098</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.001165</td>\n",
              "      <td>0.001040</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>0.001766</td>\n",
              "      <td>0.004895</td>\n",
              "      <td>0.002701</td>\n",
              "      <td>0.002610</td>\n",
              "      <td>0.001653</td>\n",
              "      <td>0.002229</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.002070</td>\n",
              "      <td>0.001923</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>0.007171</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>0.004617</td>\n",
              "      <td>0.003653</td>\n",
              "      <td>0.002937</td>\n",
              "      <td>0.001691</td>\n",
              "      <td>0.005374</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>0.001483</td>\n",
              "      <td>0.004512</td>\n",
              "      <td>0.001783</td>\n",
              "      <td>0.004028</td>\n",
              "      <td>0.002969</td>\n",
              "      <td>0.004765</td>\n",
              "      <td>0.278145</td>\n",
              "      <td>0.237842</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.004969</td>\n",
              "      <td>0.000728</td>\n",
              "      <td>0.000953</td>\n",
              "      <td>0.001020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>3.632051</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>0.005672</td>\n",
              "      <td>0.012061</td>\n",
              "      <td>0.012702</td>\n",
              "      <td>0.010364</td>\n",
              "      <td>0.002234</td>\n",
              "      <td>0.004857</td>\n",
              "      <td>0.006147</td>\n",
              "      <td>0.010590</td>\n",
              "      <td>0.004962</td>\n",
              "      <td>0.002860</td>\n",
              "      <td>0.016408</td>\n",
              "      <td>0.011866</td>\n",
              "      <td>0.006283</td>\n",
              "      <td>0.004883</td>\n",
              "      <td>2.077872e-02</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>0.005393</td>\n",
              "      <td>0.002523</td>\n",
              "      <td>0.009469</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.008789</td>\n",
              "      <td>0.002702</td>\n",
              "      <td>0.006292</td>\n",
              "      <td>0.008771</td>\n",
              "      <td>0.004686</td>\n",
              "      <td>0.015034</td>\n",
              "      <td>0.011900</td>\n",
              "      <td>0.003918</td>\n",
              "      <td>0.001139</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.002213</td>\n",
              "      <td>0.006800</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.015431</td>\n",
              "      <td>0.004555</td>\n",
              "      <td>0.008818</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>0.006667</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003279</td>\n",
              "      <td>0.004940</td>\n",
              "      <td>0.015646</td>\n",
              "      <td>3.252484e-03</td>\n",
              "      <td>0.015178</td>\n",
              "      <td>0.017294</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>0.004364</td>\n",
              "      <td>0.006062</td>\n",
              "      <td>0.001840</td>\n",
              "      <td>0.012596</td>\n",
              "      <td>0.002128</td>\n",
              "      <td>0.001872</td>\n",
              "      <td>0.004758</td>\n",
              "      <td>0.009757</td>\n",
              "      <td>0.009209</td>\n",
              "      <td>0.005601</td>\n",
              "      <td>0.003397</td>\n",
              "      <td>0.014352</td>\n",
              "      <td>0.020306</td>\n",
              "      <td>0.008854</td>\n",
              "      <td>0.012085</td>\n",
              "      <td>0.004313</td>\n",
              "      <td>0.012714</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.007223</td>\n",
              "      <td>0.003731</td>\n",
              "      <td>0.011433</td>\n",
              "      <td>0.020743</td>\n",
              "      <td>0.004911</td>\n",
              "      <td>0.007722</td>\n",
              "      <td>0.007457</td>\n",
              "      <td>0.018293</td>\n",
              "      <td>0.902186</td>\n",
              "      <td>0.817629</td>\n",
              "      <td>0.006373</td>\n",
              "      <td>0.016532</td>\n",
              "      <td>0.001233</td>\n",
              "      <td>0.001409</td>\n",
              "      <td>0.003147</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>4.176743</td>\n",
              "      <td>0.003363</td>\n",
              "      <td>0.003430</td>\n",
              "      <td>0.005791</td>\n",
              "      <td>0.010341</td>\n",
              "      <td>0.005838</td>\n",
              "      <td>0.011312</td>\n",
              "      <td>0.004964</td>\n",
              "      <td>0.017981</td>\n",
              "      <td>0.007928</td>\n",
              "      <td>0.006623</td>\n",
              "      <td>0.002662</td>\n",
              "      <td>0.024015</td>\n",
              "      <td>0.010883</td>\n",
              "      <td>0.010173</td>\n",
              "      <td>0.008458</td>\n",
              "      <td>1.973462e-02</td>\n",
              "      <td>0.010287</td>\n",
              "      <td>0.004226</td>\n",
              "      <td>0.015250</td>\n",
              "      <td>0.001701</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.016289</td>\n",
              "      <td>0.001559</td>\n",
              "      <td>0.007666</td>\n",
              "      <td>0.016739</td>\n",
              "      <td>0.006227</td>\n",
              "      <td>0.013975</td>\n",
              "      <td>0.012915</td>\n",
              "      <td>0.014803</td>\n",
              "      <td>0.003637</td>\n",
              "      <td>0.002459</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>0.004200</td>\n",
              "      <td>0.001999</td>\n",
              "      <td>0.012173</td>\n",
              "      <td>0.013635</td>\n",
              "      <td>0.014509</td>\n",
              "      <td>0.006562</td>\n",
              "      <td>0.020072</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.004690</td>\n",
              "      <td>0.013403</td>\n",
              "      <td>6.314454e-03</td>\n",
              "      <td>0.003095</td>\n",
              "      <td>0.006562</td>\n",
              "      <td>0.004805</td>\n",
              "      <td>0.006021</td>\n",
              "      <td>0.003734</td>\n",
              "      <td>0.004912</td>\n",
              "      <td>0.024297</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.006640</td>\n",
              "      <td>0.006372</td>\n",
              "      <td>0.013682</td>\n",
              "      <td>0.009685</td>\n",
              "      <td>0.004038</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>0.020292</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.012619</td>\n",
              "      <td>0.002481</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.020245</td>\n",
              "      <td>0.006215</td>\n",
              "      <td>0.011300</td>\n",
              "      <td>0.004554</td>\n",
              "      <td>0.020091</td>\n",
              "      <td>0.001368</td>\n",
              "      <td>0.008048</td>\n",
              "      <td>0.016831</td>\n",
              "      <td>0.020413</td>\n",
              "      <td>1.059475</td>\n",
              "      <td>0.947925</td>\n",
              "      <td>0.004480</td>\n",
              "      <td>0.006346</td>\n",
              "      <td>0.003083</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.000160</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2.054388</td>\n",
              "      <td>0.003817</td>\n",
              "      <td>0.002567</td>\n",
              "      <td>0.001435</td>\n",
              "      <td>0.001412</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.005699</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.008705</td>\n",
              "      <td>0.001946</td>\n",
              "      <td>0.006051</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.006179</td>\n",
              "      <td>0.002155</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>1.077666e-02</td>\n",
              "      <td>0.001357</td>\n",
              "      <td>0.001736</td>\n",
              "      <td>0.003572</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>0.003179</td>\n",
              "      <td>0.001590</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.005350</td>\n",
              "      <td>0.003230</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>0.005899</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.002870</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.002036</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>0.004887</td>\n",
              "      <td>0.002726</td>\n",
              "      <td>0.002997</td>\n",
              "      <td>0.001731</td>\n",
              "      <td>0.005255</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001379</td>\n",
              "      <td>0.002866</td>\n",
              "      <td>0.005198</td>\n",
              "      <td>7.699116e-05</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.006446</td>\n",
              "      <td>0.000782</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.000621</td>\n",
              "      <td>0.009627</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.006036</td>\n",
              "      <td>0.003864</td>\n",
              "      <td>0.000629</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.003283</td>\n",
              "      <td>0.003695</td>\n",
              "      <td>0.007537</td>\n",
              "      <td>0.006031</td>\n",
              "      <td>0.005936</td>\n",
              "      <td>0.003334</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.007286</td>\n",
              "      <td>0.002725</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.008023</td>\n",
              "      <td>0.003456</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.007772</td>\n",
              "      <td>0.008029</td>\n",
              "      <td>0.355529</td>\n",
              "      <td>0.297754</td>\n",
              "      <td>0.001638</td>\n",
              "      <td>0.005560</td>\n",
              "      <td>0.001043</td>\n",
              "      <td>0.001486</td>\n",
              "      <td>0.000283</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>1.314669</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>0.002325</td>\n",
              "      <td>0.000769</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.002357</td>\n",
              "      <td>0.001261</td>\n",
              "      <td>0.000263</td>\n",
              "      <td>0.002028</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>0.001389</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.001904</td>\n",
              "      <td>5.634453e-03</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.001449</td>\n",
              "      <td>0.002298</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.001599</td>\n",
              "      <td>0.004411</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.002327</td>\n",
              "      <td>0.003383</td>\n",
              "      <td>0.003180</td>\n",
              "      <td>0.002086</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.001458</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.002587</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.000360</td>\n",
              "      <td>0.000775</td>\n",
              "      <td>0.003314</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000844</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.004297</td>\n",
              "      <td>2.151943e-03</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>0.003297</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>0.001041</td>\n",
              "      <td>0.003803</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000843</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.001096</td>\n",
              "      <td>0.001844</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.003584</td>\n",
              "      <td>0.005645</td>\n",
              "      <td>0.002486</td>\n",
              "      <td>0.002304</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.000995</td>\n",
              "      <td>0.001499</td>\n",
              "      <td>0.005566</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>0.002143</td>\n",
              "      <td>0.002287</td>\n",
              "      <td>0.001087</td>\n",
              "      <td>0.001542</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>0.112237</td>\n",
              "      <td>0.079646</td>\n",
              "      <td>0.001889</td>\n",
              "      <td>0.001775</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.001390</td>\n",
              "      <td>0.000385</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>3.961283</td>\n",
              "      <td>0.004083</td>\n",
              "      <td>0.008334</td>\n",
              "      <td>0.002457</td>\n",
              "      <td>0.002030</td>\n",
              "      <td>0.001067</td>\n",
              "      <td>0.002592</td>\n",
              "      <td>0.015264</td>\n",
              "      <td>0.003229</td>\n",
              "      <td>0.011382</td>\n",
              "      <td>0.003966</td>\n",
              "      <td>0.003318</td>\n",
              "      <td>0.020050</td>\n",
              "      <td>0.005068</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>4.597892e-03</td>\n",
              "      <td>0.015787</td>\n",
              "      <td>0.003141</td>\n",
              "      <td>0.018666</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.014334</td>\n",
              "      <td>0.009990</td>\n",
              "      <td>0.002281</td>\n",
              "      <td>0.003579</td>\n",
              "      <td>0.013076</td>\n",
              "      <td>0.019624</td>\n",
              "      <td>0.028234</td>\n",
              "      <td>0.009002</td>\n",
              "      <td>0.028316</td>\n",
              "      <td>0.001441</td>\n",
              "      <td>0.007555</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.015971</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.022507</td>\n",
              "      <td>0.005824</td>\n",
              "      <td>0.007130</td>\n",
              "      <td>0.003647</td>\n",
              "      <td>0.016960</td>\n",
              "      <td>...</td>\n",
              "      <td>0.011609</td>\n",
              "      <td>0.007726</td>\n",
              "      <td>0.016254</td>\n",
              "      <td>2.497609e-02</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.012774</td>\n",
              "      <td>0.008482</td>\n",
              "      <td>0.006054</td>\n",
              "      <td>0.001835</td>\n",
              "      <td>0.007708</td>\n",
              "      <td>0.018248</td>\n",
              "      <td>0.002588</td>\n",
              "      <td>0.000941</td>\n",
              "      <td>0.007527</td>\n",
              "      <td>0.006787</td>\n",
              "      <td>0.015628</td>\n",
              "      <td>0.007114</td>\n",
              "      <td>0.017232</td>\n",
              "      <td>0.025197</td>\n",
              "      <td>0.030387</td>\n",
              "      <td>0.007479</td>\n",
              "      <td>0.005177</td>\n",
              "      <td>0.012829</td>\n",
              "      <td>0.003489</td>\n",
              "      <td>0.003702</td>\n",
              "      <td>0.026184</td>\n",
              "      <td>0.016968</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.004767</td>\n",
              "      <td>0.013357</td>\n",
              "      <td>0.014854</td>\n",
              "      <td>0.005316</td>\n",
              "      <td>0.014464</td>\n",
              "      <td>0.982317</td>\n",
              "      <td>0.863534</td>\n",
              "      <td>0.005231</td>\n",
              "      <td>0.006484</td>\n",
              "      <td>0.001612</td>\n",
              "      <td>0.005387</td>\n",
              "      <td>0.000059</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>3.180053</td>\n",
              "      <td>0.009991</td>\n",
              "      <td>0.002671</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.004278</td>\n",
              "      <td>0.008874</td>\n",
              "      <td>0.010721</td>\n",
              "      <td>0.003037</td>\n",
              "      <td>0.014305</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.014311</td>\n",
              "      <td>0.004186</td>\n",
              "      <td>0.004010</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>1.415883e-02</td>\n",
              "      <td>0.007447</td>\n",
              "      <td>0.002862</td>\n",
              "      <td>0.004340</td>\n",
              "      <td>0.007661</td>\n",
              "      <td>0.002173</td>\n",
              "      <td>0.009392</td>\n",
              "      <td>0.001735</td>\n",
              "      <td>0.002641</td>\n",
              "      <td>0.001626</td>\n",
              "      <td>0.006017</td>\n",
              "      <td>0.015294</td>\n",
              "      <td>0.007092</td>\n",
              "      <td>0.010044</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>0.004894</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.011069</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.016934</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>0.001350</td>\n",
              "      <td>0.007864</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.003991</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.017095</td>\n",
              "      <td>4.827654e-03</td>\n",
              "      <td>0.002657</td>\n",
              "      <td>0.011986</td>\n",
              "      <td>0.001612</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.002562</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.013337</td>\n",
              "      <td>0.000778</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.009778</td>\n",
              "      <td>0.007184</td>\n",
              "      <td>0.009640</td>\n",
              "      <td>0.003058</td>\n",
              "      <td>0.003154</td>\n",
              "      <td>0.019646</td>\n",
              "      <td>0.016372</td>\n",
              "      <td>0.005791</td>\n",
              "      <td>0.006664</td>\n",
              "      <td>0.009185</td>\n",
              "      <td>0.006864</td>\n",
              "      <td>0.003278</td>\n",
              "      <td>0.016909</td>\n",
              "      <td>0.011499</td>\n",
              "      <td>0.007486</td>\n",
              "      <td>0.007769</td>\n",
              "      <td>0.003077</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>0.011867</td>\n",
              "      <td>0.009714</td>\n",
              "      <td>0.722267</td>\n",
              "      <td>0.633147</td>\n",
              "      <td>0.009823</td>\n",
              "      <td>0.006826</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.003635</td>\n",
              "      <td>0.000503</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0.285817</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.000375</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000595</td>\n",
              "      <td>0.000505</td>\n",
              "      <td>0.000797</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.001010</td>\n",
              "      <td>0.000335</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.000231</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>1.102011e-05</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000588</td>\n",
              "      <td>0.000456</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000097</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000593</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.000469</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000187</td>\n",
              "      <td>1.473007e-04</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.000722</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000570</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000120</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000441</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.000667</td>\n",
              "      <td>0.000258</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.026401</td>\n",
              "      <td>0.012446</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000437</td>\n",
              "      <td>0.000097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0.865202</td>\n",
              "      <td>0.000703</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.001192</td>\n",
              "      <td>0.001444</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.001055</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>0.001092</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000677</td>\n",
              "      <td>2.096052e-03</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.001119</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.000674</td>\n",
              "      <td>0.001744</td>\n",
              "      <td>0.000165</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000721</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.001608</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.001480</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000305</td>\n",
              "      <td>0.000754</td>\n",
              "      <td>0.001696</td>\n",
              "      <td>6.895313e-04</td>\n",
              "      <td>0.000217</td>\n",
              "      <td>0.001031</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.000994</td>\n",
              "      <td>0.001134</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000496</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000824</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.001405</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000490</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.001430</td>\n",
              "      <td>0.000385</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000878</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000602</td>\n",
              "      <td>0.002299</td>\n",
              "      <td>0.010019</td>\n",
              "      <td>0.004214</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.001442</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>0.000443</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>1.199399</td>\n",
              "      <td>0.003219</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>0.000027</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.001540</td>\n",
              "      <td>0.000978</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.000994</td>\n",
              "      <td>0.000440</td>\n",
              "      <td>0.003007</td>\n",
              "      <td>0.000858</td>\n",
              "      <td>0.001785</td>\n",
              "      <td>0.001183</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000569</td>\n",
              "      <td>5.217426e-03</td>\n",
              "      <td>0.000937</td>\n",
              "      <td>0.001821</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.001595</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.003643</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.002263</td>\n",
              "      <td>0.003129</td>\n",
              "      <td>0.002166</td>\n",
              "      <td>0.002162</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>0.002194</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.002416</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.000366</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.002365</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.000591</td>\n",
              "      <td>0.004269</td>\n",
              "      <td>1.398116e-03</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.002478</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000281</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.003500</td>\n",
              "      <td>0.000406</td>\n",
              "      <td>0.000709</td>\n",
              "      <td>0.001280</td>\n",
              "      <td>0.000975</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.002319</td>\n",
              "      <td>0.004804</td>\n",
              "      <td>0.002171</td>\n",
              "      <td>0.002056</td>\n",
              "      <td>0.001348</td>\n",
              "      <td>0.000572</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.005351</td>\n",
              "      <td>0.001595</td>\n",
              "      <td>0.000472</td>\n",
              "      <td>0.001667</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.001017</td>\n",
              "      <td>0.003382</td>\n",
              "      <td>0.003414</td>\n",
              "      <td>0.074203</td>\n",
              "      <td>0.046368</td>\n",
              "      <td>0.001805</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.000371</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>2.126948</td>\n",
              "      <td>0.005536</td>\n",
              "      <td>0.006040</td>\n",
              "      <td>0.004831</td>\n",
              "      <td>0.005534</td>\n",
              "      <td>0.004065</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.005174</td>\n",
              "      <td>0.006049</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.004799</td>\n",
              "      <td>0.004943</td>\n",
              "      <td>0.000974</td>\n",
              "      <td>0.002819</td>\n",
              "      <td>1.474420e-02</td>\n",
              "      <td>0.002069</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.001826</td>\n",
              "      <td>0.004231</td>\n",
              "      <td>0.002390</td>\n",
              "      <td>0.004468</td>\n",
              "      <td>0.001306</td>\n",
              "      <td>0.005414</td>\n",
              "      <td>0.005074</td>\n",
              "      <td>0.002249</td>\n",
              "      <td>0.004560</td>\n",
              "      <td>0.005720</td>\n",
              "      <td>0.001445</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.000018</td>\n",
              "      <td>0.002533</td>\n",
              "      <td>0.001091</td>\n",
              "      <td>0.007764</td>\n",
              "      <td>0.002272</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.004740</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.008458</td>\n",
              "      <td>3.053203e-03</td>\n",
              "      <td>0.008471</td>\n",
              "      <td>0.008922</td>\n",
              "      <td>0.001708</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.005058</td>\n",
              "      <td>0.002553</td>\n",
              "      <td>0.006450</td>\n",
              "      <td>0.003670</td>\n",
              "      <td>0.001741</td>\n",
              "      <td>0.003838</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.002027</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.002242</td>\n",
              "      <td>0.007965</td>\n",
              "      <td>0.009255</td>\n",
              "      <td>0.005982</td>\n",
              "      <td>0.006082</td>\n",
              "      <td>0.001932</td>\n",
              "      <td>0.004168</td>\n",
              "      <td>0.001923</td>\n",
              "      <td>0.006136</td>\n",
              "      <td>0.003060</td>\n",
              "      <td>0.006432</td>\n",
              "      <td>0.007345</td>\n",
              "      <td>0.003754</td>\n",
              "      <td>0.004289</td>\n",
              "      <td>0.004074</td>\n",
              "      <td>0.008180</td>\n",
              "      <td>0.386920</td>\n",
              "      <td>0.338755</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.009257</td>\n",
              "      <td>0.003068</td>\n",
              "      <td>0.001518</td>\n",
              "      <td>0.000114</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>1.465655</td>\n",
              "      <td>0.002938</td>\n",
              "      <td>0.002274</td>\n",
              "      <td>0.002401</td>\n",
              "      <td>0.001431</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.001681</td>\n",
              "      <td>0.001830</td>\n",
              "      <td>0.003246</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>0.001365</td>\n",
              "      <td>0.002505</td>\n",
              "      <td>0.002748</td>\n",
              "      <td>0.000563</td>\n",
              "      <td>0.000154</td>\n",
              "      <td>7.341118e-03</td>\n",
              "      <td>0.000807</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001370</td>\n",
              "      <td>0.001573</td>\n",
              "      <td>0.004251</td>\n",
              "      <td>0.001360</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.002503</td>\n",
              "      <td>0.003144</td>\n",
              "      <td>0.002793</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>0.001352</td>\n",
              "      <td>0.000427</td>\n",
              "      <td>0.000220</td>\n",
              "      <td>0.001589</td>\n",
              "      <td>0.001704</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.001620</td>\n",
              "      <td>0.000838</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000227</td>\n",
              "      <td>0.003802</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001404</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>2.143966e-03</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>0.003165</td>\n",
              "      <td>0.000308</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.002208</td>\n",
              "      <td>0.000853</td>\n",
              "      <td>0.006508</td>\n",
              "      <td>0.000526</td>\n",
              "      <td>0.000936</td>\n",
              "      <td>0.002684</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001050</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.002693</td>\n",
              "      <td>0.006063</td>\n",
              "      <td>0.003019</td>\n",
              "      <td>0.003687</td>\n",
              "      <td>0.000351</td>\n",
              "      <td>0.000965</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.003429</td>\n",
              "      <td>0.001661</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.004019</td>\n",
              "      <td>0.002776</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.005321</td>\n",
              "      <td>0.004974</td>\n",
              "      <td>0.162410</td>\n",
              "      <td>0.127837</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.003163</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.002188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>2.306023</td>\n",
              "      <td>0.006238</td>\n",
              "      <td>0.006765</td>\n",
              "      <td>0.005412</td>\n",
              "      <td>0.005981</td>\n",
              "      <td>0.004486</td>\n",
              "      <td>0.000417</td>\n",
              "      <td>0.001758</td>\n",
              "      <td>0.004599</td>\n",
              "      <td>0.005830</td>\n",
              "      <td>0.006630</td>\n",
              "      <td>0.001249</td>\n",
              "      <td>0.005814</td>\n",
              "      <td>0.005350</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.003036</td>\n",
              "      <td>1.642621e-02</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.002420</td>\n",
              "      <td>0.001966</td>\n",
              "      <td>0.004783</td>\n",
              "      <td>0.002437</td>\n",
              "      <td>0.005290</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.006208</td>\n",
              "      <td>0.005575</td>\n",
              "      <td>0.002418</td>\n",
              "      <td>0.005095</td>\n",
              "      <td>0.006297</td>\n",
              "      <td>0.001547</td>\n",
              "      <td>0.001510</td>\n",
              "      <td>0.002154</td>\n",
              "      <td>0.000047</td>\n",
              "      <td>0.003025</td>\n",
              "      <td>0.001118</td>\n",
              "      <td>0.008635</td>\n",
              "      <td>0.002347</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.000332</td>\n",
              "      <td>0.005134</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>0.001749</td>\n",
              "      <td>0.009285</td>\n",
              "      <td>3.142742e-03</td>\n",
              "      <td>0.009519</td>\n",
              "      <td>0.009764</td>\n",
              "      <td>0.001793</td>\n",
              "      <td>0.001726</td>\n",
              "      <td>0.005817</td>\n",
              "      <td>0.002589</td>\n",
              "      <td>0.007221</td>\n",
              "      <td>0.004100</td>\n",
              "      <td>0.001778</td>\n",
              "      <td>0.004587</td>\n",
              "      <td>0.005383</td>\n",
              "      <td>0.002291</td>\n",
              "      <td>0.000571</td>\n",
              "      <td>0.002337</td>\n",
              "      <td>0.008923</td>\n",
              "      <td>0.010178</td>\n",
              "      <td>0.006421</td>\n",
              "      <td>0.007032</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.004551</td>\n",
              "      <td>0.001537</td>\n",
              "      <td>0.006792</td>\n",
              "      <td>0.003576</td>\n",
              "      <td>0.007246</td>\n",
              "      <td>0.008380</td>\n",
              "      <td>0.004185</td>\n",
              "      <td>0.004579</td>\n",
              "      <td>0.004196</td>\n",
              "      <td>0.009047</td>\n",
              "      <td>0.447416</td>\n",
              "      <td>0.395061</td>\n",
              "      <td>0.001542</td>\n",
              "      <td>0.010630</td>\n",
              "      <td>0.003352</td>\n",
              "      <td>0.001616</td>\n",
              "      <td>0.000265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>3.683190</td>\n",
              "      <td>0.009257</td>\n",
              "      <td>0.002557</td>\n",
              "      <td>0.006699</td>\n",
              "      <td>0.000498</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>0.009111</td>\n",
              "      <td>0.001093</td>\n",
              "      <td>0.016090</td>\n",
              "      <td>0.002095</td>\n",
              "      <td>0.007808</td>\n",
              "      <td>0.003518</td>\n",
              "      <td>0.019399</td>\n",
              "      <td>0.007984</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.003554</td>\n",
              "      <td>1.689586e-02</td>\n",
              "      <td>0.005073</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.002079</td>\n",
              "      <td>0.007112</td>\n",
              "      <td>0.002069</td>\n",
              "      <td>0.016050</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>0.002504</td>\n",
              "      <td>0.006838</td>\n",
              "      <td>0.008869</td>\n",
              "      <td>0.018145</td>\n",
              "      <td>0.006581</td>\n",
              "      <td>0.011071</td>\n",
              "      <td>0.002409</td>\n",
              "      <td>0.000004</td>\n",
              "      <td>0.000359</td>\n",
              "      <td>0.010002</td>\n",
              "      <td>0.003087</td>\n",
              "      <td>0.013058</td>\n",
              "      <td>0.010644</td>\n",
              "      <td>0.002882</td>\n",
              "      <td>0.004326</td>\n",
              "      <td>0.010779</td>\n",
              "      <td>...</td>\n",
              "      <td>0.008157</td>\n",
              "      <td>0.005675</td>\n",
              "      <td>0.013510</td>\n",
              "      <td>4.426957e-04</td>\n",
              "      <td>0.001427</td>\n",
              "      <td>0.010726</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.002291</td>\n",
              "      <td>0.002805</td>\n",
              "      <td>0.004381</td>\n",
              "      <td>0.019151</td>\n",
              "      <td>0.002730</td>\n",
              "      <td>0.000630</td>\n",
              "      <td>0.013809</td>\n",
              "      <td>0.010152</td>\n",
              "      <td>0.007824</td>\n",
              "      <td>0.003388</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.014864</td>\n",
              "      <td>0.017405</td>\n",
              "      <td>0.007821</td>\n",
              "      <td>0.014009</td>\n",
              "      <td>0.011022</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.006096</td>\n",
              "      <td>0.014245</td>\n",
              "      <td>0.010411</td>\n",
              "      <td>0.006722</td>\n",
              "      <td>0.014481</td>\n",
              "      <td>0.002362</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.007766</td>\n",
              "      <td>0.013553</td>\n",
              "      <td>0.894811</td>\n",
              "      <td>0.785059</td>\n",
              "      <td>0.005790</td>\n",
              "      <td>0.012112</td>\n",
              "      <td>0.004940</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.001807</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0.722515</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000233</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000774</td>\n",
              "      <td>0.001107</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000617</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.001053</td>\n",
              "      <td>0.000807</td>\n",
              "      <td>0.000550</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>0.000077</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.000378</td>\n",
              "      <td>6.158389e-04</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000376</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.000317</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.000574</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000468</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000604</td>\n",
              "      <td>0.000341</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000801</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000626</td>\n",
              "      <td>0.001488</td>\n",
              "      <td>3.285465e-04</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000260</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000908</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000112</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.000701</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000668</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.001056</td>\n",
              "      <td>0.000453</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000673</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.000276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0.165401</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000088</td>\n",
              "      <td>0.000752</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000331</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>6.609503e-05</td>\n",
              "      <td>0.000552</td>\n",
              "      <td>0.000442</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000671</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000178</td>\n",
              "      <td>0.000276</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000131</td>\n",
              "      <td>0.000402</td>\n",
              "      <td>0.000271</td>\n",
              "      <td>0.000314</td>\n",
              "      <td>0.000654</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000216</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>7.211696e-05</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.000457</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000424</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000344</td>\n",
              "      <td>0.000785</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.000211</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000547</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000250</td>\n",
              "      <td>0.000101</td>\n",
              "      <td>0.032513</td>\n",
              "      <td>0.014290</td>\n",
              "      <td>0.000497</td>\n",
              "      <td>0.000208</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>1.928387</td>\n",
              "      <td>0.001715</td>\n",
              "      <td>0.001605</td>\n",
              "      <td>0.003341</td>\n",
              "      <td>0.001784</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>0.004829</td>\n",
              "      <td>0.001678</td>\n",
              "      <td>0.005226</td>\n",
              "      <td>0.002939</td>\n",
              "      <td>0.005706</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.007193</td>\n",
              "      <td>0.000679</td>\n",
              "      <td>0.002921</td>\n",
              "      <td>0.003598</td>\n",
              "      <td>9.539916e-03</td>\n",
              "      <td>0.002901</td>\n",
              "      <td>0.001388</td>\n",
              "      <td>0.007538</td>\n",
              "      <td>0.007929</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>0.001853</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>0.001896</td>\n",
              "      <td>0.005860</td>\n",
              "      <td>0.002540</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.005373</td>\n",
              "      <td>0.001534</td>\n",
              "      <td>0.001098</td>\n",
              "      <td>0.002253</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.005298</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.002188</td>\n",
              "      <td>0.003440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.003804</td>\n",
              "      <td>0.004574</td>\n",
              "      <td>4.266278e-03</td>\n",
              "      <td>0.001336</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>0.001931</td>\n",
              "      <td>0.002202</td>\n",
              "      <td>0.003797</td>\n",
              "      <td>0.009457</td>\n",
              "      <td>0.004646</td>\n",
              "      <td>0.002367</td>\n",
              "      <td>0.003747</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.002838</td>\n",
              "      <td>0.007370</td>\n",
              "      <td>0.000702</td>\n",
              "      <td>0.005408</td>\n",
              "      <td>0.004026</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.004916</td>\n",
              "      <td>0.003220</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001116</td>\n",
              "      <td>0.004538</td>\n",
              "      <td>0.003950</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>0.007281</td>\n",
              "      <td>0.003153</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>0.010870</td>\n",
              "      <td>0.311842</td>\n",
              "      <td>0.264518</td>\n",
              "      <td>0.001321</td>\n",
              "      <td>0.002521</td>\n",
              "      <td>0.002764</td>\n",
              "      <td>0.003772</td>\n",
              "      <td>0.000168</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>2.861668</td>\n",
              "      <td>0.007413</td>\n",
              "      <td>0.003407</td>\n",
              "      <td>0.000350</td>\n",
              "      <td>0.001855</td>\n",
              "      <td>0.005194</td>\n",
              "      <td>0.006707</td>\n",
              "      <td>0.004204</td>\n",
              "      <td>0.007875</td>\n",
              "      <td>0.002333</td>\n",
              "      <td>0.008201</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.013313</td>\n",
              "      <td>0.003485</td>\n",
              "      <td>0.001825</td>\n",
              "      <td>0.010033</td>\n",
              "      <td>6.906429e-03</td>\n",
              "      <td>0.002240</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.001307</td>\n",
              "      <td>0.003712</td>\n",
              "      <td>0.005547</td>\n",
              "      <td>0.000240</td>\n",
              "      <td>0.006403</td>\n",
              "      <td>0.003280</td>\n",
              "      <td>0.002571</td>\n",
              "      <td>0.002672</td>\n",
              "      <td>0.007649</td>\n",
              "      <td>0.001833</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001058</td>\n",
              "      <td>0.004763</td>\n",
              "      <td>0.002291</td>\n",
              "      <td>0.003742</td>\n",
              "      <td>0.009176</td>\n",
              "      <td>0.007130</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.008799</td>\n",
              "      <td>0.008000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000078</td>\n",
              "      <td>0.004586</td>\n",
              "      <td>0.004992</td>\n",
              "      <td>1.315388e-03</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.008904</td>\n",
              "      <td>0.001998</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.004478</td>\n",
              "      <td>0.007958</td>\n",
              "      <td>0.001917</td>\n",
              "      <td>0.002638</td>\n",
              "      <td>0.005378</td>\n",
              "      <td>0.005512</td>\n",
              "      <td>0.001410</td>\n",
              "      <td>0.002436</td>\n",
              "      <td>0.000875</td>\n",
              "      <td>0.005473</td>\n",
              "      <td>0.010790</td>\n",
              "      <td>0.004644</td>\n",
              "      <td>0.006200</td>\n",
              "      <td>0.003819</td>\n",
              "      <td>0.003657</td>\n",
              "      <td>0.003523</td>\n",
              "      <td>0.012919</td>\n",
              "      <td>0.004684</td>\n",
              "      <td>0.001978</td>\n",
              "      <td>0.007699</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.006413</td>\n",
              "      <td>0.014238</td>\n",
              "      <td>0.616780</td>\n",
              "      <td>0.537177</td>\n",
              "      <td>0.002874</td>\n",
              "      <td>0.005542</td>\n",
              "      <td>0.004626</td>\n",
              "      <td>0.001168</td>\n",
              "      <td>0.000523</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0.382772</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000809</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000315</td>\n",
              "      <td>0.000361</td>\n",
              "      <td>0.000236</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000024</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>1.089354e-04</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000207</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.000786</td>\n",
              "      <td>0.000531</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000161</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000076</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000085</td>\n",
              "      <td>0.000307</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000650</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000374</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>3.783022e-04</td>\n",
              "      <td>0.000601</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000741</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000196</td>\n",
              "      <td>0.001028</td>\n",
              "      <td>0.000210</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.000297</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000060</td>\n",
              "      <td>0.000462</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000080</td>\n",
              "      <td>0.000083</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000185</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000764</td>\n",
              "      <td>0.000259</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.016740</td>\n",
              "      <td>0.006599</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.000600</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>1.415485</td>\n",
              "      <td>0.001760</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.001413</td>\n",
              "      <td>0.001409</td>\n",
              "      <td>0.003326</td>\n",
              "      <td>0.002373</td>\n",
              "      <td>0.002569</td>\n",
              "      <td>0.000586</td>\n",
              "      <td>0.002802</td>\n",
              "      <td>0.000911</td>\n",
              "      <td>0.003948</td>\n",
              "      <td>0.002043</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000089</td>\n",
              "      <td>6.790313e-03</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.002490</td>\n",
              "      <td>0.001804</td>\n",
              "      <td>0.002406</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.002582</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.002408</td>\n",
              "      <td>0.002656</td>\n",
              "      <td>0.002365</td>\n",
              "      <td>0.000126</td>\n",
              "      <td>0.005635</td>\n",
              "      <td>0.000821</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.001607</td>\n",
              "      <td>0.000503</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.000174</td>\n",
              "      <td>0.002934</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.001930</td>\n",
              "      <td>0.002439</td>\n",
              "      <td>1.847481e-03</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.003663</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>0.001057</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>0.004087</td>\n",
              "      <td>0.001727</td>\n",
              "      <td>0.001070</td>\n",
              "      <td>0.001415</td>\n",
              "      <td>0.000767</td>\n",
              "      <td>0.000458</td>\n",
              "      <td>0.000222</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.002561</td>\n",
              "      <td>0.003872</td>\n",
              "      <td>0.002334</td>\n",
              "      <td>0.002023</td>\n",
              "      <td>0.001578</td>\n",
              "      <td>0.001988</td>\n",
              "      <td>0.000289</td>\n",
              "      <td>0.003416</td>\n",
              "      <td>0.001629</td>\n",
              "      <td>0.001764</td>\n",
              "      <td>0.006788</td>\n",
              "      <td>0.001447</td>\n",
              "      <td>0.001011</td>\n",
              "      <td>0.004610</td>\n",
              "      <td>0.006153</td>\n",
              "      <td>0.145275</td>\n",
              "      <td>0.111384</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>0.002244</td>\n",
              "      <td>0.000879</td>\n",
              "      <td>0.000152</td>\n",
              "      <td>0.001374</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>1.026511</td>\n",
              "      <td>0.000715</td>\n",
              "      <td>0.000367</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000731</td>\n",
              "      <td>0.002001</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>0.001073</td>\n",
              "      <td>0.001814</td>\n",
              "      <td>0.000641</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.002489</td>\n",
              "      <td>0.000636</td>\n",
              "      <td>0.000521</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>3.908170e-03</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001333</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000747</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.001186</td>\n",
              "      <td>0.000744</td>\n",
              "      <td>0.001045</td>\n",
              "      <td>0.000517</td>\n",
              "      <td>0.001615</td>\n",
              "      <td>0.001032</td>\n",
              "      <td>0.002463</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000765</td>\n",
              "      <td>0.001376</td>\n",
              "      <td>0.000425</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.001820</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.001569</td>\n",
              "      <td>1.057774e-03</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.000287</td>\n",
              "      <td>0.001511</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.000791</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.001103</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000528</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.001231</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.000416</td>\n",
              "      <td>0.000762</td>\n",
              "      <td>0.002785</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>0.000487</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.001622</td>\n",
              "      <td>0.003538</td>\n",
              "      <td>0.037120</td>\n",
              "      <td>0.021788</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.001694</td>\n",
              "      <td>0.000634</td>\n",
              "      <td>0.000316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>1.335107</td>\n",
              "      <td>0.001899</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.001568</td>\n",
              "      <td>0.002685</td>\n",
              "      <td>0.000724</td>\n",
              "      <td>0.003152</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.003055</td>\n",
              "      <td>0.000512</td>\n",
              "      <td>0.002524</td>\n",
              "      <td>0.001755</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.001852</td>\n",
              "      <td>6.908495e-03</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>0.001611</td>\n",
              "      <td>0.002326</td>\n",
              "      <td>0.002294</td>\n",
              "      <td>0.002486</td>\n",
              "      <td>0.001794</td>\n",
              "      <td>0.000758</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>0.002853</td>\n",
              "      <td>0.001907</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.004497</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.001683</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.002051</td>\n",
              "      <td>0.001044</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.001880</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.003615</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.001489</td>\n",
              "      <td>0.002550</td>\n",
              "      <td>1.403316e-04</td>\n",
              "      <td>0.000404</td>\n",
              "      <td>0.003308</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000576</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>0.003926</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.001184</td>\n",
              "      <td>0.001582</td>\n",
              "      <td>0.000053</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.003229</td>\n",
              "      <td>0.004109</td>\n",
              "      <td>0.003166</td>\n",
              "      <td>0.002869</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.001424</td>\n",
              "      <td>0.001061</td>\n",
              "      <td>0.004638</td>\n",
              "      <td>0.001682</td>\n",
              "      <td>0.002049</td>\n",
              "      <td>0.004295</td>\n",
              "      <td>0.002320</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.003597</td>\n",
              "      <td>0.005817</td>\n",
              "      <td>0.120590</td>\n",
              "      <td>0.087346</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.002516</td>\n",
              "      <td>0.000871</td>\n",
              "      <td>0.000696</td>\n",
              "      <td>0.000159</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>1.957236</td>\n",
              "      <td>0.004847</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.002160</td>\n",
              "      <td>0.004223</td>\n",
              "      <td>0.003141</td>\n",
              "      <td>0.002694</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>0.003542</td>\n",
              "      <td>0.002535</td>\n",
              "      <td>0.005001</td>\n",
              "      <td>0.000306</td>\n",
              "      <td>0.002404</td>\n",
              "      <td>0.002277</td>\n",
              "      <td>0.002292</td>\n",
              "      <td>0.003064</td>\n",
              "      <td>1.011448e-02</td>\n",
              "      <td>0.001319</td>\n",
              "      <td>0.002147</td>\n",
              "      <td>0.003755</td>\n",
              "      <td>0.003919</td>\n",
              "      <td>0.001355</td>\n",
              "      <td>0.005237</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.001823</td>\n",
              "      <td>0.006282</td>\n",
              "      <td>0.000966</td>\n",
              "      <td>0.005344</td>\n",
              "      <td>0.005855</td>\n",
              "      <td>0.001771</td>\n",
              "      <td>0.001194</td>\n",
              "      <td>0.000465</td>\n",
              "      <td>0.000514</td>\n",
              "      <td>0.000700</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.006206</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>0.002394</td>\n",
              "      <td>0.000682</td>\n",
              "      <td>0.002114</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000209</td>\n",
              "      <td>0.000284</td>\n",
              "      <td>0.005875</td>\n",
              "      <td>2.055945e-03</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.005209</td>\n",
              "      <td>0.002066</td>\n",
              "      <td>0.001278</td>\n",
              "      <td>0.000910</td>\n",
              "      <td>0.000558</td>\n",
              "      <td>0.007713</td>\n",
              "      <td>0.001460</td>\n",
              "      <td>0.001001</td>\n",
              "      <td>0.004312</td>\n",
              "      <td>0.002773</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.003194</td>\n",
              "      <td>0.004629</td>\n",
              "      <td>0.006915</td>\n",
              "      <td>0.005239</td>\n",
              "      <td>0.004299</td>\n",
              "      <td>0.000857</td>\n",
              "      <td>0.002586</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.004965</td>\n",
              "      <td>0.002631</td>\n",
              "      <td>0.001603</td>\n",
              "      <td>0.007306</td>\n",
              "      <td>0.001706</td>\n",
              "      <td>0.003248</td>\n",
              "      <td>0.005276</td>\n",
              "      <td>0.006702</td>\n",
              "      <td>0.323725</td>\n",
              "      <td>0.275822</td>\n",
              "      <td>0.000195</td>\n",
              "      <td>0.004708</td>\n",
              "      <td>0.002589</td>\n",
              "      <td>0.001042</td>\n",
              "      <td>0.001231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>4.299276</td>\n",
              "      <td>0.012319</td>\n",
              "      <td>0.010059</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>0.011462</td>\n",
              "      <td>0.006868</td>\n",
              "      <td>0.002586</td>\n",
              "      <td>0.008481</td>\n",
              "      <td>0.004933</td>\n",
              "      <td>0.010551</td>\n",
              "      <td>0.015912</td>\n",
              "      <td>0.002026</td>\n",
              "      <td>0.012906</td>\n",
              "      <td>0.009198</td>\n",
              "      <td>0.003821</td>\n",
              "      <td>0.003440</td>\n",
              "      <td>2.653728e-02</td>\n",
              "      <td>0.001984</td>\n",
              "      <td>0.003804</td>\n",
              "      <td>0.005147</td>\n",
              "      <td>0.007715</td>\n",
              "      <td>0.001772</td>\n",
              "      <td>0.009386</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.011118</td>\n",
              "      <td>0.006340</td>\n",
              "      <td>0.004293</td>\n",
              "      <td>0.014184</td>\n",
              "      <td>0.010968</td>\n",
              "      <td>0.004046</td>\n",
              "      <td>0.002619</td>\n",
              "      <td>0.003029</td>\n",
              "      <td>0.001836</td>\n",
              "      <td>0.013034</td>\n",
              "      <td>0.001315</td>\n",
              "      <td>0.023489</td>\n",
              "      <td>0.003836</td>\n",
              "      <td>0.003846</td>\n",
              "      <td>0.002796</td>\n",
              "      <td>0.001809</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002844</td>\n",
              "      <td>0.001572</td>\n",
              "      <td>0.020641</td>\n",
              "      <td>4.807301e-03</td>\n",
              "      <td>0.015929</td>\n",
              "      <td>0.021323</td>\n",
              "      <td>0.002107</td>\n",
              "      <td>0.003867</td>\n",
              "      <td>0.012916</td>\n",
              "      <td>0.002676</td>\n",
              "      <td>0.012772</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.002565</td>\n",
              "      <td>0.007411</td>\n",
              "      <td>0.009141</td>\n",
              "      <td>0.006404</td>\n",
              "      <td>0.005958</td>\n",
              "      <td>0.003943</td>\n",
              "      <td>0.018944</td>\n",
              "      <td>0.018651</td>\n",
              "      <td>0.015501</td>\n",
              "      <td>0.015848</td>\n",
              "      <td>0.010756</td>\n",
              "      <td>0.013052</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.012589</td>\n",
              "      <td>0.008105</td>\n",
              "      <td>0.008805</td>\n",
              "      <td>0.019133</td>\n",
              "      <td>0.003839</td>\n",
              "      <td>0.009161</td>\n",
              "      <td>0.010425</td>\n",
              "      <td>0.015041</td>\n",
              "      <td>1.111582</td>\n",
              "      <td>1.007939</td>\n",
              "      <td>0.002873</td>\n",
              "      <td>0.019467</td>\n",
              "      <td>0.003995</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.004027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2.443880</td>\n",
              "      <td>0.003792</td>\n",
              "      <td>0.004574</td>\n",
              "      <td>0.007105</td>\n",
              "      <td>0.008738</td>\n",
              "      <td>0.006559</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.002635</td>\n",
              "      <td>0.003767</td>\n",
              "      <td>0.004645</td>\n",
              "      <td>0.003461</td>\n",
              "      <td>0.000829</td>\n",
              "      <td>0.008193</td>\n",
              "      <td>0.006768</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.003040</td>\n",
              "      <td>1.374055e-02</td>\n",
              "      <td>0.000452</td>\n",
              "      <td>0.003480</td>\n",
              "      <td>0.000868</td>\n",
              "      <td>0.005862</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>0.005544</td>\n",
              "      <td>0.001593</td>\n",
              "      <td>0.003607</td>\n",
              "      <td>0.005126</td>\n",
              "      <td>0.002995</td>\n",
              "      <td>0.009122</td>\n",
              "      <td>0.007033</td>\n",
              "      <td>0.001436</td>\n",
              "      <td>0.001240</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.001227</td>\n",
              "      <td>0.004133</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.008803</td>\n",
              "      <td>0.002972</td>\n",
              "      <td>0.005111</td>\n",
              "      <td>0.000649</td>\n",
              "      <td>0.003849</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001926</td>\n",
              "      <td>0.003076</td>\n",
              "      <td>0.010580</td>\n",
              "      <td>2.850241e-03</td>\n",
              "      <td>0.009610</td>\n",
              "      <td>0.010667</td>\n",
              "      <td>0.001299</td>\n",
              "      <td>0.002414</td>\n",
              "      <td>0.003986</td>\n",
              "      <td>0.001864</td>\n",
              "      <td>0.007263</td>\n",
              "      <td>0.001009</td>\n",
              "      <td>0.000612</td>\n",
              "      <td>0.004073</td>\n",
              "      <td>0.005354</td>\n",
              "      <td>0.005475</td>\n",
              "      <td>0.002559</td>\n",
              "      <td>0.002568</td>\n",
              "      <td>0.008642</td>\n",
              "      <td>0.012537</td>\n",
              "      <td>0.006262</td>\n",
              "      <td>0.006462</td>\n",
              "      <td>0.002087</td>\n",
              "      <td>0.007311</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>0.005633</td>\n",
              "      <td>0.001397</td>\n",
              "      <td>0.006851</td>\n",
              "      <td>0.011953</td>\n",
              "      <td>0.002873</td>\n",
              "      <td>0.005742</td>\n",
              "      <td>0.004834</td>\n",
              "      <td>0.011251</td>\n",
              "      <td>0.498801</td>\n",
              "      <td>0.442126</td>\n",
              "      <td>0.004441</td>\n",
              "      <td>0.010415</td>\n",
              "      <td>0.000580</td>\n",
              "      <td>0.000893</td>\n",
              "      <td>0.001299</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>0.066139</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.000592</td>\n",
              "      <td>0.000071</td>\n",
              "      <td>0.000906</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000845</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000090</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000413</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>1.081664e-04</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000560</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>0.000229</td>\n",
              "      <td>0.000228</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000168</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.000280</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.000363</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.000312</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000283</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>9.862962e-05</td>\n",
              "      <td>0.000094</td>\n",
              "      <td>0.000414</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000190</td>\n",
              "      <td>0.000191</td>\n",
              "      <td>0.000302</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000132</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.000144</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000232</td>\n",
              "      <td>0.000430</td>\n",
              "      <td>0.000177</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000096</td>\n",
              "      <td>0.000269</td>\n",
              "      <td>0.000169</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.044114</td>\n",
              "      <td>0.021336</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.000203</td>\n",
              "      <td>0.000163</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>1.841217</td>\n",
              "      <td>0.005249</td>\n",
              "      <td>0.002210</td>\n",
              "      <td>0.002625</td>\n",
              "      <td>0.000738</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.002380</td>\n",
              "      <td>0.000939</td>\n",
              "      <td>0.003474</td>\n",
              "      <td>0.001770</td>\n",
              "      <td>0.004225</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.004093</td>\n",
              "      <td>0.003153</td>\n",
              "      <td>0.002397</td>\n",
              "      <td>1.042081e-02</td>\n",
              "      <td>0.002195</td>\n",
              "      <td>0.003270</td>\n",
              "      <td>0.001372</td>\n",
              "      <td>0.003933</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.005533</td>\n",
              "      <td>0.000891</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.003073</td>\n",
              "      <td>0.002315</td>\n",
              "      <td>0.007078</td>\n",
              "      <td>0.005195</td>\n",
              "      <td>0.001882</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.001671</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.004503</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.004766</td>\n",
              "      <td>0.002638</td>\n",
              "      <td>0.001247</td>\n",
              "      <td>0.001747</td>\n",
              "      <td>0.002539</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001854</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>0.006756</td>\n",
              "      <td>2.714221e-03</td>\n",
              "      <td>0.000610</td>\n",
              "      <td>0.004378</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.000513</td>\n",
              "      <td>0.001847</td>\n",
              "      <td>0.008057</td>\n",
              "      <td>0.002258</td>\n",
              "      <td>0.000802</td>\n",
              "      <td>0.004378</td>\n",
              "      <td>0.002712</td>\n",
              "      <td>0.002293</td>\n",
              "      <td>0.003778</td>\n",
              "      <td>0.000429</td>\n",
              "      <td>0.005154</td>\n",
              "      <td>0.008550</td>\n",
              "      <td>0.003187</td>\n",
              "      <td>0.004509</td>\n",
              "      <td>0.002313</td>\n",
              "      <td>0.001332</td>\n",
              "      <td>0.000066</td>\n",
              "      <td>0.007903</td>\n",
              "      <td>0.003069</td>\n",
              "      <td>0.001063</td>\n",
              "      <td>0.006660</td>\n",
              "      <td>0.003094</td>\n",
              "      <td>0.001788</td>\n",
              "      <td>0.003740</td>\n",
              "      <td>0.006329</td>\n",
              "      <td>0.287231</td>\n",
              "      <td>0.240556</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>0.003530</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000931</td>\n",
              "      <td>0.000571</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>3.706681</td>\n",
              "      <td>0.013555</td>\n",
              "      <td>0.000687</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.002441</td>\n",
              "      <td>0.000316</td>\n",
              "      <td>0.006710</td>\n",
              "      <td>0.015306</td>\n",
              "      <td>0.002549</td>\n",
              "      <td>0.018907</td>\n",
              "      <td>0.002746</td>\n",
              "      <td>0.018931</td>\n",
              "      <td>0.008660</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.001472</td>\n",
              "      <td>9.329817e-03</td>\n",
              "      <td>0.012334</td>\n",
              "      <td>0.005884</td>\n",
              "      <td>0.001429</td>\n",
              "      <td>0.003999</td>\n",
              "      <td>0.003565</td>\n",
              "      <td>0.017943</td>\n",
              "      <td>0.003009</td>\n",
              "      <td>0.002308</td>\n",
              "      <td>0.000423</td>\n",
              "      <td>0.009234</td>\n",
              "      <td>0.020365</td>\n",
              "      <td>0.011623</td>\n",
              "      <td>0.016032</td>\n",
              "      <td>0.003441</td>\n",
              "      <td>0.004247</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.010185</td>\n",
              "      <td>0.001658</td>\n",
              "      <td>0.014107</td>\n",
              "      <td>0.009904</td>\n",
              "      <td>0.004753</td>\n",
              "      <td>0.000672</td>\n",
              "      <td>0.012131</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001868</td>\n",
              "      <td>0.000495</td>\n",
              "      <td>0.019733</td>\n",
              "      <td>4.558193e-03</td>\n",
              "      <td>0.006318</td>\n",
              "      <td>0.012308</td>\n",
              "      <td>0.001514</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>0.004234</td>\n",
              "      <td>0.002853</td>\n",
              "      <td>0.013365</td>\n",
              "      <td>0.000697</td>\n",
              "      <td>0.005559</td>\n",
              "      <td>0.008081</td>\n",
              "      <td>0.007421</td>\n",
              "      <td>0.013988</td>\n",
              "      <td>0.001935</td>\n",
              "      <td>0.007632</td>\n",
              "      <td>0.018075</td>\n",
              "      <td>0.015240</td>\n",
              "      <td>0.006248</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.004710</td>\n",
              "      <td>0.007397</td>\n",
              "      <td>0.009409</td>\n",
              "      <td>0.020106</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>0.003036</td>\n",
              "      <td>0.014525</td>\n",
              "      <td>0.001060</td>\n",
              "      <td>0.008056</td>\n",
              "      <td>0.010094</td>\n",
              "      <td>0.010716</td>\n",
              "      <td>0.897344</td>\n",
              "      <td>0.790583</td>\n",
              "      <td>0.018044</td>\n",
              "      <td>0.005722</td>\n",
              "      <td>0.002571</td>\n",
              "      <td>0.002472</td>\n",
              "      <td>0.008880</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>3.902672</td>\n",
              "      <td>0.004127</td>\n",
              "      <td>0.004427</td>\n",
              "      <td>0.010682</td>\n",
              "      <td>0.009297</td>\n",
              "      <td>0.001235</td>\n",
              "      <td>0.019935</td>\n",
              "      <td>0.002062</td>\n",
              "      <td>0.008243</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.009886</td>\n",
              "      <td>0.006446</td>\n",
              "      <td>0.022314</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.000156</td>\n",
              "      <td>0.002176</td>\n",
              "      <td>2.657606e-02</td>\n",
              "      <td>0.004230</td>\n",
              "      <td>0.002100</td>\n",
              "      <td>0.010949</td>\n",
              "      <td>0.009007</td>\n",
              "      <td>0.001546</td>\n",
              "      <td>0.010439</td>\n",
              "      <td>0.005219</td>\n",
              "      <td>0.002311</td>\n",
              "      <td>0.015438</td>\n",
              "      <td>0.003022</td>\n",
              "      <td>0.010529</td>\n",
              "      <td>0.014392</td>\n",
              "      <td>0.001019</td>\n",
              "      <td>0.004704</td>\n",
              "      <td>0.002679</td>\n",
              "      <td>0.001394</td>\n",
              "      <td>0.005715</td>\n",
              "      <td>0.003860</td>\n",
              "      <td>0.008559</td>\n",
              "      <td>0.012984</td>\n",
              "      <td>0.010006</td>\n",
              "      <td>0.002988</td>\n",
              "      <td>0.005129</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002245</td>\n",
              "      <td>0.005692</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>5.459392e-03</td>\n",
              "      <td>0.002621</td>\n",
              "      <td>0.010404</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>0.002642</td>\n",
              "      <td>0.001393</td>\n",
              "      <td>0.020967</td>\n",
              "      <td>0.001805</td>\n",
              "      <td>0.004876</td>\n",
              "      <td>0.011622</td>\n",
              "      <td>0.011801</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000541</td>\n",
              "      <td>0.000141</td>\n",
              "      <td>0.013314</td>\n",
              "      <td>0.016767</td>\n",
              "      <td>0.006806</td>\n",
              "      <td>0.009092</td>\n",
              "      <td>0.007471</td>\n",
              "      <td>0.000993</td>\n",
              "      <td>0.000298</td>\n",
              "      <td>0.012321</td>\n",
              "      <td>0.004564</td>\n",
              "      <td>0.000808</td>\n",
              "      <td>0.021111</td>\n",
              "      <td>0.008828</td>\n",
              "      <td>0.004231</td>\n",
              "      <td>0.012943</td>\n",
              "      <td>0.019230</td>\n",
              "      <td>0.976480</td>\n",
              "      <td>0.863353</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>0.010260</td>\n",
              "      <td>0.006714</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>0.001036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>4.079460</td>\n",
              "      <td>0.011127</td>\n",
              "      <td>0.002655</td>\n",
              "      <td>0.005392</td>\n",
              "      <td>0.007869</td>\n",
              "      <td>0.004230</td>\n",
              "      <td>0.003906</td>\n",
              "      <td>0.014205</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.004778</td>\n",
              "      <td>0.015202</td>\n",
              "      <td>0.011195</td>\n",
              "      <td>0.030587</td>\n",
              "      <td>0.006360</td>\n",
              "      <td>0.003940</td>\n",
              "      <td>0.006077</td>\n",
              "      <td>8.581555e-03</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.014965</td>\n",
              "      <td>0.006014</td>\n",
              "      <td>0.001958</td>\n",
              "      <td>0.006961</td>\n",
              "      <td>0.005391</td>\n",
              "      <td>0.003266</td>\n",
              "      <td>0.003158</td>\n",
              "      <td>0.012050</td>\n",
              "      <td>0.025180</td>\n",
              "      <td>0.008648</td>\n",
              "      <td>0.021792</td>\n",
              "      <td>0.004274</td>\n",
              "      <td>0.006444</td>\n",
              "      <td>0.004694</td>\n",
              "      <td>0.004792</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.012399</td>\n",
              "      <td>0.012895</td>\n",
              "      <td>0.002334</td>\n",
              "      <td>0.006878</td>\n",
              "      <td>0.016066</td>\n",
              "      <td>...</td>\n",
              "      <td>0.012435</td>\n",
              "      <td>0.005965</td>\n",
              "      <td>0.018619</td>\n",
              "      <td>1.255455e-02</td>\n",
              "      <td>0.003899</td>\n",
              "      <td>0.013663</td>\n",
              "      <td>0.003022</td>\n",
              "      <td>0.007356</td>\n",
              "      <td>0.002687</td>\n",
              "      <td>0.018485</td>\n",
              "      <td>0.016997</td>\n",
              "      <td>0.002222</td>\n",
              "      <td>0.008353</td>\n",
              "      <td>0.003175</td>\n",
              "      <td>0.008934</td>\n",
              "      <td>0.015550</td>\n",
              "      <td>0.004148</td>\n",
              "      <td>0.006454</td>\n",
              "      <td>0.018978</td>\n",
              "      <td>0.024145</td>\n",
              "      <td>0.002562</td>\n",
              "      <td>0.016871</td>\n",
              "      <td>0.005479</td>\n",
              "      <td>0.004303</td>\n",
              "      <td>0.009157</td>\n",
              "      <td>0.023762</td>\n",
              "      <td>0.014037</td>\n",
              "      <td>0.012508</td>\n",
              "      <td>0.004787</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.021054</td>\n",
              "      <td>0.003224</td>\n",
              "      <td>0.019589</td>\n",
              "      <td>1.024623</td>\n",
              "      <td>0.894329</td>\n",
              "      <td>0.006609</td>\n",
              "      <td>0.001839</td>\n",
              "      <td>0.008061</td>\n",
              "      <td>0.000279</td>\n",
              "      <td>0.000482</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>3.470469</td>\n",
              "      <td>0.008221</td>\n",
              "      <td>0.001529</td>\n",
              "      <td>0.003084</td>\n",
              "      <td>0.006619</td>\n",
              "      <td>0.002388</td>\n",
              "      <td>0.008586</td>\n",
              "      <td>0.008535</td>\n",
              "      <td>0.009140</td>\n",
              "      <td>0.005992</td>\n",
              "      <td>0.016684</td>\n",
              "      <td>0.001977</td>\n",
              "      <td>0.017526</td>\n",
              "      <td>0.000173</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.015129</td>\n",
              "      <td>2.250192e-02</td>\n",
              "      <td>0.001669</td>\n",
              "      <td>0.001676</td>\n",
              "      <td>0.003032</td>\n",
              "      <td>0.008422</td>\n",
              "      <td>0.000830</td>\n",
              "      <td>0.006275</td>\n",
              "      <td>0.003272</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>0.003548</td>\n",
              "      <td>0.001986</td>\n",
              "      <td>0.012589</td>\n",
              "      <td>0.002494</td>\n",
              "      <td>0.003852</td>\n",
              "      <td>0.001052</td>\n",
              "      <td>0.005250</td>\n",
              "      <td>0.008749</td>\n",
              "      <td>0.015138</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.024636</td>\n",
              "      <td>0.005058</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.010436</td>\n",
              "      <td>0.004824</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002413</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>0.017993</td>\n",
              "      <td>5.116954e-03</td>\n",
              "      <td>0.006062</td>\n",
              "      <td>0.011498</td>\n",
              "      <td>0.002300</td>\n",
              "      <td>0.006812</td>\n",
              "      <td>0.001899</td>\n",
              "      <td>0.010477</td>\n",
              "      <td>0.014418</td>\n",
              "      <td>0.001411</td>\n",
              "      <td>0.004021</td>\n",
              "      <td>0.010100</td>\n",
              "      <td>0.000428</td>\n",
              "      <td>0.004234</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.001118</td>\n",
              "      <td>0.024071</td>\n",
              "      <td>0.018572</td>\n",
              "      <td>0.007238</td>\n",
              "      <td>0.009511</td>\n",
              "      <td>0.010461</td>\n",
              "      <td>0.010066</td>\n",
              "      <td>0.003748</td>\n",
              "      <td>0.017451</td>\n",
              "      <td>0.013135</td>\n",
              "      <td>0.009010</td>\n",
              "      <td>0.010766</td>\n",
              "      <td>0.002875</td>\n",
              "      <td>0.008450</td>\n",
              "      <td>0.004117</td>\n",
              "      <td>0.012164</td>\n",
              "      <td>0.815197</td>\n",
              "      <td>0.721374</td>\n",
              "      <td>0.009820</td>\n",
              "      <td>0.003903</td>\n",
              "      <td>0.004143</td>\n",
              "      <td>0.004203</td>\n",
              "      <td>0.005768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>3.334816</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.005278</td>\n",
              "      <td>0.000127</td>\n",
              "      <td>0.001634</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.005305</td>\n",
              "      <td>0.002907</td>\n",
              "      <td>0.011213</td>\n",
              "      <td>0.008116</td>\n",
              "      <td>0.014722</td>\n",
              "      <td>0.002307</td>\n",
              "      <td>0.019702</td>\n",
              "      <td>0.002137</td>\n",
              "      <td>0.000143</td>\n",
              "      <td>0.005099</td>\n",
              "      <td>1.581070e-02</td>\n",
              "      <td>0.010751</td>\n",
              "      <td>0.006015</td>\n",
              "      <td>0.000742</td>\n",
              "      <td>0.005510</td>\n",
              "      <td>0.002750</td>\n",
              "      <td>0.013473</td>\n",
              "      <td>0.001860</td>\n",
              "      <td>0.001525</td>\n",
              "      <td>0.001485</td>\n",
              "      <td>0.010570</td>\n",
              "      <td>0.015668</td>\n",
              "      <td>0.007583</td>\n",
              "      <td>0.009033</td>\n",
              "      <td>0.003717</td>\n",
              "      <td>0.005694</td>\n",
              "      <td>0.003378</td>\n",
              "      <td>0.012735</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>0.018625</td>\n",
              "      <td>0.007787</td>\n",
              "      <td>0.005598</td>\n",
              "      <td>0.008307</td>\n",
              "      <td>0.011730</td>\n",
              "      <td>...</td>\n",
              "      <td>0.007166</td>\n",
              "      <td>0.003537</td>\n",
              "      <td>0.019259</td>\n",
              "      <td>7.929514e-03</td>\n",
              "      <td>0.005029</td>\n",
              "      <td>0.011905</td>\n",
              "      <td>0.003756</td>\n",
              "      <td>0.000883</td>\n",
              "      <td>0.001207</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.013850</td>\n",
              "      <td>0.001022</td>\n",
              "      <td>0.001939</td>\n",
              "      <td>0.006637</td>\n",
              "      <td>0.010220</td>\n",
              "      <td>0.010246</td>\n",
              "      <td>0.007135</td>\n",
              "      <td>0.003591</td>\n",
              "      <td>0.019578</td>\n",
              "      <td>0.019295</td>\n",
              "      <td>0.005204</td>\n",
              "      <td>0.002379</td>\n",
              "      <td>0.006868</td>\n",
              "      <td>0.006053</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>0.018399</td>\n",
              "      <td>0.014163</td>\n",
              "      <td>0.005055</td>\n",
              "      <td>0.007787</td>\n",
              "      <td>0.002220</td>\n",
              "      <td>0.002954</td>\n",
              "      <td>0.010681</td>\n",
              "      <td>0.013920</td>\n",
              "      <td>0.777104</td>\n",
              "      <td>0.685020</td>\n",
              "      <td>0.009713</td>\n",
              "      <td>0.008162</td>\n",
              "      <td>0.003197</td>\n",
              "      <td>0.007588</td>\n",
              "      <td>0.002268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>4.596904</td>\n",
              "      <td>0.011626</td>\n",
              "      <td>0.011411</td>\n",
              "      <td>0.007545</td>\n",
              "      <td>0.008523</td>\n",
              "      <td>0.004982</td>\n",
              "      <td>0.002566</td>\n",
              "      <td>0.006629</td>\n",
              "      <td>0.007055</td>\n",
              "      <td>0.010773</td>\n",
              "      <td>0.019667</td>\n",
              "      <td>0.001154</td>\n",
              "      <td>0.011385</td>\n",
              "      <td>0.007789</td>\n",
              "      <td>0.001146</td>\n",
              "      <td>0.003673</td>\n",
              "      <td>2.558360e-02</td>\n",
              "      <td>0.000561</td>\n",
              "      <td>0.006428</td>\n",
              "      <td>0.009152</td>\n",
              "      <td>0.008384</td>\n",
              "      <td>0.003706</td>\n",
              "      <td>0.011395</td>\n",
              "      <td>0.000890</td>\n",
              "      <td>0.017267</td>\n",
              "      <td>0.004546</td>\n",
              "      <td>0.002852</td>\n",
              "      <td>0.011562</td>\n",
              "      <td>0.010239</td>\n",
              "      <td>0.004966</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.001960</td>\n",
              "      <td>0.014837</td>\n",
              "      <td>0.000627</td>\n",
              "      <td>0.023487</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>0.003484</td>\n",
              "      <td>0.003433</td>\n",
              "      <td>0.004698</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001182</td>\n",
              "      <td>0.005613</td>\n",
              "      <td>0.017115</td>\n",
              "      <td>2.915740e-03</td>\n",
              "      <td>0.018548</td>\n",
              "      <td>0.019297</td>\n",
              "      <td>0.002652</td>\n",
              "      <td>0.005243</td>\n",
              "      <td>0.013755</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.013938</td>\n",
              "      <td>0.003445</td>\n",
              "      <td>0.006401</td>\n",
              "      <td>0.009418</td>\n",
              "      <td>0.012167</td>\n",
              "      <td>0.006105</td>\n",
              "      <td>0.005319</td>\n",
              "      <td>0.004196</td>\n",
              "      <td>0.016725</td>\n",
              "      <td>0.018819</td>\n",
              "      <td>0.015472</td>\n",
              "      <td>0.012077</td>\n",
              "      <td>0.008239</td>\n",
              "      <td>0.012440</td>\n",
              "      <td>0.005107</td>\n",
              "      <td>0.012035</td>\n",
              "      <td>0.011980</td>\n",
              "      <td>0.006082</td>\n",
              "      <td>0.021389</td>\n",
              "      <td>0.003577</td>\n",
              "      <td>0.011647</td>\n",
              "      <td>0.009782</td>\n",
              "      <td>0.017188</td>\n",
              "      <td>1.199856</td>\n",
              "      <td>1.093996</td>\n",
              "      <td>0.002801</td>\n",
              "      <td>0.017369</td>\n",
              "      <td>0.004291</td>\n",
              "      <td>0.001323</td>\n",
              "      <td>0.001288</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2.545666</td>\n",
              "      <td>0.007399</td>\n",
              "      <td>0.005647</td>\n",
              "      <td>0.003454</td>\n",
              "      <td>0.006982</td>\n",
              "      <td>0.003731</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.003885</td>\n",
              "      <td>0.002233</td>\n",
              "      <td>0.004246</td>\n",
              "      <td>0.008861</td>\n",
              "      <td>0.000310</td>\n",
              "      <td>0.005327</td>\n",
              "      <td>0.004586</td>\n",
              "      <td>0.001406</td>\n",
              "      <td>0.001891</td>\n",
              "      <td>1.568622e-02</td>\n",
              "      <td>0.001815</td>\n",
              "      <td>0.002870</td>\n",
              "      <td>0.003125</td>\n",
              "      <td>0.003798</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.005267</td>\n",
              "      <td>0.000149</td>\n",
              "      <td>0.006482</td>\n",
              "      <td>0.002924</td>\n",
              "      <td>0.002033</td>\n",
              "      <td>0.007654</td>\n",
              "      <td>0.005894</td>\n",
              "      <td>0.000599</td>\n",
              "      <td>0.001526</td>\n",
              "      <td>0.002060</td>\n",
              "      <td>0.001189</td>\n",
              "      <td>0.006762</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.010913</td>\n",
              "      <td>0.003085</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.000867</td>\n",
              "      <td>0.001003</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001828</td>\n",
              "      <td>0.000477</td>\n",
              "      <td>0.011697</td>\n",
              "      <td>3.193358e-03</td>\n",
              "      <td>0.009480</td>\n",
              "      <td>0.011091</td>\n",
              "      <td>0.001202</td>\n",
              "      <td>0.001968</td>\n",
              "      <td>0.007486</td>\n",
              "      <td>0.001584</td>\n",
              "      <td>0.006703</td>\n",
              "      <td>0.003004</td>\n",
              "      <td>0.003124</td>\n",
              "      <td>0.003255</td>\n",
              "      <td>0.005767</td>\n",
              "      <td>0.003096</td>\n",
              "      <td>0.001707</td>\n",
              "      <td>0.002836</td>\n",
              "      <td>0.009849</td>\n",
              "      <td>0.010910</td>\n",
              "      <td>0.008397</td>\n",
              "      <td>0.007359</td>\n",
              "      <td>0.004466</td>\n",
              "      <td>0.006556</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.007510</td>\n",
              "      <td>0.003917</td>\n",
              "      <td>0.004593</td>\n",
              "      <td>0.009759</td>\n",
              "      <td>0.002742</td>\n",
              "      <td>0.005854</td>\n",
              "      <td>0.005829</td>\n",
              "      <td>0.008659</td>\n",
              "      <td>0.522858</td>\n",
              "      <td>0.463783</td>\n",
              "      <td>0.002378</td>\n",
              "      <td>0.010099</td>\n",
              "      <td>0.001870</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>0.001558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2.570623</td>\n",
              "      <td>0.007328</td>\n",
              "      <td>0.002250</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.003279</td>\n",
              "      <td>0.001966</td>\n",
              "      <td>0.007083</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.004978</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.006323</td>\n",
              "      <td>0.008336</td>\n",
              "      <td>0.001775</td>\n",
              "      <td>0.000247</td>\n",
              "      <td>8.691745e-03</td>\n",
              "      <td>0.001767</td>\n",
              "      <td>0.000590</td>\n",
              "      <td>0.001610</td>\n",
              "      <td>0.000841</td>\n",
              "      <td>0.004562</td>\n",
              "      <td>0.008055</td>\n",
              "      <td>0.000847</td>\n",
              "      <td>0.001827</td>\n",
              "      <td>0.002858</td>\n",
              "      <td>0.002980</td>\n",
              "      <td>0.004264</td>\n",
              "      <td>0.006512</td>\n",
              "      <td>0.002818</td>\n",
              "      <td>0.004343</td>\n",
              "      <td>0.000555</td>\n",
              "      <td>0.002645</td>\n",
              "      <td>0.003215</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>0.008154</td>\n",
              "      <td>0.007500</td>\n",
              "      <td>0.005183</td>\n",
              "      <td>0.001212</td>\n",
              "      <td>0.005852</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001942</td>\n",
              "      <td>0.000218</td>\n",
              "      <td>0.006198</td>\n",
              "      <td>5.081907e-03</td>\n",
              "      <td>0.003026</td>\n",
              "      <td>0.011750</td>\n",
              "      <td>0.000756</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.001170</td>\n",
              "      <td>0.002721</td>\n",
              "      <td>0.006730</td>\n",
              "      <td>0.000394</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.005949</td>\n",
              "      <td>0.008405</td>\n",
              "      <td>0.004258</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>0.001956</td>\n",
              "      <td>0.006208</td>\n",
              "      <td>0.012745</td>\n",
              "      <td>0.007804</td>\n",
              "      <td>0.008143</td>\n",
              "      <td>0.001769</td>\n",
              "      <td>0.002809</td>\n",
              "      <td>0.003363</td>\n",
              "      <td>0.008778</td>\n",
              "      <td>0.004017</td>\n",
              "      <td>0.003418</td>\n",
              "      <td>0.008670</td>\n",
              "      <td>0.002932</td>\n",
              "      <td>0.003376</td>\n",
              "      <td>0.010093</td>\n",
              "      <td>0.008535</td>\n",
              "      <td>0.526937</td>\n",
              "      <td>0.461477</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.005786</td>\n",
              "      <td>0.002134</td>\n",
              "      <td>0.002387</td>\n",
              "      <td>0.002721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>41</th>\n",
              "      <td>1.626000</td>\n",
              "      <td>0.003188</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000661</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001211</td>\n",
              "      <td>0.005195</td>\n",
              "      <td>0.001082</td>\n",
              "      <td>0.002096</td>\n",
              "      <td>0.000419</td>\n",
              "      <td>0.004179</td>\n",
              "      <td>0.001051</td>\n",
              "      <td>0.006129</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.000740</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>6.972603e-03</td>\n",
              "      <td>0.000958</td>\n",
              "      <td>0.000929</td>\n",
              "      <td>0.003549</td>\n",
              "      <td>0.003314</td>\n",
              "      <td>0.002038</td>\n",
              "      <td>0.004064</td>\n",
              "      <td>0.000319</td>\n",
              "      <td>0.002042</td>\n",
              "      <td>0.003723</td>\n",
              "      <td>0.002219</td>\n",
              "      <td>0.000851</td>\n",
              "      <td>0.005889</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000919</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>0.000501</td>\n",
              "      <td>0.000479</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.002074</td>\n",
              "      <td>0.002882</td>\n",
              "      <td>0.003484</td>\n",
              "      <td>0.001585</td>\n",
              "      <td>0.003955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000493</td>\n",
              "      <td>0.001569</td>\n",
              "      <td>0.003803</td>\n",
              "      <td>1.201784e-04</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.004144</td>\n",
              "      <td>0.000836</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>0.000776</td>\n",
              "      <td>0.004023</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000689</td>\n",
              "      <td>0.002168</td>\n",
              "      <td>0.002573</td>\n",
              "      <td>0.000746</td>\n",
              "      <td>0.000213</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>0.004077</td>\n",
              "      <td>0.004749</td>\n",
              "      <td>0.001138</td>\n",
              "      <td>0.001765</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.001586</td>\n",
              "      <td>0.000549</td>\n",
              "      <td>0.005195</td>\n",
              "      <td>0.002363</td>\n",
              "      <td>0.001580</td>\n",
              "      <td>0.006618</td>\n",
              "      <td>0.001896</td>\n",
              "      <td>0.001108</td>\n",
              "      <td>0.003916</td>\n",
              "      <td>0.007656</td>\n",
              "      <td>0.211316</td>\n",
              "      <td>0.169877</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.002872</td>\n",
              "      <td>0.001327</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>42</th>\n",
              "      <td>2.506036</td>\n",
              "      <td>0.006708</td>\n",
              "      <td>0.004712</td>\n",
              "      <td>0.005690</td>\n",
              "      <td>0.002778</td>\n",
              "      <td>0.001858</td>\n",
              "      <td>0.002868</td>\n",
              "      <td>0.004681</td>\n",
              "      <td>0.007036</td>\n",
              "      <td>0.005701</td>\n",
              "      <td>0.009542</td>\n",
              "      <td>0.000039</td>\n",
              "      <td>0.010417</td>\n",
              "      <td>0.005467</td>\n",
              "      <td>0.001107</td>\n",
              "      <td>0.002007</td>\n",
              "      <td>1.667544e-02</td>\n",
              "      <td>0.002306</td>\n",
              "      <td>0.001618</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.009132</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.007929</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>0.008385</td>\n",
              "      <td>0.004655</td>\n",
              "      <td>0.002722</td>\n",
              "      <td>0.007916</td>\n",
              "      <td>0.006330</td>\n",
              "      <td>0.000632</td>\n",
              "      <td>0.003396</td>\n",
              "      <td>0.001633</td>\n",
              "      <td>0.000045</td>\n",
              "      <td>0.006831</td>\n",
              "      <td>0.001957</td>\n",
              "      <td>0.011088</td>\n",
              "      <td>0.001628</td>\n",
              "      <td>0.001508</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.009697</td>\n",
              "      <td>...</td>\n",
              "      <td>0.002044</td>\n",
              "      <td>0.001834</td>\n",
              "      <td>0.008295</td>\n",
              "      <td>2.747289e-03</td>\n",
              "      <td>0.005157</td>\n",
              "      <td>0.009149</td>\n",
              "      <td>0.001811</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.005301</td>\n",
              "      <td>0.006529</td>\n",
              "      <td>0.004913</td>\n",
              "      <td>0.002977</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.002898</td>\n",
              "      <td>0.006693</td>\n",
              "      <td>0.001518</td>\n",
              "      <td>0.001922</td>\n",
              "      <td>0.000420</td>\n",
              "      <td>0.010864</td>\n",
              "      <td>0.011164</td>\n",
              "      <td>0.005629</td>\n",
              "      <td>0.005419</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.003193</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>0.008266</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>0.008821</td>\n",
              "      <td>0.014824</td>\n",
              "      <td>0.001801</td>\n",
              "      <td>0.000822</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.008523</td>\n",
              "      <td>0.510583</td>\n",
              "      <td>0.454115</td>\n",
              "      <td>0.002133</td>\n",
              "      <td>0.010953</td>\n",
              "      <td>0.004485</td>\n",
              "      <td>0.000864</td>\n",
              "      <td>0.001154</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>43</th>\n",
              "      <td>0.988799</td>\n",
              "      <td>0.001703</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>0.000537</td>\n",
              "      <td>0.001963</td>\n",
              "      <td>0.001631</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000832</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.001722</td>\n",
              "      <td>0.001176</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.001593</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>3.460770e-03</td>\n",
              "      <td>0.000237</td>\n",
              "      <td>0.001886</td>\n",
              "      <td>0.000422</td>\n",
              "      <td>0.001443</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>0.002372</td>\n",
              "      <td>0.000352</td>\n",
              "      <td>0.001127</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.002136</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.001763</td>\n",
              "      <td>0.000062</td>\n",
              "      <td>0.001097</td>\n",
              "      <td>0.000293</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.002065</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>0.000432</td>\n",
              "      <td>0.000902</td>\n",
              "      <td>0.001965</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.000924</td>\n",
              "      <td>0.003318</td>\n",
              "      <td>3.812016e-04</td>\n",
              "      <td>0.000536</td>\n",
              "      <td>0.001740</td>\n",
              "      <td>0.000699</td>\n",
              "      <td>0.000288</td>\n",
              "      <td>0.001824</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.000426</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>0.000079</td>\n",
              "      <td>0.000199</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000036</td>\n",
              "      <td>0.001398</td>\n",
              "      <td>0.002570</td>\n",
              "      <td>0.000773</td>\n",
              "      <td>0.000527</td>\n",
              "      <td>0.000587</td>\n",
              "      <td>0.001757</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.003048</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000087</td>\n",
              "      <td>0.001792</td>\n",
              "      <td>0.000107</td>\n",
              "      <td>0.001031</td>\n",
              "      <td>0.001912</td>\n",
              "      <td>0.001848</td>\n",
              "      <td>0.032265</td>\n",
              "      <td>0.017667</td>\n",
              "      <td>0.000859</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.000876</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>3.746928</td>\n",
              "      <td>0.009938</td>\n",
              "      <td>0.007092</td>\n",
              "      <td>0.004021</td>\n",
              "      <td>0.004443</td>\n",
              "      <td>0.001795</td>\n",
              "      <td>0.006776</td>\n",
              "      <td>0.002615</td>\n",
              "      <td>0.018026</td>\n",
              "      <td>0.001320</td>\n",
              "      <td>0.022331</td>\n",
              "      <td>0.001113</td>\n",
              "      <td>0.014045</td>\n",
              "      <td>0.002756</td>\n",
              "      <td>0.003724</td>\n",
              "      <td>0.003839</td>\n",
              "      <td>1.769920e-02</td>\n",
              "      <td>0.007076</td>\n",
              "      <td>0.003614</td>\n",
              "      <td>0.002271</td>\n",
              "      <td>0.010683</td>\n",
              "      <td>0.002284</td>\n",
              "      <td>0.004489</td>\n",
              "      <td>0.000533</td>\n",
              "      <td>0.003475</td>\n",
              "      <td>0.001340</td>\n",
              "      <td>0.008038</td>\n",
              "      <td>0.007701</td>\n",
              "      <td>0.008097</td>\n",
              "      <td>0.009757</td>\n",
              "      <td>0.006890</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>0.001914</td>\n",
              "      <td>0.007087</td>\n",
              "      <td>0.009224</td>\n",
              "      <td>0.022608</td>\n",
              "      <td>0.001356</td>\n",
              "      <td>0.000387</td>\n",
              "      <td>0.002747</td>\n",
              "      <td>0.005765</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005594</td>\n",
              "      <td>0.004700</td>\n",
              "      <td>0.015743</td>\n",
              "      <td>3.247744e-03</td>\n",
              "      <td>0.010289</td>\n",
              "      <td>0.018230</td>\n",
              "      <td>0.005634</td>\n",
              "      <td>0.002727</td>\n",
              "      <td>0.000583</td>\n",
              "      <td>0.001806</td>\n",
              "      <td>0.020270</td>\n",
              "      <td>0.005151</td>\n",
              "      <td>0.002542</td>\n",
              "      <td>0.015585</td>\n",
              "      <td>0.007373</td>\n",
              "      <td>0.003024</td>\n",
              "      <td>0.004476</td>\n",
              "      <td>0.001354</td>\n",
              "      <td>0.014431</td>\n",
              "      <td>0.014162</td>\n",
              "      <td>0.011068</td>\n",
              "      <td>0.016006</td>\n",
              "      <td>0.013351</td>\n",
              "      <td>0.010564</td>\n",
              "      <td>0.000354</td>\n",
              "      <td>0.017544</td>\n",
              "      <td>0.008802</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.010669</td>\n",
              "      <td>0.004083</td>\n",
              "      <td>0.004394</td>\n",
              "      <td>0.020856</td>\n",
              "      <td>0.015805</td>\n",
              "      <td>0.921428</td>\n",
              "      <td>0.821414</td>\n",
              "      <td>0.002241</td>\n",
              "      <td>0.010248</td>\n",
              "      <td>0.001174</td>\n",
              "      <td>0.004559</td>\n",
              "      <td>0.007412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>45</th>\n",
              "      <td>2.158018</td>\n",
              "      <td>0.002502</td>\n",
              "      <td>0.001895</td>\n",
              "      <td>0.002691</td>\n",
              "      <td>0.002648</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.005099</td>\n",
              "      <td>0.000651</td>\n",
              "      <td>0.004626</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.004513</td>\n",
              "      <td>0.000056</td>\n",
              "      <td>0.007878</td>\n",
              "      <td>0.003981</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.002228</td>\n",
              "      <td>1.231658e-02</td>\n",
              "      <td>0.000835</td>\n",
              "      <td>0.001383</td>\n",
              "      <td>0.001337</td>\n",
              "      <td>0.003733</td>\n",
              "      <td>0.001970</td>\n",
              "      <td>0.005339</td>\n",
              "      <td>0.001294</td>\n",
              "      <td>0.000117</td>\n",
              "      <td>0.004470</td>\n",
              "      <td>0.001219</td>\n",
              "      <td>0.003095</td>\n",
              "      <td>0.006582</td>\n",
              "      <td>0.001572</td>\n",
              "      <td>0.000504</td>\n",
              "      <td>0.001838</td>\n",
              "      <td>0.001924</td>\n",
              "      <td>0.003155</td>\n",
              "      <td>0.001284</td>\n",
              "      <td>0.004591</td>\n",
              "      <td>0.002933</td>\n",
              "      <td>0.004612</td>\n",
              "      <td>0.001384</td>\n",
              "      <td>0.006895</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000970</td>\n",
              "      <td>0.001798</td>\n",
              "      <td>0.004726</td>\n",
              "      <td>1.234304e-04</td>\n",
              "      <td>0.001537</td>\n",
              "      <td>0.006159</td>\n",
              "      <td>0.000343</td>\n",
              "      <td>0.001614</td>\n",
              "      <td>0.003732</td>\n",
              "      <td>0.001473</td>\n",
              "      <td>0.007829</td>\n",
              "      <td>0.000371</td>\n",
              "      <td>0.000043</td>\n",
              "      <td>0.005153</td>\n",
              "      <td>0.002606</td>\n",
              "      <td>0.001782</td>\n",
              "      <td>0.002567</td>\n",
              "      <td>0.000516</td>\n",
              "      <td>0.004194</td>\n",
              "      <td>0.009743</td>\n",
              "      <td>0.003862</td>\n",
              "      <td>0.006179</td>\n",
              "      <td>0.001221</td>\n",
              "      <td>0.000961</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.007631</td>\n",
              "      <td>0.001556</td>\n",
              "      <td>0.003189</td>\n",
              "      <td>0.009387</td>\n",
              "      <td>0.004227</td>\n",
              "      <td>0.000693</td>\n",
              "      <td>0.004841</td>\n",
              "      <td>0.010826</td>\n",
              "      <td>0.391570</td>\n",
              "      <td>0.331632</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.002830</td>\n",
              "      <td>0.001714</td>\n",
              "      <td>0.000670</td>\n",
              "      <td>0.002040</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2.592563</td>\n",
              "      <td>0.007846</td>\n",
              "      <td>0.002006</td>\n",
              "      <td>0.003326</td>\n",
              "      <td>0.006332</td>\n",
              "      <td>0.004472</td>\n",
              "      <td>0.003753</td>\n",
              "      <td>0.002902</td>\n",
              "      <td>0.004464</td>\n",
              "      <td>0.003682</td>\n",
              "      <td>0.008846</td>\n",
              "      <td>0.002235</td>\n",
              "      <td>0.006085</td>\n",
              "      <td>0.004089</td>\n",
              "      <td>0.002759</td>\n",
              "      <td>0.001605</td>\n",
              "      <td>1.481862e-02</td>\n",
              "      <td>0.001636</td>\n",
              "      <td>0.001778</td>\n",
              "      <td>0.004254</td>\n",
              "      <td>0.004918</td>\n",
              "      <td>0.000653</td>\n",
              "      <td>0.005736</td>\n",
              "      <td>0.000380</td>\n",
              "      <td>0.004067</td>\n",
              "      <td>0.008816</td>\n",
              "      <td>0.001951</td>\n",
              "      <td>0.005681</td>\n",
              "      <td>0.006891</td>\n",
              "      <td>0.001628</td>\n",
              "      <td>0.003143</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.002451</td>\n",
              "      <td>0.001081</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>0.003616</td>\n",
              "      <td>0.002531</td>\n",
              "      <td>0.003078</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.007130</td>\n",
              "      <td>2.736634e-03</td>\n",
              "      <td>0.005809</td>\n",
              "      <td>0.009166</td>\n",
              "      <td>0.001655</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.000823</td>\n",
              "      <td>0.000214</td>\n",
              "      <td>0.010509</td>\n",
              "      <td>0.002528</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.005706</td>\n",
              "      <td>0.006096</td>\n",
              "      <td>0.003413</td>\n",
              "      <td>0.001787</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>0.006236</td>\n",
              "      <td>0.009546</td>\n",
              "      <td>0.006285</td>\n",
              "      <td>0.006691</td>\n",
              "      <td>0.002614</td>\n",
              "      <td>0.004170</td>\n",
              "      <td>0.001190</td>\n",
              "      <td>0.008311</td>\n",
              "      <td>0.005502</td>\n",
              "      <td>0.002955</td>\n",
              "      <td>0.010221</td>\n",
              "      <td>0.002318</td>\n",
              "      <td>0.004184</td>\n",
              "      <td>0.006840</td>\n",
              "      <td>0.009138</td>\n",
              "      <td>0.534244</td>\n",
              "      <td>0.471135</td>\n",
              "      <td>0.000359</td>\n",
              "      <td>0.007972</td>\n",
              "      <td>0.003455</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.001352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>47</th>\n",
              "      <td>0.092976</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000223</td>\n",
              "      <td>0.000234</td>\n",
              "      <td>0.000551</td>\n",
              "      <td>0.000016</td>\n",
              "      <td>0.000771</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000158</td>\n",
              "      <td>0.000854</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.000241</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000028</td>\n",
              "      <td>0.000381</td>\n",
              "      <td>0.000122</td>\n",
              "      <td>5.921349e-06</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000578</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000146</td>\n",
              "      <td>0.000040</td>\n",
              "      <td>0.000194</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>0.000278</td>\n",
              "      <td>0.000253</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000188</td>\n",
              "      <td>0.000273</td>\n",
              "      <td>0.000597</td>\n",
              "      <td>0.000412</td>\n",
              "      <td>0.000523</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000257</td>\n",
              "      <td>0.000311</td>\n",
              "      <td>1.269844e-04</td>\n",
              "      <td>0.000198</td>\n",
              "      <td>0.000481</td>\n",
              "      <td>0.000201</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>0.000147</td>\n",
              "      <td>0.000294</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000438</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.000070</td>\n",
              "      <td>0.000221</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>0.000193</td>\n",
              "      <td>0.000176</td>\n",
              "      <td>0.000265</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.000184</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.000286</td>\n",
              "      <td>0.000239</td>\n",
              "      <td>0.000321</td>\n",
              "      <td>0.034603</td>\n",
              "      <td>0.014204</td>\n",
              "      <td>0.000421</td>\n",
              "      <td>0.000119</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000126</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>48</th>\n",
              "      <td>0.554565</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000803</td>\n",
              "      <td>0.001123</td>\n",
              "      <td>0.000566</td>\n",
              "      <td>0.000507</td>\n",
              "      <td>0.000395</td>\n",
              "      <td>0.001076</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000377</td>\n",
              "      <td>0.000313</td>\n",
              "      <td>0.000171</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>2.501351e-04</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.000183</td>\n",
              "      <td>0.000336</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000186</td>\n",
              "      <td>0.001387</td>\n",
              "      <td>0.000761</td>\n",
              "      <td>0.000318</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000369</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000267</td>\n",
              "      <td>0.000116</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000433</td>\n",
              "      <td>0.000304</td>\n",
              "      <td>0.000573</td>\n",
              "      <td>0.000698</td>\n",
              "      <td>0.000643</td>\n",
              "      <td>0.000285</td>\n",
              "      <td>0.000567</td>\n",
              "      <td>0.000548</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000383</td>\n",
              "      <td>0.000750</td>\n",
              "      <td>0.000663</td>\n",
              "      <td>8.661078e-04</td>\n",
              "      <td>0.000565</td>\n",
              "      <td>0.000434</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.001090</td>\n",
              "      <td>0.000262</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>0.000766</td>\n",
              "      <td>0.000182</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000135</td>\n",
              "      <td>0.000261</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.000460</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>0.000389</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000301</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000713</td>\n",
              "      <td>0.000115</td>\n",
              "      <td>0.000542</td>\n",
              "      <td>0.000303</td>\n",
              "      <td>0.000656</td>\n",
              "      <td>0.000251</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>0.000881</td>\n",
              "      <td>0.002724</td>\n",
              "      <td>0.000727</td>\n",
              "      <td>0.000683</td>\n",
              "      <td>0.000074</td>\n",
              "      <td>0.000866</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>0.000330</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>2.456249</td>\n",
              "      <td>0.000478</td>\n",
              "      <td>0.003227</td>\n",
              "      <td>0.003100</td>\n",
              "      <td>0.002214</td>\n",
              "      <td>0.003553</td>\n",
              "      <td>0.004720</td>\n",
              "      <td>0.002740</td>\n",
              "      <td>0.005312</td>\n",
              "      <td>0.003715</td>\n",
              "      <td>0.005863</td>\n",
              "      <td>0.005407</td>\n",
              "      <td>0.016399</td>\n",
              "      <td>0.002142</td>\n",
              "      <td>0.001901</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>1.129841e-02</td>\n",
              "      <td>0.003677</td>\n",
              "      <td>0.007622</td>\n",
              "      <td>0.006528</td>\n",
              "      <td>0.006746</td>\n",
              "      <td>0.002575</td>\n",
              "      <td>0.005406</td>\n",
              "      <td>0.000401</td>\n",
              "      <td>0.009310</td>\n",
              "      <td>0.010349</td>\n",
              "      <td>0.002967</td>\n",
              "      <td>0.002161</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>0.001418</td>\n",
              "      <td>0.004674</td>\n",
              "      <td>0.000494</td>\n",
              "      <td>0.000391</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.001910</td>\n",
              "      <td>0.004173</td>\n",
              "      <td>0.001300</td>\n",
              "      <td>0.001745</td>\n",
              "      <td>0.000515</td>\n",
              "      <td>0.006520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001196</td>\n",
              "      <td>0.000364</td>\n",
              "      <td>0.001117</td>\n",
              "      <td>2.339486e-04</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.004124</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.002389</td>\n",
              "      <td>0.000333</td>\n",
              "      <td>0.002603</td>\n",
              "      <td>0.011222</td>\n",
              "      <td>0.002112</td>\n",
              "      <td>0.004388</td>\n",
              "      <td>0.003866</td>\n",
              "      <td>0.002683</td>\n",
              "      <td>0.000448</td>\n",
              "      <td>0.003133</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.006247</td>\n",
              "      <td>0.002252</td>\n",
              "      <td>0.003796</td>\n",
              "      <td>0.002440</td>\n",
              "      <td>0.002255</td>\n",
              "      <td>0.007250</td>\n",
              "      <td>0.006064</td>\n",
              "      <td>0.009766</td>\n",
              "      <td>0.003960</td>\n",
              "      <td>0.014158</td>\n",
              "      <td>0.003219</td>\n",
              "      <td>0.000631</td>\n",
              "      <td>0.008206</td>\n",
              "      <td>0.012873</td>\n",
              "      <td>0.487539</td>\n",
              "      <td>0.428655</td>\n",
              "      <td>0.002262</td>\n",
              "      <td>0.006582</td>\n",
              "      <td>0.005787</td>\n",
              "      <td>0.001665</td>\n",
              "      <td>0.002065</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>50 rows Ã— 1351 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2     ...      1348      1349      1350\n",
              "0   0.136901  0.000289  0.000323  ...  0.000070  0.000227  0.000018\n",
              "1   4.161448  0.007069  0.002558  ...  0.000966  0.008188  0.006677\n",
              "2   0.344760  0.000149  0.000294  ...  0.000485  0.000411  0.000109\n",
              "3   1.533932  0.004435  0.002577  ...  0.001985  0.001812  0.001010\n",
              "4   0.842596  0.000099  0.000544  ...  0.001477  0.000338  0.000579\n",
              "5   0.740930  0.000041  0.000492  ...  0.000795  0.000450  0.000549\n",
              "6   1.810221  0.003367  0.002930  ...  0.000728  0.000953  0.001020\n",
              "7   3.632051  0.002685  0.005672  ...  0.001233  0.001409  0.003147\n",
              "8   4.176743  0.003363  0.003430  ...  0.003083  0.000649  0.000160\n",
              "9   2.054388  0.003817  0.002567  ...  0.001043  0.001486  0.000283\n",
              "10  1.314669  0.002875  0.002325  ...  0.000016  0.001390  0.000385\n",
              "11  3.961283  0.004083  0.008334  ...  0.001612  0.005387  0.000059\n",
              "12  3.180053  0.009991  0.002671  ...  0.000151  0.003635  0.000503\n",
              "13  0.285817  0.000150  0.000375  ...  0.000383  0.000437  0.000097\n",
              "14  0.865202  0.000703  0.000084  ...  0.001442  0.000548  0.000443\n",
              "15  1.199399  0.003219  0.001794  ...  0.000590  0.000504  0.000371\n",
              "16  2.126948  0.005536  0.006040  ...  0.003068  0.001518  0.000114\n",
              "17  1.465655  0.002938  0.002274  ...  0.000213  0.000527  0.002188\n",
              "18  2.306023  0.006238  0.006765  ...  0.003352  0.001616  0.000265\n",
              "19  3.683190  0.009257  0.002557  ...  0.004940  0.002907  0.001807\n",
              "20  0.722515  0.000522  0.000233  ...  0.001251  0.000513  0.000276\n",
              "21  0.165401  0.000365  0.000381  ...  0.000134  0.000245  0.000015\n",
              "22  1.928387  0.001715  0.001605  ...  0.002764  0.003772  0.000168\n",
              "23  2.861668  0.007413  0.003407  ...  0.004626  0.001168  0.000523\n",
              "24  0.382772  0.000108  0.000303  ...  0.000600  0.000416  0.000110\n",
              "25  1.415485  0.001760  0.000248  ...  0.000879  0.000152  0.001374\n",
              "26  1.026511  0.000715  0.000367  ...  0.001694  0.000634  0.000316\n",
              "27  1.335107  0.001899  0.001970  ...  0.000871  0.000696  0.000159\n",
              "28  1.957236  0.004847  0.001459  ...  0.002589  0.001042  0.001231\n",
              "29  4.299276  0.012319  0.010059  ...  0.003995  0.001051  0.004027\n",
              "30  2.443880  0.003792  0.004574  ...  0.000580  0.000893  0.001299\n",
              "31  0.066139  0.000288  0.000261  ...  0.000327  0.000203  0.000163\n",
              "32  1.841217  0.005249  0.002210  ...  0.000022  0.000931  0.000571\n",
              "33  3.706681  0.013555  0.000687  ...  0.002571  0.002472  0.008880\n",
              "34  3.902672  0.004127  0.004427  ...  0.006714  0.001997  0.001036\n",
              "35  4.079460  0.011127  0.002655  ...  0.008061  0.000279  0.000482\n",
              "36  3.470469  0.008221  0.001529  ...  0.004143  0.004203  0.005768\n",
              "37  3.334816  0.010040  0.005278  ...  0.003197  0.007588  0.002268\n",
              "38  4.596904  0.011626  0.011411  ...  0.004291  0.001323  0.001288\n",
              "39  2.545666  0.007399  0.005647  ...  0.001870  0.000114  0.001558\n",
              "40  2.570623  0.007328  0.002250  ...  0.002134  0.002387  0.002721\n",
              "41  1.626000  0.003188  0.000336  ...  0.001327  0.000300  0.000800\n",
              "42  2.506036  0.006708  0.004712  ...  0.004485  0.000864  0.001154\n",
              "43  0.988799  0.001703  0.000095  ...  0.000876  0.000750  0.000218\n",
              "44  3.746928  0.009938  0.007092  ...  0.001174  0.004559  0.007412\n",
              "45  2.158018  0.002502  0.001895  ...  0.001714  0.000670  0.002040\n",
              "46  2.592563  0.007846  0.002006  ...  0.003455  0.001025  0.001352\n",
              "47  0.092976  0.000253  0.000223  ...  0.000183  0.000224  0.000126\n",
              "48  0.554565  0.000197  0.000067  ...  0.000866  0.001232  0.000330\n",
              "49  2.456249  0.000478  0.003227  ...  0.005787  0.001665  0.002065\n",
              "\n",
              "[50 rows x 1351 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "riqXu-TTTqGW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred.to_csv(r'850_250_50.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7sy38_Nf2t6",
        "colab_type": "code",
        "outputId": "ab351dfa-c618-4c5c-e4d3-6b1b56e38938",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "ypred = pd.DataFrame(predictions)\n",
        "test"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rval</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>...</th>\n",
              "      <th>2510</th>\n",
              "      <th>2511</th>\n",
              "      <th>2512</th>\n",
              "      <th>2513</th>\n",
              "      <th>2514</th>\n",
              "      <th>2515</th>\n",
              "      <th>2516</th>\n",
              "      <th>2517</th>\n",
              "      <th>2518</th>\n",
              "      <th>2519</th>\n",
              "      <th>2520</th>\n",
              "      <th>2521</th>\n",
              "      <th>2522</th>\n",
              "      <th>2523</th>\n",
              "      <th>2524</th>\n",
              "      <th>2525</th>\n",
              "      <th>2526</th>\n",
              "      <th>2527</th>\n",
              "      <th>2528</th>\n",
              "      <th>2529</th>\n",
              "      <th>2530</th>\n",
              "      <th>2531</th>\n",
              "      <th>2532</th>\n",
              "      <th>2533</th>\n",
              "      <th>2534</th>\n",
              "      <th>2535</th>\n",
              "      <th>2536</th>\n",
              "      <th>2537</th>\n",
              "      <th>2538</th>\n",
              "      <th>2539</th>\n",
              "      <th>2540</th>\n",
              "      <th>2541</th>\n",
              "      <th>2542</th>\n",
              "      <th>2543</th>\n",
              "      <th>2544</th>\n",
              "      <th>2545</th>\n",
              "      <th>2546</th>\n",
              "      <th>2547</th>\n",
              "      <th>2548</th>\n",
              "      <th>2549</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>2.27</td>\n",
              "      <td>0.000720</td>\n",
              "      <td>0.001612</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.001114</td>\n",
              "      <td>0.001004</td>\n",
              "      <td>0.000882</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.001115</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000865</td>\n",
              "      <td>0.000869</td>\n",
              "      <td>0.000918</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.001007</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000943</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.555966</td>\n",
              "      <td>0.560266</td>\n",
              "      <td>0.561135</td>\n",
              "      <td>0.563992</td>\n",
              "      <td>0.564947</td>\n",
              "      <td>0.571890</td>\n",
              "      <td>0.572845</td>\n",
              "      <td>0.000955</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>2.29</td>\n",
              "      <td>0.000726</td>\n",
              "      <td>0.001626</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.001013</td>\n",
              "      <td>0.000889</td>\n",
              "      <td>0.000950</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.001125</td>\n",
              "      <td>0.000973</td>\n",
              "      <td>0.000873</td>\n",
              "      <td>0.000877</td>\n",
              "      <td>0.000926</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.001016</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000951</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.559453</td>\n",
              "      <td>0.562145</td>\n",
              "      <td>0.563022</td>\n",
              "      <td>0.564148</td>\n",
              "      <td>0.565112</td>\n",
              "      <td>0.566075</td>\n",
              "      <td>0.567039</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000900</td>\n",
              "      <td>0.000726</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>2.31</td>\n",
              "      <td>0.000732</td>\n",
              "      <td>0.001640</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.001134</td>\n",
              "      <td>0.001021</td>\n",
              "      <td>0.000897</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001135</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.000880</td>\n",
              "      <td>0.000885</td>\n",
              "      <td>0.000934</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.001025</td>\n",
              "      <td>0.000899</td>\n",
              "      <td>0.000960</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.570138</td>\n",
              "      <td>0.571023</td>\n",
              "      <td>0.573930</td>\n",
              "      <td>0.574902</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.001944</td>\n",
              "      <td>0.000972</td>\n",
              "      <td>0.000977</td>\n",
              "      <td>0.001884</td>\n",
              "      <td>0.000732</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>2.32</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>0.001647</td>\n",
              "      <td>0.000981</td>\n",
              "      <td>0.001139</td>\n",
              "      <td>0.001026</td>\n",
              "      <td>0.000901</td>\n",
              "      <td>0.000963</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.000986</td>\n",
              "      <td>0.000884</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.001029</td>\n",
              "      <td>0.000903</td>\n",
              "      <td>0.000964</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.001952</td>\n",
              "      <td>0.572606</td>\n",
              "      <td>0.573495</td>\n",
              "      <td>0.578237</td>\n",
              "      <td>0.579213</td>\n",
              "      <td>0.580189</td>\n",
              "      <td>0.581166</td>\n",
              "      <td>0.000976</td>\n",
              "      <td>0.011934</td>\n",
              "      <td>0.012846</td>\n",
              "      <td>0.000736</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>2.33</td>\n",
              "      <td>0.000739</td>\n",
              "      <td>0.001654</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>0.001144</td>\n",
              "      <td>0.001030</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>0.000967</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.000990</td>\n",
              "      <td>0.000888</td>\n",
              "      <td>0.000892</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.001033</td>\n",
              "      <td>0.000907</td>\n",
              "      <td>0.000968</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.575686</td>\n",
              "      <td>0.576628</td>\n",
              "      <td>0.577521</td>\n",
              "      <td>0.587375</td>\n",
              "      <td>0.588355</td>\n",
              "      <td>0.589336</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000980</td>\n",
              "      <td>0.000985</td>\n",
              "      <td>0.000916</td>\n",
              "      <td>0.000739</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>2.95</td>\n",
              "      <td>0.000935</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.001247</td>\n",
              "      <td>0.001448</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.001146</td>\n",
              "      <td>0.001224</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001450</td>\n",
              "      <td>0.001253</td>\n",
              "      <td>0.001124</td>\n",
              "      <td>0.001130</td>\n",
              "      <td>0.001193</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001308</td>\n",
              "      <td>0.001149</td>\n",
              "      <td>0.001225</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.725755</td>\n",
              "      <td>0.731343</td>\n",
              "      <td>0.732473</td>\n",
              "      <td>0.738503</td>\n",
              "      <td>0.739744</td>\n",
              "      <td>0.740985</td>\n",
              "      <td>0.742227</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001247</td>\n",
              "      <td>0.001159</td>\n",
              "      <td>0.000935</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>2.96</td>\n",
              "      <td>0.000938</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.001453</td>\n",
              "      <td>0.001309</td>\n",
              "      <td>0.001150</td>\n",
              "      <td>0.001228</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001454</td>\n",
              "      <td>0.001257</td>\n",
              "      <td>0.001128</td>\n",
              "      <td>0.001133</td>\n",
              "      <td>0.001197</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001152</td>\n",
              "      <td>0.001230</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.732473</td>\n",
              "      <td>0.738080</td>\n",
              "      <td>0.739213</td>\n",
              "      <td>0.745264</td>\n",
              "      <td>0.746509</td>\n",
              "      <td>0.747755</td>\n",
              "      <td>0.749000</td>\n",
              "      <td>0.001246</td>\n",
              "      <td>0.001251</td>\n",
              "      <td>0.001163</td>\n",
              "      <td>0.000938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>2.97</td>\n",
              "      <td>0.000942</td>\n",
              "      <td>0.001167</td>\n",
              "      <td>0.001256</td>\n",
              "      <td>0.001458</td>\n",
              "      <td>0.001313</td>\n",
              "      <td>0.001153</td>\n",
              "      <td>0.001232</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001459</td>\n",
              "      <td>0.001262</td>\n",
              "      <td>0.001132</td>\n",
              "      <td>0.001137</td>\n",
              "      <td>0.001201</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001317</td>\n",
              "      <td>0.001156</td>\n",
              "      <td>0.001234</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.002500</td>\n",
              "      <td>0.003749</td>\n",
              "      <td>0.728904</td>\n",
              "      <td>0.736301</td>\n",
              "      <td>0.737439</td>\n",
              "      <td>0.743510</td>\n",
              "      <td>0.744759</td>\n",
              "      <td>0.746009</td>\n",
              "      <td>0.747259</td>\n",
              "      <td>0.001250</td>\n",
              "      <td>0.001256</td>\n",
              "      <td>0.002423</td>\n",
              "      <td>0.000942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>2.98</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.001171</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.001463</td>\n",
              "      <td>0.001318</td>\n",
              "      <td>0.001157</td>\n",
              "      <td>0.001237</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001464</td>\n",
              "      <td>0.001266</td>\n",
              "      <td>0.001136</td>\n",
              "      <td>0.001141</td>\n",
              "      <td>0.001205</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001160</td>\n",
              "      <td>0.001238</td>\n",
              "      <td>0.001254</td>\n",
              "      <td>0.002508</td>\n",
              "      <td>0.003762</td>\n",
              "      <td>0.729158</td>\n",
              "      <td>0.734803</td>\n",
              "      <td>0.735944</td>\n",
              "      <td>0.739694</td>\n",
              "      <td>0.740948</td>\n",
              "      <td>0.742202</td>\n",
              "      <td>0.743456</td>\n",
              "      <td>0.761121</td>\n",
              "      <td>0.001260</td>\n",
              "      <td>0.002431</td>\n",
              "      <td>0.000945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>2.99</td>\n",
              "      <td>0.000948</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.001468</td>\n",
              "      <td>0.001322</td>\n",
              "      <td>0.001161</td>\n",
              "      <td>0.001241</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001469</td>\n",
              "      <td>0.001270</td>\n",
              "      <td>0.001140</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>...</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001326</td>\n",
              "      <td>0.001164</td>\n",
              "      <td>0.001242</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.002516</td>\n",
              "      <td>0.003775</td>\n",
              "      <td>0.005033</td>\n",
              "      <td>0.001209</td>\n",
              "      <td>0.001145</td>\n",
              "      <td>0.001470</td>\n",
              "      <td>0.001258</td>\n",
              "      <td>0.819916</td>\n",
              "      <td>0.821174</td>\n",
              "      <td>0.822432</td>\n",
              "      <td>0.001264</td>\n",
              "      <td>0.001175</td>\n",
              "      <td>0.000948</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65 rows Ã— 2551 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Rval         0         1         2  ...      2546      2547      2548      2549\n",
              "200  2.27  0.000720  0.001612  0.000960  ...  0.000955  0.000960  0.000892  0.000720\n",
              "201  2.29  0.000726  0.001626  0.000968  ...  0.000964  0.000968  0.000900  0.000726\n",
              "202  2.31  0.000732  0.001640  0.000977  ...  0.000972  0.000977  0.001884  0.000732\n",
              "203  2.32  0.000736  0.001647  0.000981  ...  0.000976  0.011934  0.012846  0.000736\n",
              "204  2.33  0.000739  0.001654  0.000985  ...  0.000980  0.000985  0.000916  0.000739\n",
              "..    ...       ...       ...       ...  ...       ...       ...       ...       ...\n",
              "260  2.95  0.000935  0.001159  0.001247  ...  0.001241  0.001247  0.001159  0.000935\n",
              "261  2.96  0.000938  0.001163  0.001251  ...  0.001246  0.001251  0.001163  0.000938\n",
              "262  2.97  0.000942  0.001167  0.001256  ...  0.001250  0.001256  0.002423  0.000942\n",
              "263  2.98  0.000945  0.001171  0.001260  ...  0.761121  0.001260  0.002431  0.000945\n",
              "264  2.99  0.000948  0.001175  0.001264  ...  0.822432  0.001264  0.001175  0.000948\n",
              "\n",
              "[65 rows x 2551 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bswMm75axXww",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = y_pred.round(4)\n",
        "resultA = test.round(4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7a1vs1dilXc",
        "colab_type": "code",
        "outputId": "f1e87d09-6cfe-47ac-8d23-448321660263",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "resultA = resultA.drop(['Rval'],axis = 1)\n",
        "resultA"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2510</th>\n",
              "      <th>2511</th>\n",
              "      <th>2512</th>\n",
              "      <th>2513</th>\n",
              "      <th>2514</th>\n",
              "      <th>2515</th>\n",
              "      <th>2516</th>\n",
              "      <th>2517</th>\n",
              "      <th>2518</th>\n",
              "      <th>2519</th>\n",
              "      <th>2520</th>\n",
              "      <th>2521</th>\n",
              "      <th>2522</th>\n",
              "      <th>2523</th>\n",
              "      <th>2524</th>\n",
              "      <th>2525</th>\n",
              "      <th>2526</th>\n",
              "      <th>2527</th>\n",
              "      <th>2528</th>\n",
              "      <th>2529</th>\n",
              "      <th>2530</th>\n",
              "      <th>2531</th>\n",
              "      <th>2532</th>\n",
              "      <th>2533</th>\n",
              "      <th>2534</th>\n",
              "      <th>2535</th>\n",
              "      <th>2536</th>\n",
              "      <th>2537</th>\n",
              "      <th>2538</th>\n",
              "      <th>2539</th>\n",
              "      <th>2540</th>\n",
              "      <th>2541</th>\n",
              "      <th>2542</th>\n",
              "      <th>2543</th>\n",
              "      <th>2544</th>\n",
              "      <th>2545</th>\n",
              "      <th>2546</th>\n",
              "      <th>2547</th>\n",
              "      <th>2548</th>\n",
              "      <th>2549</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>200</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.5560</td>\n",
              "      <td>0.5603</td>\n",
              "      <td>0.5611</td>\n",
              "      <td>0.5640</td>\n",
              "      <td>0.5649</td>\n",
              "      <td>0.5719</td>\n",
              "      <td>0.5728</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>201</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.5595</td>\n",
              "      <td>0.5621</td>\n",
              "      <td>0.5630</td>\n",
              "      <td>0.5641</td>\n",
              "      <td>0.5651</td>\n",
              "      <td>0.5661</td>\n",
              "      <td>0.5670</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.5701</td>\n",
              "      <td>0.5710</td>\n",
              "      <td>0.5739</td>\n",
              "      <td>0.5749</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>203</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.5726</td>\n",
              "      <td>0.5735</td>\n",
              "      <td>0.5782</td>\n",
              "      <td>0.5792</td>\n",
              "      <td>0.5802</td>\n",
              "      <td>0.5812</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0119</td>\n",
              "      <td>0.0128</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>204</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.5757</td>\n",
              "      <td>0.5766</td>\n",
              "      <td>0.5775</td>\n",
              "      <td>0.5874</td>\n",
              "      <td>0.5884</td>\n",
              "      <td>0.5893</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>260</th>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.7258</td>\n",
              "      <td>0.7313</td>\n",
              "      <td>0.7325</td>\n",
              "      <td>0.7385</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.7410</td>\n",
              "      <td>0.7422</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>261</th>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.7325</td>\n",
              "      <td>0.7381</td>\n",
              "      <td>0.7392</td>\n",
              "      <td>0.7453</td>\n",
              "      <td>0.7465</td>\n",
              "      <td>0.7478</td>\n",
              "      <td>0.7490</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>262</th>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.7289</td>\n",
              "      <td>0.7363</td>\n",
              "      <td>0.7374</td>\n",
              "      <td>0.7435</td>\n",
              "      <td>0.7448</td>\n",
              "      <td>0.7460</td>\n",
              "      <td>0.7473</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>263</th>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.7292</td>\n",
              "      <td>0.7348</td>\n",
              "      <td>0.7359</td>\n",
              "      <td>0.7397</td>\n",
              "      <td>0.7409</td>\n",
              "      <td>0.7422</td>\n",
              "      <td>0.7435</td>\n",
              "      <td>0.7611</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>264</th>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.8199</td>\n",
              "      <td>0.8212</td>\n",
              "      <td>0.8224</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0009</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65 rows Ã— 2550 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       0       1       2       3     ...    2546    2547    2548    2549\n",
              "200  0.0007  0.0016  0.0010  0.0011  ...  0.0010  0.0010  0.0009  0.0007\n",
              "201  0.0007  0.0016  0.0010  0.0011  ...  0.0010  0.0010  0.0009  0.0007\n",
              "202  0.0007  0.0016  0.0010  0.0011  ...  0.0010  0.0010  0.0019  0.0007\n",
              "203  0.0007  0.0016  0.0010  0.0011  ...  0.0010  0.0119  0.0128  0.0007\n",
              "204  0.0007  0.0017  0.0010  0.0011  ...  0.0010  0.0010  0.0009  0.0007\n",
              "..      ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "260  0.0009  0.0012  0.0012  0.0014  ...  0.0012  0.0012  0.0012  0.0009\n",
              "261  0.0009  0.0012  0.0013  0.0015  ...  0.0012  0.0013  0.0012  0.0009\n",
              "262  0.0009  0.0012  0.0013  0.0015  ...  0.0012  0.0013  0.0024  0.0009\n",
              "263  0.0009  0.0012  0.0013  0.0015  ...  0.7611  0.0013  0.0024  0.0009\n",
              "264  0.0009  0.0012  0.0013  0.0015  ...  0.8224  0.0013  0.0012  0.0009\n",
              "\n",
              "[65 rows x 2550 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtOA9BcTjjmd",
        "colab_type": "code",
        "outputId": "7017e86e-fc3f-48c3-a31c-04588facb114",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        }
      },
      "source": [
        "ypred = ypred.drop([0],axis = 1)\n",
        "ypred"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>2511</th>\n",
              "      <th>2512</th>\n",
              "      <th>2513</th>\n",
              "      <th>2514</th>\n",
              "      <th>2515</th>\n",
              "      <th>2516</th>\n",
              "      <th>2517</th>\n",
              "      <th>2518</th>\n",
              "      <th>2519</th>\n",
              "      <th>2520</th>\n",
              "      <th>2521</th>\n",
              "      <th>2522</th>\n",
              "      <th>2523</th>\n",
              "      <th>2524</th>\n",
              "      <th>2525</th>\n",
              "      <th>2526</th>\n",
              "      <th>2527</th>\n",
              "      <th>2528</th>\n",
              "      <th>2529</th>\n",
              "      <th>2530</th>\n",
              "      <th>2531</th>\n",
              "      <th>2532</th>\n",
              "      <th>2533</th>\n",
              "      <th>2534</th>\n",
              "      <th>2535</th>\n",
              "      <th>2536</th>\n",
              "      <th>2537</th>\n",
              "      <th>2538</th>\n",
              "      <th>2539</th>\n",
              "      <th>2540</th>\n",
              "      <th>2541</th>\n",
              "      <th>2542</th>\n",
              "      <th>2543</th>\n",
              "      <th>2544</th>\n",
              "      <th>2545</th>\n",
              "      <th>2546</th>\n",
              "      <th>2547</th>\n",
              "      <th>2548</th>\n",
              "      <th>2549</th>\n",
              "      <th>2550</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0001</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0001</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>-0.0037</td>\n",
              "      <td>-0.0067</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>-0.0056</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>-0.0012</td>\n",
              "      <td>0.0095</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>-0.0037</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0049</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0069</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>-0.0072</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>-0.0020</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.6876</td>\n",
              "      <td>0.0222</td>\n",
              "      <td>0.4675</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0007</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.0016</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0113</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>-0.0059</td>\n",
              "      <td>-0.0117</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>-0.0062</td>\n",
              "      <td>-0.0032</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0154</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>-0.0024</td>\n",
              "      <td>-0.0059</td>\n",
              "      <td>-0.0049</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>-0.0074</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>-0.0063</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0050</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>-0.0116</td>\n",
              "      <td>-0.0057</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0131</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>-0.0065</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>0.0091</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>0.0087</td>\n",
              "      <td>0.0128</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0087</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.6119</td>\n",
              "      <td>0.0182</td>\n",
              "      <td>0.4185</td>\n",
              "      <td>0.0138</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>-0.0006</td>\n",
              "      <td>-0.0047</td>\n",
              "      <td>0.0089</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.0073</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0026</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0130</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0020</td>\n",
              "      <td>-0.0024</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>0.0132</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>-0.0050</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0033</td>\n",
              "      <td>-0.0054</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>-0.0089</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0008</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>-0.0041</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0046</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.7937</td>\n",
              "      <td>0.0197</td>\n",
              "      <td>0.5393</td>\n",
              "      <td>0.0183</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>0.0026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0163</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>-0.0037</td>\n",
              "      <td>-0.0104</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>-0.0066</td>\n",
              "      <td>-0.0082</td>\n",
              "      <td>-0.0076</td>\n",
              "      <td>-0.0047</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>-0.0029</td>\n",
              "      <td>0.0179</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0083</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0087</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>-0.0020</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0084</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>-0.0036</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>-0.0018</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>-0.0029</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0010</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>0.7158</td>\n",
              "      <td>0.0169</td>\n",
              "      <td>0.4880</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.0029</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0058</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0102</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0009</td>\n",
              "      <td>-0.0045</td>\n",
              "      <td>-0.0012</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>-0.0006</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0035</td>\n",
              "      <td>-0.0018</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>-0.0006</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>-0.0043</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>0.0028</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0005</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>-0.0001</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0056</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>-0.0036</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.7367</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.4981</td>\n",
              "      <td>0.0137</td>\n",
              "      <td>-0.0004</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.0018</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>-0.0084</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>-0.0077</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0106</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0116</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>-0.0029</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>0.0061</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>-0.0078</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0036</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>-0.0019</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>-0.0090</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0126</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0026</td>\n",
              "      <td>-0.0045</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0101</td>\n",
              "      <td>-0.0034</td>\n",
              "      <td>0.0041</td>\n",
              "      <td>0.0158</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.9694</td>\n",
              "      <td>0.0268</td>\n",
              "      <td>0.6520</td>\n",
              "      <td>0.0178</td>\n",
              "      <td>-0.0000</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0013</td>\n",
              "      <td>0.0053</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>-0.0087</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>-0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0099</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>-0.0035</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0064</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>-0.0129</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>-0.0057</td>\n",
              "      <td>-0.0093</td>\n",
              "      <td>-0.0066</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0043</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>-0.0069</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0111</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>-0.0107</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0092</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0148</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0033</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>-0.0036</td>\n",
              "      <td>0.0023</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0029</td>\n",
              "      <td>-0.0005</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.9333</td>\n",
              "      <td>0.0234</td>\n",
              "      <td>0.6345</td>\n",
              "      <td>0.0173</td>\n",
              "      <td>-0.0012</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0019</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>-0.0144</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>0.0171</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>-0.0050</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>-0.0127</td>\n",
              "      <td>0.0059</td>\n",
              "      <td>0.0119</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>-0.0059</td>\n",
              "      <td>-0.0024</td>\n",
              "      <td>-0.0115</td>\n",
              "      <td>-0.0045</td>\n",
              "      <td>0.0084</td>\n",
              "      <td>0.0104</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0260</td>\n",
              "      <td>0.0100</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>-0.0021</td>\n",
              "      <td>0.0055</td>\n",
              "      <td>0.0032</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0081</td>\n",
              "      <td>0.0133</td>\n",
              "      <td>-0.0054</td>\n",
              "      <td>-0.0125</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>-0.0032</td>\n",
              "      <td>0.0065</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0017</td>\n",
              "      <td>0.0012</td>\n",
              "      <td>0.0031</td>\n",
              "      <td>-0.0067</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>-0.0025</td>\n",
              "      <td>-0.0016</td>\n",
              "      <td>-0.0129</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0174</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0068</td>\n",
              "      <td>0.0050</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>0.0070</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>0.0096</td>\n",
              "      <td>-0.0061</td>\n",
              "      <td>-0.0067</td>\n",
              "      <td>0.0004</td>\n",
              "      <td>0.0122</td>\n",
              "      <td>0.0003</td>\n",
              "      <td>0.0078</td>\n",
              "      <td>1.0342</td>\n",
              "      <td>0.0284</td>\n",
              "      <td>0.6960</td>\n",
              "      <td>0.0209</td>\n",
              "      <td>-0.0052</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0081</td>\n",
              "      <td>0.0108</td>\n",
              "      <td>0.0098</td>\n",
              "      <td>0.0034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>-0.0097</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>-0.0051</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>-0.0114</td>\n",
              "      <td>-0.0013</td>\n",
              "      <td>0.0006</td>\n",
              "      <td>0.0099</td>\n",
              "      <td>-0.0003</td>\n",
              "      <td>-0.0117</td>\n",
              "      <td>0.0085</td>\n",
              "      <td>0.0156</td>\n",
              "      <td>0.0011</td>\n",
              "      <td>-0.0032</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0067</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0091</td>\n",
              "      <td>0.0283</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0093</td>\n",
              "      <td>-0.0088</td>\n",
              "      <td>0.0092</td>\n",
              "      <td>-0.0057</td>\n",
              "      <td>0.0024</td>\n",
              "      <td>0.0094</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.0077</td>\n",
              "      <td>0.0088</td>\n",
              "      <td>0.0219</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>-0.0159</td>\n",
              "      <td>0.0014</td>\n",
              "      <td>0.0010</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0002</td>\n",
              "      <td>-0.0056</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>-0.0085</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0047</td>\n",
              "      <td>-0.0028</td>\n",
              "      <td>-0.0111</td>\n",
              "      <td>-0.0094</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>0.0177</td>\n",
              "      <td>-0.0011</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0027</td>\n",
              "      <td>0.0020</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>0.0021</td>\n",
              "      <td>0.0103</td>\n",
              "      <td>0.0063</td>\n",
              "      <td>0.0122</td>\n",
              "      <td>0.0016</td>\n",
              "      <td>-0.0084</td>\n",
              "      <td>0.0056</td>\n",
              "      <td>-0.0014</td>\n",
              "      <td>-0.0137</td>\n",
              "      <td>0.0066</td>\n",
              "      <td>0.0110</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>0.0091</td>\n",
              "      <td>1.0370</td>\n",
              "      <td>0.0258</td>\n",
              "      <td>0.6998</td>\n",
              "      <td>0.0301</td>\n",
              "      <td>-0.0089</td>\n",
              "      <td>-0.0112</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0018</td>\n",
              "      <td>-0.0005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>64</th>\n",
              "      <td>-0.0076</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>-0.0063</td>\n",
              "      <td>0.0229</td>\n",
              "      <td>0.0194</td>\n",
              "      <td>-0.0058</td>\n",
              "      <td>0.0048</td>\n",
              "      <td>0.0025</td>\n",
              "      <td>0.0124</td>\n",
              "      <td>-0.0056</td>\n",
              "      <td>-0.0166</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>0.0073</td>\n",
              "      <td>0.0072</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0034</td>\n",
              "      <td>0.0146</td>\n",
              "      <td>-0.0031</td>\n",
              "      <td>0.0291</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0184</td>\n",
              "      <td>-0.0024</td>\n",
              "      <td>0.0129</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0073</td>\n",
              "      <td>0.0144</td>\n",
              "      <td>0.0071</td>\n",
              "      <td>0.0087</td>\n",
              "      <td>0.0076</td>\n",
              "      <td>0.0155</td>\n",
              "      <td>-0.0022</td>\n",
              "      <td>-0.0168</td>\n",
              "      <td>0.0079</td>\n",
              "      <td>0.0042</td>\n",
              "      <td>0.0002</td>\n",
              "      <td>0.0036</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0045</td>\n",
              "      <td>-0.0048</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0039</td>\n",
              "      <td>-0.0095</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0226</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0039</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>-0.0056</td>\n",
              "      <td>0.0044</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>-0.0040</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0062</td>\n",
              "      <td>0.0166</td>\n",
              "      <td>0.0109</td>\n",
              "      <td>0.0054</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0086</td>\n",
              "      <td>-0.0062</td>\n",
              "      <td>-0.0009</td>\n",
              "      <td>0.0040</td>\n",
              "      <td>0.0139</td>\n",
              "      <td>-0.0027</td>\n",
              "      <td>0.0051</td>\n",
              "      <td>0.8245</td>\n",
              "      <td>0.0242</td>\n",
              "      <td>0.5671</td>\n",
              "      <td>0.0203</td>\n",
              "      <td>-0.0094</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0086</td>\n",
              "      <td>0.0143</td>\n",
              "      <td>0.0038</td>\n",
              "      <td>0.0040</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>65 rows Ã— 2550 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      1       2       3       4     ...    2547    2548    2549    2550\n",
              "0  -0.0001  0.0022  0.0049  0.0001  ... -0.0002  0.0062  0.0031  0.0007\n",
              "1  -0.0016  0.0029  0.0058 -0.0046  ... -0.0047  0.0089  0.0038  0.0020\n",
              "2  -0.0073  0.0038  0.0028 -0.0016  ...  0.0006  0.0034  0.0031  0.0026\n",
              "3  -0.0091  0.0053  0.0076 -0.0051  ... -0.0027  0.0051  0.0057  0.0041\n",
              "4  -0.0029  0.0037  0.0058  0.0002  ... -0.0003  0.0032  0.0034  0.0002\n",
              "..     ...     ...     ...     ...  ...     ...     ...     ...     ...\n",
              "60 -0.0018  0.0034  0.0075 -0.0016  ...  0.0013  0.0053  0.0059  0.0015\n",
              "61 -0.0087  0.0034 -0.0007 -0.0008  ...  0.0019  0.0070  0.0039  0.0005\n",
              "62 -0.0144  0.0044  0.0015 -0.0025  ... -0.0081  0.0108  0.0098  0.0034\n",
              "63 -0.0097  0.0097  0.0027 -0.0051  ... -0.0105  0.0030  0.0018 -0.0005\n",
              "64 -0.0076  0.0075  0.0109 -0.0063  ... -0.0086  0.0143  0.0038  0.0040\n",
              "\n",
              "[65 rows x 2550 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aj1AdngAix8",
        "colab_type": "code",
        "outputId": "d0a85267-dc95-486f-b739-439e679cd625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "flag = 0\n",
        "count = 0\n",
        "for i in range(0,2549):\n",
        "  for j in range(0,65):\n",
        "    count = count+1\n",
        "    if(resultA[i+1][j+200] == abs(ypred[i+1][j])):\n",
        "      flag = flag+1\n",
        "flag/count"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.002613392884087274"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF6Z1xhSzbxJ",
        "colab_type": "code",
        "outputId": "88b232a6-b7d6-4138-83d9-ed63a257058e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "predictions"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-fd2951b82103>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"
          ]
        }
      ]
    }
  ]
}